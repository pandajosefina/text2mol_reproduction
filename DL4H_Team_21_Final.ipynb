{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AqmCs3gBwf6-"
   },
   "source": [
    "# **Decoding Molecular Structures: An In-Depth Replication of Text2Mol for Advanced Cross-Modal Retrieval Using Natural Language Queries**\n",
    "\n",
    "**Course: CS 598 Deep Learning for Healthcare, Spring 2024**\n",
    "\n",
    "**Team 21:** Sherry Li(xuehail2@illinois.edu) and Jo Yang(jiaoy2@illinois.edu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ki4nxawDW-n"
   },
   "source": [
    "---\n",
    "# <font color=blue><font>Getting Started: Download Files, Explore GitHub Repository and Video Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-mI3_maTDY5U"
   },
   "source": [
    "Essential files required for running this notebook must be downloaded from our Google Cloud Storage bucket. Below are detailed instructions for accessing these files. For a comprehensive view of our project, please explore the GitHub repository and the video presentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QmukH-F6EtJs"
   },
   "source": [
    "## Download Files from Google Cloud Storage\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "C-6XHWk2Qf44"
   },
   "source": [
    "The required files for this notebook are hosted in a publicly and anonymously accessible Google Cloud Storage (GCS) bucket. It is essential to download these before proceeding to ensure all data and models needed for the project are available.  \n",
    "\n",
    "**GCS bucket URL:** [gs://598dhl_text2mol_team21](https://console.cloud.google.com/storage/browser/598dhl_text2mol_team21)\n",
    "\n",
    "**Contents in the bucket and shared Google folder:**\n",
    "*   **data:** The folder contains all the processed data needed for this project.\n",
    "*   **code:** We utilized the original authors' code to reproduce the baseline models and their ensembles (Edwards et al, 2021). Then we conducted extensive systematic ablation studies and further experiments that extend beyond the scope of the initial publication. This folder includes the complete code for this project, including both the authors' original code and our extended implementations.\n",
    "*   **model checkpoint:** Each model checkpoint in our project exceeds 400 MB. Due to the storage limitations imposed by Colab, we only download one MLP model checkpoint\n",
    " of [2024_4_8_epoch40_sample100_mlp1](https://console.cloud.google.com/storage/browser/598dhl_text2mol_team21) to demonstrate the model evaluation process. The model checkpoints for all the models trained in this project are accessible by the **All Model Checkpoints URL:** https://drive.google.com/drive/folders/1rBOlPcRzkQ-lf7qCSuWbFOT2fqc3GopY?usp=drive_link\n",
    "\n",
    "*   **embeddings:** All embeddings for all the models trained are available in the **All Model Embeddings URL:** https://drive.google.com/drive/folders/1mGnP54MZUGfq-sKa-XB_Yn1Uypx3Pgul?usp=drive_link.\n",
    "*   **model evaluation results:** This folder contains the results of the evaluation metrics of all the model evaluations.\n",
    "*   **training_loss_validation_loss_computation_time:** Contains detailed results of training and validation losses and actual computational times for all the models.\n",
    "*   **mha_weights.pkl:** Contains the saved parameters for the multi-head attention layer.\n",
    "*   **images:** Contains the original images cited in this notebook.\n",
    "\n",
    "\n",
    "\n",
    "**Steps to download:**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r6mIfX3hWzpr",
    "outputId": "94071fa8-beb1-47c1-b5e4-4723c3bc6063"
   },
   "outputs": [],
   "source": [
    "#@title Download Files from GCS Bucket and Store in '/content/tar_files'\n",
    "\n",
    "# 1. Download compressed files from the GCS bucket\n",
    "!wget -P \"/content/\" https://storage.googleapis.com/598dhl_text2mol_team21/tar_files.tar.gz\n",
    "print(\"Downloading files completed!\")\n",
    "\n",
    "# 2. Decompress and store the files in the dir\n",
    "!tar xzf \"/content/tar_files.tar.gz\" -C \"/content/\"\n",
    "print(\"Unzipping files completed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R42F3W2he5rf"
   },
   "source": [
    "## Explore GitHub Repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KTzp47fhfAOr"
   },
   "source": [
    "All code and notebooks used in this project are available in the public GitHub repository.\n",
    "\n",
    "**GitHub Repository URL:** https://github.com/sherrylinice/598dhl.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vlDpW7affLZB"
   },
   "source": [
    "## Video Presentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PRhRZOhmfblV"
   },
   "source": [
    "The 4-minitues video presentation provides an overview of the project and insights into the research findings.\n",
    "\n",
    "**Video presentation URL:** https://mediaspace.illinois.edu/media/t/1_zekf6gtq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_QBCVv445UcB"
   },
   "source": [
    "---\n",
    "# <font color=blue>1. INTRODUCTION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uicjmij55bc1"
   },
   "source": [
    "## 1.1 Background Of The Problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MQ0sNuMePBXx"
   },
   "source": [
    "  * *what type of problem:* both pharmaceutical research and chemistry information retrieval tools require mapping natural language descriptions of the molecules to their corresponding molecular graphs. The lack of descriptions of the molecules and the inability to effectively retrieve the relevant molecules based on text descriptions resulted in a low utilization of the extensive molecule database available. This paper introduces Text2Mol which converts both the natural language text descriptions and the molecular graphs into embeddings and retrieves the most relevant molecules based on the natural text descriptions.\n",
    "\n",
    "  * *what is the importance/meaning of solving the problem*: The innovative methodology proposed by the paper facilitates the translation of textual descriptions to molecules at scale. This advancement can help significantly enhance the drug discovery process in pharmaceutical research and enable a more comprehensive search of the corresponding molecules based on the natural language queries.\n",
    "\n",
    "  * *what is the difficulty of the problem*: the challenge of the problem lies in the fact that molecular graphs and natural text descriptions have very different structures. Molecular graphs possess unique structures which make them distinct from typical linguistic formats. Aligning natural text descriptions with the corresponding molecular graphs requires first translating them into comparable linguistic dimensions.\n",
    "\n",
    "  * *the state of the art methods and effectiveness*: two baseline models were proposed in the paper. The MLP model uses Morgan fingerprinting to convert the molecular graphs into embeddings whereas the GCN uses a graph architecture to learn the relationships between the different nodes in the molecular graphs. The effectiveness of these two models were evaluated by Mean Rank and Mean Reciprocal Rank(MRR)(Edwards et al., 2021).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "da7s6CMO5tWd"
   },
   "source": [
    "## 1.2 Paper Explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FxmkE5s355nE"
   },
   "source": [
    "  * *what did the paper propose*:\n",
    "  \n",
    "  The paper proposed Text2Mol, which requires the creation of a common semantic embedding space for molecules and textual descriptions to enable retrieval.\n",
    "      - A text encoder translates natural language text descriptions into embeddings, using a pre-trained SciBERT(Beltagy et al., 2019) followed by a linear layer.\n",
    "      - A molecule encoder converts molecules into embeddings.\n",
    "\n",
    "        - The multi-layer perceptron (MLP) architecture uses Morgan fingerprinting (Rogers & Hahn, 2010) to convert the visual representation of the molecules into embeddings. These embeddings are fed into a two-layer MLP followed by a linear layer.\n",
    "       - The three-layer graph convolutional network (GCN) architecture uses nodes to represent the atoms in the molecule graph and learns the relationships between the different substructures.\n",
    "\n",
    "  The paper also implemented an ensemble approach where both the GCN and MLP architectures are incorporated.\n",
    "\n",
    "  A transformer decoder with integrated cross-modal attention is applied:\n",
    "      - Utilizing the text representation output from SciBERT as a source sequence, and the node representations from Mol2vec (Jaeger et al., 2018) GCN as a target sequence.\n",
    "      - Applying a cross-modal attention between the two encoders.\n",
    "      - Leveraging a symmetric contrastive loss function augmented with negative sampling for enhanced modality integration.\n",
    "\n",
    "\n",
    "  * *what is the innovations of the method*: the innovation of the paper lies in the multiple techniques that were employed to improve the performance of the baseline models:\n",
    "    - The ensemble method integrate the MLP and GCN architectures and capitalize on their combined strengths\n",
    "    - The cross-modal attention module effectively aligns the linguistic and structural aspects of the data, and better captures the relationship between the two\n",
    "    - The use of a symmetric contrastive loss function further enhances the integration of the different modals and ensures that the model balances the optimization of the text encoder and the molecule encoder\n",
    "\n",
    "\n",
    "  * *how well the proposed method work (in its own metrics)*: the all ensemble approach proposed by the paper, which incorporates the ensemble of the MLP and the GCN model with cross-modal attention achieved the lowest mean rank on the test set. It reduced the mean rank from 30.38 on the MLP baseline model to 20.21. Additionally, the MRR improved from 0.372 to 0.499 (Edwards et al., 2021). The all-ensemble model also outperformed the MLP only ensemble model and the GCN only ensemble model, further proving that the all-ensemble structure effectively leverages the strengths of both the MLP and GCN models.\n",
    "\n",
    "\n",
    "  * *what is the contribution to the reasearch regime (referring the Background above, how important the paper is to the problem):* the techniques proposed in the paper including the ensemble approach, the cross-modal attention and the symmetric contrastive loss function substantially improved the performance of the text to molecule retrieval tasks. They improved the MRR by close to 35% compared to the MLP baseline model. This significant improvement will greatly enhance the accuracy of information retrieval tools in retrieving relevant molecular graphs based on natural text descriptions. This improved accuracy will enable a higher utilization of molecule database. It will help expedite the process of discovering the potential compounds for pharmaceutical researchers and accelerate the advancement of innovation in the pharmaceutical industry. Moreover, the ensemble approach and the cross-modal attention module are also potentially applicable in other scenarios where multiple models are employed and information from various aspects needs to be captured.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Klcz3ZEg6SXJ"
   },
   "source": [
    "---\n",
    "# <font color=blue>2. SCOPE OF REPRODUCIBILITY</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uygL9tTPSVHB"
   },
   "source": [
    " - **Hypothesis 1:** All the configurations of baseline model architectures, and the chosen hyperparameters, are required to achived optimal model performances, such that changing any of them will result in performance decline.\n",
    "   \n",
    "   - **Motivation:** According to Edwards et al. (Edwards et al., 2021), the composition of the dataset, the architecture of the baseline model, and the choices of hyperparameters are crucial for effectively unifying textual and molecular modalities within a shared semantic embedding space. Therefore, we aim to rigorously test these components' impact on model performance.\n",
    "   \n",
    "   - **Experiments:** To test our hypothesis, we carried out a series of ablation studies and extended experiments. These included:\n",
    "\n",
    "     - **Removing normalization layers** to understand their role in model convergence and stability.\n",
    "     - **Removing MLP hidden layers** to explore the depth's influence on the model's ability to learn complex representations.\n",
    "     - **Removing the convolutional layer from the GCN** to assess the necessity of additional convolutional processing in capturing molecular details.\n",
    "     - **Switching to max pooling in the GCN** to examine how changes in pooling affect the aggregation of molecular features.\n",
    "     - **Adjusting batch size to 16** to observe the impact on the model's learning process and gradient estimation.\n",
    "     - **Incorporating a dropout rate of 0.5** to determine its effect on model overfitting and generalization.\n",
    "     - **Modifying learning rates** and **loss functions,** and implementing **early stopping** to evaluate their influence on training effectiveness and model optimization.\n",
    "     - Additionally, we investigated how **varying text lengths** affect model performance.\n",
    "     - To better understand the interaction between textual and molecular modalities in the shared semantic space, we employed **t-SNE for embedding visualization**, providing a graphical representation of how well the model integrates different modalities.\n",
    "\n",
    "- **Hypothesis 2:** The ensemble approach boosts the baseline performance of the model.\n",
    "\n",
    "   - **Motivation:** Given that the GCN architecture appears to be more effective at retrieving the most difficult examples in the validation set and the MLP architecture is more effective at retrieving the most difficult examples in the test set (Edwards et al., 2021) , integrating these two architectures is expected to improve the performance of the model.\n",
    "   \n",
    "   - **Experiments:** We evaluated the baseline MLP and GCN models and compared their performance against various ensemble strategies.\n",
    "\n",
    "     - **Pure MLP or GCN model ensembles** to assess the effectiveness of integrating different model layers.\n",
    "     - **Hybrid ensembles** to explore synergistic effects between MLP and GCN architectures.\n",
    "     - **Additional experiments with alternative ensemble strategies** such as max rank and weighted rank average ensembles to determine the most effective method for combining model outputs.\n",
    "\n",
    "- **Hypothesis 3:** The models with the cross-modal attention layer will further improve the performance of the MLP model baseline. This enhancement is anticipated through the introduction of new association rules extracted from the attention model.\n",
    "\n",
    "  - **Motivation:** The cross-modal attention mechanism used in the paper only considers the ten most confident association rules between the text and the molecule, ensuring that the most important association information is captured while maintaining efficiency. We anticipate that the association rules extracted from the attention model will provide further insights into the relations between the text descriptions and the molecular structures. And this will translate into further improvement of the performance of the MLP baseline model.\n",
    "\n",
    "  - **Experiments:** We explored various model configurations to assess the impact of additional attention layers on performance:\n",
    "\n",
    "    - **MLP model combined with the attention layer (trained with GCN + transformer decoder)** compared against the baseline MLP model and an **MLP model integrated with FP-Growth association rules**. This comparison helps to elucidate the value added by advanced neural architectures and data mining techniques within the same framework.\n",
    "    -  **Additional experiment focusing on the pure effect of the attention layer**, comparing the baseline GCN model with a GCN model enhanced by the attention layer. This direct comparison aims to isolate and evaluate the specific contribution of the attention mechanism to the model's performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWAHJ_1CdtaA"
   },
   "source": [
    "---\n",
    "# <font color=blue>3. METHODOLOGY</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QsqIzOEeUHAC"
   },
   "source": [
    "The methodology section of our project comprehensively outlines the data distribution, model architecture, training procedures, and evaluation metrics, forming the core framework through which we systematically investigate through ablation studies and additional experiments the integration of textual and molecular modalities within a shared semantic space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZBzwSHJRalwq"
   },
   "source": [
    "## 3.1 Environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fQ_kx7zpb-YS"
   },
   "source": [
    "### 3.1.1 Python Version"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wdi_vNU4DZJf"
   },
   "source": [
    "The python version used is 3.9.16."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tBK3Ve1-cI_C"
   },
   "source": [
    "### 3.1.2 Dependencies/Packages Needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cniL3pE8DlIK"
   },
   "source": [
    "The following packages are required: `torch` (pre-installed), `transformers`, `scikit-learn`(pre-installed), `numpy`(pre-installed), and `torch_geometric`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84,
     "referenced_widgets": [
      "f83a7aaaa6b14802a34044ccdd932b08",
      "c9ef4a6f36ab406885c00cdf76d0ddf2",
      "86dcd2ed50c14b349286786aa4c3a9cd",
      "6ea3f8bb38a0462fa8f2e98ecdf937f6",
      "ad51fd1e718b41aa9c9732fae04d2c9e",
      "fa523698e082448db270af282d427f91",
      "3cd267c740384717bd920a7705177e4d",
      "375e90df0c8a45ddb087a6c8249a9900",
      "35697ac58f234506bf9c29b996a017bd",
      "621cd25b67524460b998c428c33b2a68",
      "516f0a7f931c42ffa45a297bfa628e38"
     ]
    },
    "id": "7fGl8SgnBe1M",
    "outputId": "6ecd343a-2837-4899-dbf2-6922fdd884b2"
   },
   "outputs": [],
   "source": [
    "#@title Install Required Packages with Progress Bar\n",
    "\n",
    "# Install required packages with progress bar\n",
    "from tqdm.notebook import tqdm\n",
    "import time\n",
    "\n",
    "total_packages = 2\n",
    "with tqdm(total=total_packages, desc='Installing Packages') as pbar:\n",
    "    try:\n",
    "        !pip3 install torch_geometric --quiet\n",
    "        pbar.update(1)\n",
    "        !pip3 install transformers --quiet\n",
    "        pbar.update(2)\n",
    "    except Exception as e:\n",
    "        print(f\"Error occurred during installation: {str(e)}\")\n",
    "\n",
    "# Import required packages\n",
    "import numpy as np\n",
    "from google.colab import drive\n",
    "import csv\n",
    "import torch\n",
    "import matplotlib.pyplot as plt\n",
    "import transformers\n",
    "\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from transformers import BertTokenizer, BertTokenizerFast, BertModel\n",
    "from torch_geometric.nn import GCNConv\n",
    "from torch_geometric.nn import global_mean_pool\n",
    "from torch_geometric.nn import global_max_pool\n",
    "from torch.nn import TransformerDecoder, TransformerDecoderLayer\n",
    "\n",
    "import pandas as pd\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import os\n",
    "import os.path as osp\n",
    "import shutil\n",
    "import math\n",
    "import pickle\n",
    "import numpy as np\n",
    "import torch.optim as optim\n",
    "from transformers.optimization import get_linear_schedule_with_warmup\n",
    "import argparse\n",
    "import zipfile\n",
    "import random\n",
    "from torch.utils.data import Dataset, DataLoader, RandomSampler\n",
    "\n",
    "from torch_geometric.data import Dataset as GeoDataset\n",
    "from torch_geometric.data import DataLoader as GeoDataLoader\n",
    "from torch_geometric.data import Data, Batch\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "import operator\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.manifold import TSNE\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "print(\"Dependencies all installed!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHeiiqnx6qDh"
   },
   "source": [
    "##  3.2 Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FmonxiWueDz1"
   },
   "source": [
    "### 3.2.1 Data Download Instructions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3vuwI9ZUpC8Q"
   },
   "source": [
    "In this project, we utilized the ChEBI-20 dataset created by the authors for the purpose of reproducibility, accessible through their GitHub repository (Edwards, 2021). The dataset can be downloaded from the public GCS Bucket indicated in \"Getting Started\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ziN0iP5SdpDI"
   },
   "source": [
    "### 3.2.2 Data Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2NbPHUTMbkD3"
   },
   "source": [
    "  #### 3.2.2.1 Source Of The Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a9y_2XmgpK--"
   },
   "source": [
    "  The authors utilized data from PubChem and the Chemical Entities of Biological Interest (ChEBI) repository to construct their dataset. Specifically, they obtained 102,980 compound-description pairs by scraping compound annotations from PubChem that were derived from the ChEBI database. Subsequently, they curated a subset, consisting 33,010 compound-description pairs with description length longer than 20 words, designated as ChEBI-20. We are utilizing the ChEBI-20 (Edwards, 2021) in this project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AcU8cOIBhq1K"
   },
   "source": [
    " #### 3.2.2.2 Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr4IjtdmpOGN"
   },
   "source": [
    "The authors developed a comprehensive data preprocessing approach that involves several steps for encoding and utilizing chemical and textual data.\n",
    "\n",
    "- **Molecular Data Representation using Morgan Fingerprinting**:\n",
    "   - The preprocessing begins with the molecular data. Molecules are initially represented using Morgan fingerprinting, a variant of the Extended Connectivity Fingerprint. This method generates a fixed-size vector representation of the molecule.\n",
    "   - Morgan fingerprinting captures the topological structure of the molecule by considering circular neighborhoods (or substructures) around each atom. These are hashed into a fixed-length bit vector, providing a compact representation that reflects the molecule's structural characteristics.\n",
    "\n",
    "- **Mapping Molecules to Mol2Vec Embeddings**:\n",
    "   - The Morgan fingerprints, which serve as \"sentences\" of substructure identifiers, are then converted into embedding vectors using the Mol2Vec methodology. This technique is analogous to the Word2Vec model in natural language processing.\n",
    "   - Mol2Vec treats substructures as \"words\" and learns their embeddings such that substructures with similar chemical environments have similar vector representations in the embedding space. The process leverages the training approach of Word2Vec, particularly using the skip-gram model.\n",
    "\n",
    "- **Textual Data Integration**:\n",
    "   - Each molecule is associated with a Compound ID (CID), which links to textual descriptions detailing the molecule’s properties and characteristics. These descriptions serve as the natural language context for the molecules.\n",
    "   - The textual data corresponding to each CID is fetched and used as part of the input to the model. This integration allows the system to consider both structured chemical data and unstructured textual descriptions in its learning process.\n",
    "\n",
    "- **Concatenation of Molecular and Textual Embeddings**:\n",
    "   - The embeddings generated from Mol2Vec for the molecules and the embeddings from SciBERT for the descriptions are joined by CID.\n",
    "\n",
    "Note that the dataset provided by the author is already preprocessed. They did not explictly provide the data preprocessing commands in their Github repository. Instead, they made only the processed ChEBI-20 dataset available, while indicating that the raw data from ChEBI could be accessed and downloaded."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RC24bEK5h7QI"
   },
   "source": [
    "#### 3.2.2.3 Descriptive Statistics and Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YWryD8T8pRWo"
   },
   "source": [
    "The dataset contains 33,010 records in total of text descriptions and their corresponding molecule embeddings. The split between training, validation and test datasets are 80%/10%/10% (See output cell below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qCCJuA1LLXPj",
    "outputId": "dd6ca549-c487-4ef7-d590-2bf5abcc4df4"
   },
   "outputs": [],
   "source": [
    "#@title Dataset Summary: Record Count and Split Ratios\n",
    "\n",
    "# Directory to load raw data\n",
    "training_data = '/content/tar_files/data/training.txt'\n",
    "val_data = '/content/tar_files/data/val.txt'\n",
    "test_data = '/content/tar_files/data/test.txt'\n",
    "\n",
    "def load_raw_data(raw_data_dir):\n",
    "    descriptions = {}\n",
    "    mols = {}\n",
    "    cids = []\n",
    "    with open(raw_data_dir) as f:\n",
    "        reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames=['cid', 'mol2vec', 'desc'])\n",
    "        for index, line in enumerate(reader):\n",
    "            cid = line['cid']\n",
    "            mol2vec = [float(x) for x in line['mol2vec'].split()]\n",
    "            descriptions[cid] = line['desc']\n",
    "            mols[cid] = mol2vec\n",
    "            cids.append(cid)\n",
    "\n",
    "    return descriptions, mols, cids\n",
    "\n",
    "train_desc, train_mols, train_cids = load_raw_data(training_data)\n",
    "val_desc, val_mols, val_cids = load_raw_data(val_data)\n",
    "test_desc, test_mols, test_cids = load_raw_data(test_data)\n",
    "\n",
    "# Calculate dataset statistics\n",
    "def calculate_stats(training_data, val_data, test_data):\n",
    "    total_records = len(training_data) + len(val_data) + len(test_data)\n",
    "    unique_molecules = len(set().union(training_data, val_data, test_data))\n",
    "    return total_records, unique_molecules\n",
    "\n",
    "size, unique_molecules = calculate_stats(train_cids, val_cids, test_cids)\n",
    "train_pct = len(train_desc)/size*100\n",
    "val_pct = len(val_desc)/size*100\n",
    "test_pct = len(test_desc)/size*100\n",
    "\n",
    "# Combine all descriptions to calculate text statistics\n",
    "all_desc = {**train_desc, **val_desc, **test_desc}\n",
    "lengths = [len(desc) for desc in all_desc.values()]\n",
    "avg_length = np.mean(lengths)\n",
    "std_dev_length = np.std(lengths)\n",
    "median_length = np.median(lengths)\n",
    "max_length = np.max(lengths)\n",
    "min_length = np.min(lengths)\n",
    "\n",
    "print(f\"Total records: {size} molecule-text description pairs, comprised of {unique_molecules} distinct molecules.\")\n",
    "print(f\"Data split across datasets: {train_pct:.1f}% training, {val_pct:.1f}% validation, and {test_pct:.1f}% test.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e59C-YXmOdU5"
   },
   "source": [
    "Next, we examined the distribution of the length of the text descriptions of the molecules and the dataset skew towards longer descriptions. The average length of the text description is 314, with a max length of 1458 and min length of 88. This is likely because longer descriptions tend to be less noisy and more informative (Edwards et al., 2021), shown in Figure 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 524
    },
    "id": "CFZ4WMQ2v7lx",
    "outputId": "3bdec2b4-b449-43f9-89bb-dad90d0bb44e"
   },
   "outputs": [],
   "source": [
    "#@title Distribution Analysis: Text Description Lengths in the Dataset\n",
    "\n",
    "# text descriptions histogram\n",
    "plt.hist(lengths);\n",
    "plt.xlabel(\"Lengths of the Text Descriptions\");\n",
    "plt.ylabel(\"Number of Records\");\n",
    "plt.title(\"Figure 1: Histogram of Text Description Lengths Across the Dataset\");\n",
    "\n",
    "# text description statistics\n",
    "print(f\"Text descriptions statistics:\")\n",
    "print(f\"The average length is {avg_length:.1f} characters with a standard deviation of {std_dev_length:.1f} characters.\")\n",
    "print(f\"The median length is {median_length} characters, ranging from {min_length} to {max_length} characters.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTzeYAZrEQbb"
   },
   "source": [
    "#### 3.2.2.4 Sample Raw Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qda4mVK6piGj"
   },
   "source": [
    "We also sampled the raw data. The following shows the raw data of Glutaric Acid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wgv8omAkWNHf",
    "outputId": "76588200-b9f9-4050-dfd8-960aa03cc008"
   },
   "outputs": [],
   "source": [
    "#@title Example Data Entry: CID 743 - Glutaric Acid\n",
    "\n",
    "#Example raw data\n",
    "example_cid = '743'\n",
    "mol_length =len(train_mols[example_cid])\n",
    "example_text = train_desc[example_cid]\n",
    "\n",
    "print(f\"This is an example of the raw data:\")\n",
    "print(f\"CID is {example_cid},\")\n",
    "print(f\"its mol2vec embedding length is {mol_length},\")\n",
    "print(f\"and its text description is: '{example_text}'.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p2laXEJ5sZcl"
   },
   "source": [
    "We can also explore the molecular structure of CID 743 from the PubChem dataset at: [PubChem Dataset: CID 743](https://pubchem.ncbi.nlm.nih.gov/compound/743#section=2D-Structure)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GMWrPmJsfv7"
   },
   "source": [
    "![](https://drive.google.com/uc?id=14x__vjANi_o7RoOMCecXFDgx_WrdhcTu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2PHTGhxE6zBA"
   },
   "source": [
    "##  3.3 Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Yl4vbbXMoWqq"
   },
   "source": [
    "Based on the original article of Text2Mol (Edwards et al., 2021) and the implementation in Github repository (Edwards, 2021), three models are proposed and used in the paper including the multi-layer perceptron (MLP) model, the three-layer graph convolutional network (GCN) and the cross-modal attention modal. Each of these models will be discussed in detail in the subsequent sections.\n",
    "\n",
    "In this section, we will also provide detailed descriptions of our implementations for ablation studies and additional experiments that extend beyond the scope of the original paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3muyDPFPbozY"
   },
   "source": [
    "### 3.3.1 MLP Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKMlHWWVklgb"
   },
   "source": [
    "#### 3.3.1.1 Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zvI7e23ypsgG"
   },
   "source": [
    "  * Model architecture:\n",
    "    - layers:\n",
    "      - The MLP model has one linear layer (text_hidden1) to transform the input text descriptions into embeddings before passing them through the normalization layer\n",
    "      - Three linear layers are used to process the input molecule information before a normalization layer is applied\n",
    "    - activation function: the model uses the ReLU activation function for the 2 linear layers for molecule information processing\n",
    "  * Training objectives:\n",
    "    - loss function: the model uses the symmetric contrastive loss function where it considers the output from both the text description submodel and the molecule submodel. The model is optimized towards embeddings that better capture the similarity of the text descriptions and the molecules.\n",
    "    - optimizer: the model uses the Adam Optimizer\n",
    "  * Others:\n",
    "    - the model incorporates a pre-trained BERT-based transformer model to encode the input text descriptions into embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "m0qFxhszkryb"
   },
   "source": [
    "#### 3.3.1.2 Implementation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "brA7bbVWlU7t"
   },
   "source": [
    "The code block defines the `AblationOption` class, which encapsulates seven ablation strategies that we implemented and not originally in the paper. These include:\n",
    "\n",
    "- `normalization_layer_removal`: removing all normalization layers in the MLP model\n",
    "- `add_dropout`: adding a dropout rate of 0.5 in the MLP model\n",
    "- `hidden_layer_removal`: removing the second hidden layer of the molecule encoder in the MLP model\n",
    "- `max_pool`: changing the pooling method from mean pooling to maximum pooling in the GCN model\n",
    "- `conv_layer_removal`: removing the second convolutional layer in the GCN model\n",
    "- `change_loss`: changing the contrasive loss function to naive loss.\n",
    "- `text_length_ablation`: segmentating the dataset by text description length of 300 characters and individually examine their model performances.\n",
    "\n",
    "Besides, we also conducted two additional ablation studies as described in *4.4 Ablation Studies*.\n",
    "\n",
    "- changing batch sizes\n",
    "- changing learning rates\n",
    "\n",
    "This approach allows us to systematically explore the impact of different model architecture components and hyperparameters on model performance, providing insights that extend the foundational analysis presented in the original study."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8sSu6l_mnDRK"
   },
   "outputs": [],
   "source": [
    "# @title Ablations Overview: Implemented Ablation Options\n",
    "\n",
    "class AblationOption:\n",
    "  def __init__(self, normalization_layer_removal, max_pool, hidden_layer_removal, conv_layer_removal, add_dropout, change_loss, text_length_ablation):\n",
    "    self.normalization_layer_removal = normalization_layer_removal\n",
    "    self.max_pool = max_pool\n",
    "    self.hidden_layer_removal = hidden_layer_removal\n",
    "    self.conv_layer_removal = conv_layer_removal\n",
    "    self.add_dropout = add_dropout\n",
    "    self.change_loss = change_loss\n",
    "    self.text_length_ablation = text_length_ablation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZM96nblWo4xo"
   },
   "source": [
    "This code defines an `MLPModel` class, a multi-layer perceptron architecture, which integrates a text encoder using SciBERT and a molecule encoder. The ablation studies that we conducted within this model explore the effects of removing certain layers (hidden layers and normalization layers) and adding dropout, allowing for an evaluation of these components' contributions to model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gBdVZoTvsSFV"
   },
   "outputs": [],
   "source": [
    "# @title Implementation of MLP Model and Associated Ablation Options\n",
    "\n",
    "class MLPModel(nn.Module):\n",
    "    def __init__(self, ninp, nout, nhid, ablation_option):\n",
    "        super(MLPModel, self).__init__()\n",
    "\n",
    "        self.text_hidden1 = nn.Linear(ninp, nout)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.ablation_option = ablation_option\n",
    "\n",
    "        self.mol_hidden1 = nn.Linear(nout, nhid)\n",
    "\n",
    "        # Ablation Study: 'hidden_layer_removal'\n",
    "        # If 'True', the 2nd hidden layer of the molecule encoder will be removed.\n",
    "        if not ablation_option.hidden_layer_removal:\n",
    "            self.mol_hidden2 = nn.Linear(nhid, nhid)\n",
    "\n",
    "        self.mol_hidden3 = nn.Linear(nhid, nout)\n",
    "\n",
    "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
    "        self.register_parameter( 'temp' , self.temp )\n",
    "\n",
    "        # Ablation Study: 'add_dropout'\n",
    "        # If'True', define a dropout rate of 0.5.\n",
    "        if ablation_option.add_dropout:\n",
    "            self.dropout = nn.Dropout(0.5)\n",
    "\n",
    "        # Ablation Study: 'normalization_layer_removal'\n",
    "        # If 'True', remove all normalization layers.\n",
    "        if not ablation_option.normalization_layer_removal:\n",
    "            self.ln1 = nn.LayerNorm((nout))\n",
    "            self.ln2 = nn.LayerNorm((nout))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "\n",
    "        self.other_params = list(self.parameters())\n",
    "\n",
    "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        self.text_transformer_model.train()\n",
    "\n",
    "    def forward(self, text, molecule, text_mask = None):\n",
    "\n",
    "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
    "\n",
    "        text_x = text_encoder_output['pooler_output']\n",
    "\n",
    "        text_x = self.text_hidden1(text_x)\n",
    "\n",
    "        x = self.relu(self.mol_hidden1(molecule))\n",
    "\n",
    "        # Ablation Study: 'add_dropout'\n",
    "        # If 'True', add the predefined dropout.\n",
    "        if self.ablation_option.add_dropout:\n",
    "            x = self.dropout(x)\n",
    "\n",
    "        # Ablation Study: 'hidden_layer_removal'\n",
    "        # If 'True', output will not pass through the 2nd hidden layer with non-linear activation.\n",
    "        if not self.ablation_option.hidden_layer_removal:\n",
    "            x = self.relu(self.mol_hidden2(x))\n",
    "\n",
    "        x = self.mol_hidden3(x)\n",
    "\n",
    "        # Ablation Study: 'normalization_layer_removal'\n",
    "        # If 'True', output will not pass through any normalization layer of either the text or molecule encoders.\n",
    "        if not self.ablation_option.normalization_layer_removal:\n",
    "            x = self.ln1(x)\n",
    "            text_x = self.ln2(text_x)\n",
    "\n",
    "        x = x * torch.exp(self.temp)\n",
    "        text_x = text_x * torch.exp(self.temp)\n",
    "\n",
    "        return text_x, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NU6dJ9h14Tf2"
   },
   "source": [
    "#### 3.3.1.3 Pretrained MLP Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "emh8amEe9qzQ"
   },
   "source": [
    "All the pretrained MLP model checkpoints are available at:\n",
    "\n",
    "**All Model Checkpoints URL:** https://drive.google.com/drive/folders/1rBOlPcRzkQ-lf7qCSuWbFOT2fqc3GopY?usp=drive_link\n",
    "\n",
    "And we have loaded one MLP model checkpoint to demonstrate its evaluation process and results in the Results section.\n",
    "\n",
    "All the MLP model embeddings are available at:\n",
    "\n",
    "**All Model Embeddings URL:** https://drive.google.com/drive/folders/1mGnP54MZUGfq-sKa-XB_Yn1Uypx3Pgul?usp=drive_link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DZIjHJzD4ap"
   },
   "source": [
    "### 3.3.2 GCN Model\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2bN5PvbW-pS9"
   },
   "source": [
    "#### 3.3.2.1 Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i9FG0Qap0qp"
   },
   "source": [
    "  * Model architecture:\n",
    "    - layers:\n",
    "      - The GCN model has one linear layer (text_hidden1) to transform the input text descriptions into embeddings before passing them through the normalization layer\n",
    "     - Three Graph Convolutional Network (GCN) layers are used to generate the node embeddings from the graph data before a normalization layer is applied\n",
    "    - activation function: the model uses the ReLU activation function for the 2 linear layers for molecule information processing and the layers of the GCN\n",
    "  * Training objectives:\n",
    "    - loss function: the model uses the symmetric contrastive loss function where it considers the output from both the text description submodel and the molecule submodel. The model is optimized towards embeddings that better capture the similarity of the text descriptions and the molecules.\n",
    "    - optimizer: the model uses the Adam Optimizer\n",
    "  * Others:\n",
    "    - the model incorporates a pre-trained BERT-based transformer model to encode the input text descriptions into embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7MOEF8Ss-1Ji"
   },
   "source": [
    "#### 3.3.2.2 Implementation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fykC8UixfWzt"
   },
   "source": [
    "This code defines a `GCNModel` class, incorporating a graph convolutional network (GCN) for molecular encoding and a text encoder using SciBERT. The model integrates several layers for processing graph-based molecular data and textual information, utilizing layer normalization and nonlinear activations to enhance learning.\n",
    "\n",
    "The ablation studies implemented within this model include the removal of convolutional layers to assess their impact on the model's performance and an option to switch between mean and max pooling methods to investigate how different pooling strategies affect the final molecular representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "28CnLBkOD6DH"
   },
   "outputs": [],
   "source": [
    "# @title Implementation of GCN Model and Associated Ablation Options\n",
    "\n",
    "class GCNModel(nn.Module):\n",
    "    def __init__(self, num_node_features, ninp, nout, nhid, graph_hidden_channels,ablation_option):\n",
    "        super(GCNModel, self).__init__()\n",
    "\n",
    "        self.text_hidden1 = nn.Linear(ninp, nout)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.ablation_option = ablation_option\n",
    "\n",
    "        self.temp = nn.Parameter(torch.Tensor([0.07]))\n",
    "        self.register_parameter( 'temp' , self.temp )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm((nout))\n",
    "        self.ln2 = nn.LayerNorm((nout))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "\n",
    "        self.conv1 = GCNConv(num_node_features, graph_hidden_channels)\n",
    "\n",
    "        # Ablation Study: 'conv_layer_removal'\n",
    "        # If 'True', remove the 2nd convolutional layer of the molecule encoder.\n",
    "        if not ablation_option.conv_layer_removal:\n",
    "            self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "\n",
    "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
    "\n",
    "        self.mol_hidden2 = nn.Linear(nhid, nhid)\n",
    "        self.mol_hidden3 = nn.Linear(nhid, nout)\n",
    "\n",
    "        self.other_params = list(self.parameters())\n",
    "\n",
    "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        self.text_transformer_model.train()\n",
    "\n",
    "    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n",
    "\n",
    "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
    "\n",
    "        text_x = text_encoder_output['pooler_output']\n",
    "        text_x = self.text_hidden1(text_x)\n",
    "\n",
    "        x = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        batch = graph_batch.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "\n",
    "        # Ablation Study: 'conv_layer_removal'\n",
    "        # If 'True', output will not pass through the 2nd convolutional layer.\n",
    "        if not self.ablation_option.conv_layer_removal:\n",
    "            x = self.conv2(x, edge_index)\n",
    "            x = x.relu()\n",
    "\n",
    "\n",
    "        x = self.conv3(x, edge_index)\n",
    "\n",
    "        # Ablation study: 'max_pool'\n",
    "        # If 'True', changing global_mean_pool to global_max_pool.\n",
    "        if not self.ablation_option.max_pool:\n",
    "            x = global_mean_pool(x, batch)  # [batch_size, graph_hidden_channels]\n",
    "        else:\n",
    "            x = global_max_pool(x, batch)\n",
    "\n",
    "        x = self.mol_hidden1(x).relu()\n",
    "        x = self.mol_hidden2(x).relu()\n",
    "        x = self.mol_hidden3(x)\n",
    "\n",
    "        x = self.ln1(x)\n",
    "        text_x = self.ln2(text_x)\n",
    "\n",
    "        x = x * torch.exp(self.temp)\n",
    "        text_x = text_x * torch.exp(self.temp)\n",
    "\n",
    "        return text_x, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gPyktcVKBRiB"
   },
   "source": [
    "#### 3.3.2.3 Pretrained GCN Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Me_NOgUQfEzt"
   },
   "source": [
    "Similarly, All the pretrained GCN model checkpoints are available at:\n",
    "\n",
    "**All Model Checkpoints URL:** https://drive.google.com/drive/folders/1rBOlPcRzkQ-lf7qCSuWbFOT2fqc3GopY?usp=drive_link\n",
    "\n",
    "The training time for the GCN model is significantly longer than the MLP model. We will not be demonstrating the training process in the notebook due to the runtime limits.\n",
    "\n",
    "All the GCN model embeddings are available at:\n",
    "\n",
    "**All Model Embeddings URL:** https://drive.google.com/drive/folders/1mGnP54MZUGfq-sKa-XB_Yn1Uypx3Pgul?usp=drive_link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kX8N1m2vD-_D"
   },
   "source": [
    "###3.3.3 Attention Layer\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Iy4IE7RLB3mA"
   },
   "source": [
    "#### 3.3.3.1 Model Description"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I0pyceVyp8NN"
   },
   "source": [
    "  * Model architecture:\n",
    "    - layers:\n",
    "      - The Attention model has two linear layers to transform the input text descriptions into embeddings before passing them through the normalization layer\n",
    "     - Three Graph Convolutional Network (GCN) layers are used to generate the node embeddings from the graph data\n",
    "      - Two linear layers are used to process the node embedding outputs from the GCN before a normalization layer is applied\n",
    "     - A Transformer decoder with three layers. The decoder applies text to text attention to capture the relevant information within the text embeddings.\n",
    "    - activation function: the model uses the ReLU activation function for one linear layer for molecule information processing and the layers of the GCN. tanh is used as the activation function for the output of “text_hidden1” layer\n",
    "  * Training objectives:\n",
    "    - loss function: the model uses the symmetric contrastive loss function with negative sampling. The contrastive loss function considers the output from both the text description submodel and the molecule submodel. Negative sampling is used to minimize information leakage\n",
    "    - optimizer: the model uses the Adam Optimizer\n",
    "  * Others:\n",
    "    - the model incorporates a pre-trained BERT-based transformer model to encode the input text descriptions into embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TtmEEzdaCALi"
   },
   "source": [
    "#### 3.3.3.2 Implementation code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "s_Cy0dYdCPVI"
   },
   "source": [
    "This code defines an `AttentionModel` class that combines a transformer decoder for text processing with a graph convolutional network (GCN) for molecular data, enabling cross-modal learning by attending over graph-based molecular features and textual features from SciBERT embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "10gm9ltMD8nd"
   },
   "outputs": [],
   "source": [
    "# @title Attention Layer Implementation\n",
    "\n",
    "class AttentionModel(nn.Module):\n",
    "  # (Edwards, 2021).\n",
    "    def __init__(self, num_node_features, ninp, nout, nhid, nhead, nlayers, graph_hidden_channels, mol_trunc_length, temp, dropout=0.5):\n",
    "        super(AttentionModel, self).__init__()\n",
    "\n",
    "        self.text_hidden1 = nn.Linear(ninp, nhid)\n",
    "        self.text_hidden2 = nn.Linear(nhid, nout)\n",
    "\n",
    "        self.ninp = ninp\n",
    "        self.nhid = nhid\n",
    "        self.nout = nout\n",
    "        self.num_node_features = num_node_features\n",
    "        self.graph_hidden_channels = graph_hidden_channels\n",
    "        self.mol_trunc_length = mol_trunc_length\n",
    "\n",
    "        self.drop = nn.Dropout(p=dropout)\n",
    "\n",
    "        decoder_layers = TransformerDecoderLayer(ninp, nhead, nhid, dropout)\n",
    "        self.text_transformer_decoder = TransformerDecoder(decoder_layers, nlayers)\n",
    "\n",
    "        self.temp = nn.Parameter(torch.Tensor([temp]))\n",
    "        self.register_parameter( 'temp' , self.temp )\n",
    "\n",
    "        self.ln1 = nn.LayerNorm((nout))\n",
    "        self.ln2 = nn.LayerNorm((nout))\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "        self.selu = nn.SELU()\n",
    "\n",
    "        #For GCN:\n",
    "        self.conv1 = GCNConv(self.num_node_features, graph_hidden_channels)\n",
    "        self.conv2 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "        self.conv3 = GCNConv(graph_hidden_channels, graph_hidden_channels)\n",
    "        self.mol_hidden1 = nn.Linear(graph_hidden_channels, nhid)\n",
    "        self.mol_hidden2 = nn.Linear(nhid, nout)\n",
    "\n",
    "        self.other_params = list(self.parameters()) #get all but bert params\n",
    "\n",
    "        self.text_transformer_model = BertModel.from_pretrained('allenai/scibert_scivocab_uncased')\n",
    "        self.text_transformer_model.train()\n",
    "\n",
    "        self.device = 'cpu'\n",
    "\n",
    "    def set_device(self, dev):\n",
    "        self.to(dev)\n",
    "        self.device = dev\n",
    "\n",
    "    def forward(self, text, graph_batch, text_mask = None, molecule_mask = None):\n",
    "\n",
    "        text_encoder_output = self.text_transformer_model(text, attention_mask = text_mask)\n",
    "\n",
    "        #Obtain node embeddings\n",
    "        x = graph_batch.x\n",
    "        edge_index = graph_batch.edge_index\n",
    "        batch = graph_batch.batch\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = self.conv2(x, edge_index)\n",
    "        x = x.relu()\n",
    "        mol_x = self.conv3(x, edge_index)\n",
    "\n",
    "        #turn pytorch geometric output into the correct format for transformer\n",
    "        #requires recovering the nodes from each graph into a separate dimension\n",
    "        node_features = torch.zeros((graph_batch.num_graphs, self.mol_trunc_length, self.graph_hidden_channels)).to(self.device)\n",
    "        for i, p in enumerate(graph_batch.ptr):\n",
    "          if p == 0:\n",
    "            old_p = p\n",
    "            continue\n",
    "          node_features[i - 1, :p-old_p, :] = mol_x[old_p:torch.min(p, old_p + self.mol_trunc_length), :]\n",
    "          old_p = p\n",
    "        node_features = torch.transpose(node_features, 0, 1)\n",
    "\n",
    "        text_output = self.text_transformer_decoder(text_encoder_output['last_hidden_state'].transpose(0,1), node_features,\n",
    "                                                            tgt_key_padding_mask = text_mask == 0, memory_key_padding_mask = ~molecule_mask)\n",
    "\n",
    "        #Readout layer\n",
    "        x = global_mean_pool(mol_x, batch)  # [batch_size, graph_hidden_channels]\n",
    "\n",
    "        x = self.mol_hidden1(x)\n",
    "        x = x.relu()\n",
    "        x = self.mol_hidden2(x)\n",
    "\n",
    "        text_x = torch.tanh(self.text_hidden1(text_output[0,:,:])) #[CLS] pooler\n",
    "        text_x = self.text_hidden2(text_x)\n",
    "\n",
    "        x = self.ln1(x)\n",
    "        text_x = self.ln2(text_x)\n",
    "\n",
    "        x = x * torch.exp(self.temp)\n",
    "        text_x = text_x * torch.exp(self.temp)\n",
    "\n",
    "        return text_x, x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G28d8YejDJ9J"
   },
   "source": [
    "#### 3.3.3.3 Pretrained Attention Layer Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jvZxx4LLaE2T"
   },
   "source": [
    "The training time for the Attention model is significantly longer than the MLP model. We will not be demonstrating the training process in the notebook due to the runtime limits. All the pretrained attention model checkpoints are available at:\n",
    "\n",
    "**All Model Checkpoints URL:** https://drive.google.com/drive/folders/1rBOlPcRzkQ-lf7qCSuWbFOT2fqc3GopY?usp=drive_link\n",
    "\n",
    "And the mha_weights.pkl is accessble through the public GCS bucket as mentioned in the \"Getting Started\" section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UoSpxnEbYmN"
   },
   "source": [
    "## 3.4 Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9NXyFpDCpTEM"
   },
   "source": [
    "### 3.4.1 Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-GnnP7HLqDlI"
   },
   "source": [
    "* **Learning Rate:** 3e-5(SciBERT) and 1e-4.\n",
    "* **Batch size:** 32\n",
    "* **Hidden layers:** 3(MLP), 3(GCN)\n",
    "* **Drop out:** None.\n",
    "\n",
    "In the original \"Text2Mol\" paper, the authors specify a set of hyperparameters for training their models. They use a finetuning learning rate of 3e-5 for the SciBERT model while the rest of the model components are trained with a learning rate of 1e-4. The batch size for training is set at 32, and both the MLP and GCN models consist of 3 hidden layers each. No dropout is applied in the training process, which suggests an emphasis on maintaining all features during the learning phase.\n",
    "\n",
    "In our ablation studies, we adjust these hyperparameters to investigate their impact on model performance. Specifically, we halve the learning rates, setting the finetuning rate for SciBERT to 1.5e-5 and the rate for the rest of the model to 5e-5. Additionally, we reduce the batch size to 16 to explore the effects of a smaller batch size on the training dynamics and model generalization. These modifications are intended to test the robustness and sensitivity of the models to changes in training configurations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UWaVpvTcArde"
   },
   "source": [
    "### 3.4.2 Computational Requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JYtRTLgmp4Mk"
   },
   "source": [
    "* **Total number of trials:** 20 (see [All Model Checkpoints](https://drive.google.com/drive/folders/1rBOlPcRzkQ-lf7qCSuWbFOT2fqc3GopY?usp=drive_link) )\n",
    "  - 6 runs of the baseline models\n",
    "  - 11 runs of the ablation studies\n",
    "  - 1 run of the attention model\n",
    "  - 2 runs of additional experiments.\n",
    "\n",
    "  - In addition, 13 runs of ensembles, 3 runs of the cross-modal attention and FPGrowth association rules, 2 runs of additional experiments (ensemble strategy exploration).\n",
    "* **Training epochs:** 40 (Except that in our ablation study on early stopping, we trained three MLP models, halting their training at epochs 15, 20, and 25 respectively.)\n",
    "* **Type of Hardware:** All experiments were conducted on the UIUC Campus Cluster utilizing a single Nvidia A5000 GPU, 4 CPUs per task, and 32GB of memory.\n",
    "* **Average runtime for each epoch:**\n",
    "  - MLP model: 314 s\n",
    "  - GCN model: 659 s\n",
    "  - Attention model: 967 s\n",
    "* **GPU hrs used:**  approximately 96.7 hours\n",
    "  - MLP model: 3.5 hours per trial, 14 trials, totalling 49.0 hours\n",
    "  - GCN model: 7.4 hours per trial, 5 trials, totalling 37.0 hours\n",
    "  - Attention layer: 10.7 hours per trial\n",
    "* **Actural computational time** for each baseline MLP and GCN model is shown in Table 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GHS83a9qzNLo"
   },
   "outputs": [],
   "source": [
    "# @title Summarize Actual Runtimes for Model Training and Validation\n",
    "def ReadModelTrainingLog(filename):\n",
    "    res = pd.read_csv(filename)\n",
    "    return res\n",
    "\n",
    "def GenerateLogTable(model_log_file_map):\n",
    "    table = None\n",
    "    for model_name, file_name in model_log_file_map.items():\n",
    "        res = ReadModelTrainingLog(file_name)\n",
    "\n",
    "        train_time = res['Train Time'].sum()\n",
    "        valid_time = res['Valid Time'].sum()\n",
    "\n",
    "        row = pd.DataFrame({\n",
    "                'Model': [model_name],\n",
    "                'Train time': [train_time],\n",
    "                'Valid time': [valid_time],\n",
    "            })\n",
    "\n",
    "        if table is None:\n",
    "            table = row\n",
    "        else:\n",
    "            table = pd.concat([table, row], ignore_index=True)\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 497
    },
    "id": "LZkQ2zbZy80u",
    "outputId": "dd072af7-5e47-49c6-ecbd-901770111093"
   },
   "outputs": [],
   "source": [
    "model_log_filenames1 = {\n",
    "    'MLP1': '/content/tar_files/training loss_validation loss_computation_time/mlp1.csv',\n",
    "    'MLP2': '/content/tar_files/training loss_validation loss_computation_time/mlp2.csv',\n",
    "    'MLP3': '/content/tar_files/training loss_validation loss_computation_time/mlp3.csv',\n",
    "    'GCN1': '/content/tar_files/training loss_validation loss_computation_time/gcn1.csv',\n",
    "    'GCN2': '/content/tar_files/training loss_validation loss_computation_time/gcn2.csv',\n",
    "    'GCN3': '/content/tar_files/training loss_validation loss_computation_time/gcn3.csv',\n",
    "    'Ablation: add dropput': '/content/tar_files/training loss_validation loss_computation_time/ablation_add_dropout_rate_0.5_mlp.csv',\n",
    "    'Ablation: change pooling': '/content/tar_files/training loss_validation loss_computation_time/ablation_max_pool_gcn.csv',\n",
    "    'Ablation: reduce batch size': '/content/tar_files/training loss_validation loss_computation_time/ablation_reduce_batch_size_16_mlp.csv',\n",
    "    'Ablation: change learning rate': '/content/tar_files/training loss_validation loss_computation_time/ablation_reduce_learning_rate_mlp.csv',\n",
    "    'Ablation: remove convolutional layer': '/content/tar_files/training loss_validation loss_computation_time/ablation_remove_convolutional_layer_2_gcn.csv',\n",
    "    'Ablation: remove hidden layer': '/content/tar_files/training loss_validation loss_computation_time/ablation_remove_molecule_encoder_hidden_layer_2_mlp.csv',\n",
    "    'Ablation: remove normalization layers': '/content/tar_files/training loss_validation loss_computation_time/ablation_remove_normalization_layers_mlp.csv',\n",
    "}\n",
    "\n",
    "table1 = GenerateLogTable(model_log_filenames1)\n",
    "display(HTML('<h3>Table 1. Actural Model Training and Validation Time (s) </h3>'))\n",
    "table1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kTN2OIwqrTm3"
   },
   "source": [
    "### 3.4.3 Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HgslOtppr81K"
   },
   "source": [
    "#### 3.4.3.1 Losses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBD9HxuTNGp9"
   },
   "source": [
    "We conducted an ablation study which changes the loss function to `naive_loss` function, computing cross-entropy loss using only direct matches between vectors `v1` and `v2`, unlike `contrastive_loss` which also considers reciprocal matches to ensure symmetry in similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7nkx-Oukrqxd"
   },
   "outputs": [],
   "source": [
    "# @title Implementation of Losses and Associated Ablation Option\n",
    "\n",
    "CE = nn.CrossEntropyLoss()\n",
    "\n",
    "# Ablation Study: 'change_loss'\n",
    "# define naive_loss using only direct matches between v1 and v2.\n",
    "def naive_loss(v1, v2):\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "  return CE(logits, labels)\n",
    "\n",
    "def contrastive_loss(v1, v2):\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  labels = torch.arange(logits.shape[0], device=v1.device)\n",
    "  return CE(logits, labels) + CE(torch.transpose(logits, 0, 1), labels)\n",
    "\n",
    "BCEL = nn.BCEWithLogitsLoss()\n",
    "\n",
    "def negative_sampling_contrastive_loss(v1, v2, labels):\n",
    "  logits = torch.matmul(v1,torch.transpose(v2, 0, 1))\n",
    "  eye = torch.diag_embed(labels).to(v1.device)\n",
    "  return BCEL(logits, eye) + BCEL(torch.transpose(logits, 0, 1), eye), logits.diag() > 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fXS_JKpssDwZ"
   },
   "source": [
    "#### 3.4.3.2 Dataloaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bk5R5fw5SUuB"
   },
   "source": [
    "This code implements an extended study not in the original paper to examine the impact of text length on model performance. It processes text descriptions based on their length:\n",
    "\n",
    "1. **Long Texts**: If `text_length_ablation` is set to \"long\", only descriptions longer than 300 characters are processed.\n",
    "2. **Short Texts**: If set to \"short\", it processes descriptions of 300 characters or fewer.\n",
    "3. **No Restriction**: Without specific settings, it processes all descriptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FPjgjHjeSgLT"
   },
   "outputs": [],
   "source": [
    "#@title Implementation of Dataloaders and Additional Experiment of Various Text Lengths\n",
    "\n",
    "class GenerateData():\n",
    "  def __init__(self, text_trunc_length, path_train, path_val, path_test, path_molecules, path_token_embs, text_length_ablation):\n",
    "    self.path_train = path_train\n",
    "    self.path_val = path_val\n",
    "    self.path_test = path_test\n",
    "    self.path_molecules = path_molecules\n",
    "    self.path_token_embs = path_token_embs\n",
    "\n",
    "    self.text_trunc_length = text_trunc_length\n",
    "    self.text_length_ablation = text_length_ablation\n",
    "\n",
    "    self.prep_text_tokenizer()\n",
    "\n",
    "    if path_molecules is not None:\n",
    "      self.load_substructures()\n",
    "\n",
    "    self.store_descriptions()\n",
    "\n",
    "  def load_substructures(self):\n",
    "    self.molecule_sentences = {}\n",
    "    self.molecule_tokens = {}\n",
    "\n",
    "    total_tokens = set()\n",
    "    self.max_mol_length = 0\n",
    "    with open(self.path_molecules) as f:\n",
    "      for line in f:\n",
    "        spl = line.split(\":\")\n",
    "        cid = spl[0]\n",
    "        tokens = spl[1].strip()\n",
    "        self.molecule_sentences[cid] = tokens\n",
    "        t = tokens.split()\n",
    "        total_tokens.update(t)\n",
    "        size = len(t)\n",
    "        if size > self.max_mol_length: self.max_mol_length = size\n",
    "\n",
    "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
    "\n",
    "  def prep_text_tokenizer(self):\n",
    "    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "  def store_descriptions(self):\n",
    "    self.descriptions = {}\n",
    "\n",
    "    self.mols = {}\n",
    "\n",
    "    self.training_cids = []\n",
    "\n",
    "    with open(self.path_train) as f:\n",
    "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "      for n, line in enumerate(reader):\n",
    "\n",
    "        # Additional Experiment Beyond Original Paper: Text Length\n",
    "        # If text_length_ablation is set to \"long\", only process descriptions longer than 300 characters\n",
    "        if self.text_length_ablation == \"long\":\n",
    "          if len(line['desc']) > 300:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.training_cids.append(line['cid'])\n",
    "\n",
    "        # If text_length_ablation is set to \"short\", only process descriptions that are 300 characters or shorter\n",
    "        elif self.text_length_ablation == \"short\":\n",
    "          if len(line['desc']) <= 300:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.training_cids.append(line['cid'])\n",
    "\n",
    "        # If no specific text length ablation is set, process all descriptions\n",
    "        else:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.training_cids.append(line['cid'])\n",
    "\n",
    "    self.validation_cids = []\n",
    "\n",
    "    with open(self.path_val) as f:\n",
    "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "      for n, line in enumerate(reader):\n",
    "        if self.text_length_ablation == \"long\":\n",
    "          if len(line['desc']) > 300:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.validation_cids.append(line['cid'])\n",
    "        elif self.text_length_ablation == \"short\":\n",
    "          if len(line['desc']) <= 300:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.validation_cids.append(line['cid'])\n",
    "        else:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.validation_cids.append(line['cid'])\n",
    "\n",
    "    self.test_cids = []\n",
    "\n",
    "    with open(self.path_test) as f:\n",
    "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "      for n, line in enumerate(reader):\n",
    "        if self.text_length_ablation == \"long\":\n",
    "          if len(line['desc']) > 300:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.test_cids.append(line['cid'])\n",
    "        elif self.text_length_ablation == \"short\":\n",
    "          if len(line['desc']) <= 300:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.test_cids.append(line['cid'])\n",
    "        else:\n",
    "            self.descriptions[line['cid']] = line['desc']\n",
    "            self.mols[line['cid']] = line['mol2vec']\n",
    "            self.test_cids.append(line['cid'])\n",
    "\n",
    "    self.test_cids_sample  = self.test_cids\n",
    "\n",
    "  def generate_examples_train(self):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "    np.random.shuffle(self.training_cids)\n",
    "\n",
    "    for cid in self.training_cids:\n",
    "      text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length,\n",
    "                                        padding='max_length', return_tensors = 'np')\n",
    "\n",
    "      yield {\n",
    "          'cid': cid,\n",
    "          'input': {\n",
    "              'text': {\n",
    "                'input_ids': text_input['input_ids'].squeeze(),\n",
    "                'attention_mask': text_input['attention_mask'].squeeze(),\n",
    "              },\n",
    "              'molecule' : {\n",
    "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "                    'cid' : cid\n",
    "              },\n",
    "          },\n",
    "      }\n",
    "\n",
    "  def generate_examples_val(self):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "    np.random.shuffle(self.validation_cids)\n",
    "\n",
    "    for cid in self.validation_cids:\n",
    "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "                                         max_length=self.text_trunc_length, return_tensors = 'np')\n",
    "\n",
    "        mol_input = []\n",
    "\n",
    "        yield {\n",
    "            'cid': cid,\n",
    "            'input': {\n",
    "                'text': {\n",
    "                  'input_ids': text_input['input_ids'].squeeze(),\n",
    "                  'attention_mask': text_input['attention_mask'].squeeze(),\n",
    "                },\n",
    "                'molecule' : {\n",
    "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "                    'cid' : cid\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "\n",
    "  def generate_examples_test(self):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "    np.random.shuffle(self.test_cids)\n",
    "\n",
    "    for cid in self.test_cids:\n",
    "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "                                         max_length=self.text_trunc_length, return_tensors = 'np')\n",
    "\n",
    "        mol_input = []\n",
    "\n",
    "        yield {\n",
    "            'cid': cid,\n",
    "            'input': {\n",
    "                'text': {\n",
    "                  'input_ids': text_input['input_ids'].squeeze(),\n",
    "                  'attention_mask': text_input['attention_mask'].squeeze(),\n",
    "                },\n",
    "                'molecule' : {\n",
    "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "                    'cid' : cid\n",
    "                }\n",
    "            },\n",
    "        }\n",
    "\n",
    "class MolDataset(Dataset):\n",
    "  'PyTorch dataset for MLP version of model'\n",
    "  def __init__(self, gen, length):\n",
    "      'Initialization'\n",
    "\n",
    "      self.gen = gen\n",
    "      self.it = iter(self.gen())\n",
    "\n",
    "      self.length = length\n",
    "\n",
    "  def __len__(self):\n",
    "      'Denotes the total number of samples'\n",
    "      return self.length\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      'Generates one sample of data'\n",
    "\n",
    "      try:\n",
    "        ex = next(self.it)\n",
    "      except StopIteration:\n",
    "        self.it = iter(self.gen())\n",
    "        ex = next(self.it)\n",
    "\n",
    "      X = ex['input']\n",
    "      y = 1\n",
    "\n",
    "      return X, y\n",
    "\n",
    "\n",
    "def get_dataloader(data_generator, params):\n",
    "\n",
    "    training_set = MolDataset(data_generator.generate_examples_train, len(data_generator.training_cids))\n",
    "    validation_set = MolDataset(data_generator.generate_examples_val, len(data_generator.validation_cids))\n",
    "    test_set = MolDataset(data_generator.generate_examples_test, len(data_generator.test_cids))\n",
    "\n",
    "    training_generator = DataLoader(training_set,\n",
    "                                    **params)\n",
    "    validation_generator = DataLoader(validation_set,\n",
    "                                      **params)\n",
    "    test_generator = DataLoader(test_set, **params)\n",
    "\n",
    "    return training_generator, validation_generator, test_generator\n",
    "\n",
    "\n",
    "class MoleculeGraphDataset(GeoDataset):\n",
    "    def __init__(self, root, cids, data_path, gt, transform=None, pre_transform=None):\n",
    "        self.cids = cids\n",
    "        self.data_path = data_path\n",
    "        self.gt = gt\n",
    "        super(MoleculeGraphDataset, self).__init__(root, transform, pre_transform)\n",
    "\n",
    "        self.idx_to_cid = {}\n",
    "        i = 0\n",
    "        for raw_path in self.raw_paths:\n",
    "            cid = int(raw_path.split('/')[-1][:-6])\n",
    "            self.idx_to_cid[i] = cid\n",
    "            i += 1\n",
    "\n",
    "    @property\n",
    "    def raw_file_names(self):\n",
    "        return [cid + \".graph\" for cid in self.cids]\n",
    "\n",
    "    @property\n",
    "    def processed_file_names(self):\n",
    "        return ['data_{}.pt'.format(cid) for cid in self.cids]\n",
    "\n",
    "    def download(self):\n",
    "        # Download to `self.raw_dir`.\n",
    "        print(self.raw_dir)\n",
    "        print(osp.join(self.raw_dir, \"mol_graphs.zip\"))\n",
    "        print(osp.exists(osp.join(self.raw_dir, \"mol_graphs.zip\")))\n",
    "        if not osp.exists(osp.join(self.raw_dir, \"mol_graphs.zip\")):\n",
    "            shutil.copy(self.data_path, os.path.join(self.raw_dir, \"mol_graphs.zip\"))\n",
    "\n",
    "    def process_graph(self, raw_path):\n",
    "      edge_index  = []\n",
    "      x = []\n",
    "      with open(raw_path, 'r') as f:\n",
    "        next(f)\n",
    "        for line in f: #edges\n",
    "          if line != \"\\n\":\n",
    "            edge = *map(int, line.split()),\n",
    "            edge_index.append(edge)\n",
    "          else:\n",
    "            break\n",
    "        next(f)\n",
    "        for line in f: #get mol2vec features:\n",
    "          substruct_id = line.strip().split()[-1]\n",
    "          if substruct_id in self.gt.token_embs:\n",
    "            x.append(self.gt.token_embs[substruct_id])\n",
    "          else:\n",
    "            x.append(self.gt.token_embs['UNK'])\n",
    "\n",
    "        return torch.LongTensor(edge_index).T, torch.FloatTensor(x)\n",
    "\n",
    "    def process(self):\n",
    "\n",
    "        with zipfile.ZipFile(osp.join(self.raw_dir, \"mol_graphs.zip\"), 'r') as zip_ref:\n",
    "            zip_ref.extractall(self.raw_dir)\n",
    "\n",
    "        i = 0\n",
    "        for raw_path in self.raw_paths:\n",
    "            # Read data from `raw_path`.\n",
    "\n",
    "            cid = int(raw_path.split('/')[-1][:-6])\n",
    "\n",
    "            edge_index, x = self.process_graph(raw_path)\n",
    "            data = Data(x=x, edge_index = edge_index)\n",
    "\n",
    "            if self.pre_filter is not None and not self.pre_filter(data):\n",
    "                continue\n",
    "\n",
    "            if self.pre_transform is not None:\n",
    "                data = self.pre_transform(data)\n",
    "\n",
    "            torch.save(data, osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
    "            i += 1\n",
    "\n",
    "    def len(self):\n",
    "        return len(self.processed_file_names)\n",
    "\n",
    "    def get(self, idx):\n",
    "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(self.idx_to_cid[idx])))\n",
    "        return data\n",
    "\n",
    "    def get_cid(self, cid):\n",
    "        data = torch.load(osp.join(self.processed_dir, 'data_{}.pt'.format(cid)))\n",
    "        return data\n",
    "\n",
    "class CustomGraphCollater(object):\n",
    "    def __init__(self, dataset, follow_batch = [], exclude_keys = []):\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "        self.dataset = dataset\n",
    "\n",
    "    def collate(self, batch):\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, Data):\n",
    "            return Batch.from_data_list(batch)\n",
    "\n",
    "        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n",
    "\n",
    "    def __call__(self, cids):\n",
    "\n",
    "        return self.collate([self.dataset.get_cid(int(cid)) for cid in cids])\n",
    "\n",
    "\n",
    "def get_graph_data(data_generator, graph_data_path):\n",
    "    root = osp.join(graph_data_path[:-len(osp.basename(graph_data_path))], 'graph-data/')\n",
    "    #graph_data_path = \"input/mol_graphs.zip\"\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "\n",
    "    mg_data_tr = MoleculeGraphDataset(root, data_generator.training_cids, graph_data_path, data_generator)\n",
    "    graph_batcher_tr = CustomGraphCollater(mg_data_tr)\n",
    "\n",
    "    mg_data_val = MoleculeGraphDataset(root, data_generator.validation_cids, graph_data_path, data_generator)\n",
    "    graph_batcher_val = CustomGraphCollater(mg_data_val)\n",
    "\n",
    "    mg_data_test = MoleculeGraphDataset(root, data_generator.test_cids, graph_data_path, data_generator)\n",
    "    graph_batcher_test = CustomGraphCollater(mg_data_test)\n",
    "\n",
    "    return graph_batcher_tr, graph_batcher_val, graph_batcher_test\n",
    "\n",
    "#Loaders for attention association rule models\n",
    "\n",
    "class GenerateDataAttention():\n",
    "  def __init__(self, text_trunc_length, path_train, path_val, path_test, path_molecules, path_token_embs, sample):\n",
    "    self.path_train = path_train\n",
    "    self.path_val = path_val\n",
    "    self.path_test = path_test\n",
    "    self.path_molecules = path_molecules\n",
    "    self.path_token_embs = path_token_embs\n",
    "\n",
    "    self.text_trunc_length = text_trunc_length\n",
    "\n",
    "    self.sample = sample\n",
    "\n",
    "    self.prep_text_tokenizer()\n",
    "\n",
    "    self.load_substructures()\n",
    "\n",
    "    self.store_descriptions()\n",
    "\n",
    "  def load_substructures(self):\n",
    "    self.molecule_sentences = {}\n",
    "    self.molecule_tokens = {}\n",
    "\n",
    "    total_tokens = set()\n",
    "    self.max_mol_length = 0\n",
    "    with open(self.path_molecules) as f:\n",
    "      for line in f:\n",
    "        spl = line.split(\":\")\n",
    "        cid = spl[0]\n",
    "        tokens = spl[1].strip()\n",
    "        self.molecule_sentences[cid] = tokens\n",
    "        t = tokens.split()\n",
    "        total_tokens.update(t)\n",
    "        size = len(t)\n",
    "        if size > self.max_mol_length: self.max_mol_length = size\n",
    "\n",
    "    self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
    "\n",
    "  def prep_text_tokenizer(self):\n",
    "    self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "  def store_descriptions(self):\n",
    "    self.descriptions = {}\n",
    "\n",
    "    self.mols = {}\n",
    "\n",
    "    self.training_cids = []\n",
    "    #get training set cids...\n",
    "    with open(self.path_train) as f:\n",
    "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "      for n, line in enumerate(reader):\n",
    "        self.descriptions[line['cid']] = line['desc']\n",
    "        self.mols[line['cid']] = line['mol2vec']\n",
    "        self.training_cids.append(line['cid'])\n",
    "\n",
    "    if self.sample == True:\n",
    "          self.training_cids  = random.sample(self.training_cids, int(len(self.training_cids)/10))\n",
    "\n",
    "    self.validation_cids = []\n",
    "    #get validation set cids...\n",
    "    with open(self.path_val) as f:\n",
    "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "      for n, line in enumerate(reader):\n",
    "        self.descriptions[line['cid']] = line['desc']\n",
    "        self.mols[line['cid']] = line['mol2vec']\n",
    "        self.validation_cids.append(line['cid'])\n",
    "\n",
    "    if self.sample == True:\n",
    "          self.validation_cids  = random.sample(self.validation_cids, int(len(self.validation_cids)/10))\n",
    "\n",
    "    self.test_cids = []\n",
    "    with open(self.path_test) as f:\n",
    "      reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "      for n, line in enumerate(reader):\n",
    "        self.descriptions[line['cid']] = line['desc']\n",
    "        self.mols[line['cid']] = line['mol2vec']\n",
    "        self.test_cids.append(line['cid'])\n",
    "\n",
    "    if self.sample == True:\n",
    "          self.test_cids  = random.sample(self.test_cids, int(len(self.test_cids)/10))\n",
    "\n",
    "  #transformers can't take array with full attention so have to pad a 0...\n",
    "  def padarray(self, A, size, value=0):\n",
    "      t = size - len(A)\n",
    "      return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n",
    "\n",
    "\n",
    "  def generate_examples_train(self):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "    np.random.shuffle(self.training_cids)\n",
    "\n",
    "    for cid in self.training_cids:\n",
    "      label = np.random.randint(2)\n",
    "      rand_cid = np.random.choice(self.training_cids)\n",
    "      if label:\n",
    "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "                                        padding='max_length', return_tensors = 'np')\n",
    "      else:\n",
    "        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "                                        padding='max_length', return_tensors = 'np')\n",
    "\n",
    "      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "      yield {\n",
    "          'cid': cid,\n",
    "          'input': {\n",
    "              'text': {\n",
    "                'input_ids': text_ids,\n",
    "                'attention_mask': text_mask,\n",
    "              },\n",
    "              'molecule' : {\n",
    "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "                    'cid' : cid\n",
    "              },\n",
    "          },\n",
    "          'label': label\n",
    "      }\n",
    "\n",
    "\n",
    "  def generate_examples_val(self):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "    np.random.shuffle(self.validation_cids)\n",
    "\n",
    "    for cid in self.validation_cids:\n",
    "      label = np.random.randint(2)\n",
    "      rand_cid = np.random.choice(self.validation_cids)\n",
    "      if label:\n",
    "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "                                        padding='max_length', return_tensors = 'np')\n",
    "      else:\n",
    "        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "                                        padding='max_length', return_tensors = 'np')\n",
    "\n",
    "\n",
    "      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "      yield {\n",
    "          'cid': cid,\n",
    "          'input': {\n",
    "              'text': {\n",
    "                'input_ids': text_ids,\n",
    "                'attention_mask': text_mask,\n",
    "              },\n",
    "              'molecule' : {\n",
    "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "                    'cid' : cid\n",
    "              },\n",
    "          },\n",
    "          'label': label\n",
    "      }\n",
    "\n",
    "  def generate_examples_test(self):\n",
    "    \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "    np.random.shuffle(self.test_cids)\n",
    "\n",
    "    for cid in self.test_cids:\n",
    "      label = np.random.randint(2)\n",
    "      rand_cid = np.random.choice(self.test_cids)\n",
    "      if label:\n",
    "        text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "                                        padding='max_length', return_tensors = 'np')\n",
    "      else:\n",
    "        text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "                                        padding='max_length', return_tensors = 'np')\n",
    "\n",
    "\n",
    "      text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "      text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "      yield {\n",
    "          'cid': cid,\n",
    "          'input': {\n",
    "              'text': {\n",
    "                'input_ids': text_ids,\n",
    "                'attention_mask': text_mask,\n",
    "              },\n",
    "              'molecule' : {\n",
    "                    'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "                    'cid' : cid\n",
    "              },\n",
    "          },\n",
    "          'label': label\n",
    "      }\n",
    "\n",
    "\n",
    "class AttentionDataset(Dataset):\n",
    "  'Characterizes a dataset for PyTorch'\n",
    "  def __init__(self, gen, length):\n",
    "      'Initialization'\n",
    "\n",
    "      self.gen = gen\n",
    "      self.it = iter(self.gen())\n",
    "\n",
    "      self.length = length\n",
    "\n",
    "  def __len__(self):\n",
    "      'Denotes the total number of samples'\n",
    "      return self.length\n",
    "\n",
    "\n",
    "  def __getitem__(self, index):\n",
    "      'Generates one sample of data'\n",
    "\n",
    "      try:\n",
    "        ex = next(self.it)\n",
    "      except StopIteration:\n",
    "        self.it = iter(self.gen())\n",
    "        ex = next(self.it)\n",
    "\n",
    "      X = ex['input']\n",
    "      y = ex['label']\n",
    "\n",
    "      return X, y\n",
    "\n",
    "\n",
    "\n",
    "class AttentionGraphCollater(object):\n",
    "    def __init__(self, dataset, mask_len, follow_batch = [], exclude_keys = []):\n",
    "        self.follow_batch = follow_batch\n",
    "        self.exclude_keys = exclude_keys\n",
    "        self.dataset = dataset\n",
    "        self.mask_len = mask_len\n",
    "        self.mask_indices = np.array(range(mask_len))\n",
    "\n",
    "    def generate_mask(self, sz):\n",
    "        rv = torch.zeros((self.mask_len), dtype = torch.bool)\n",
    "        rv = rv.masked_fill(torch.BoolTensor(self.mask_indices < sz), bool(1)) #pytorch transformer input version\n",
    "        rv[-1] = 0 #set last value to 0 because pytorch can't handle all 1s\n",
    "        return rv\n",
    "\n",
    "    def get_masks(self, batch):\n",
    "      return torch.stack([self.generate_mask(b.x.shape[0]) for b in batch])\n",
    "\n",
    "    def collate(self, batch):\n",
    "        elem = batch[0]\n",
    "        if isinstance(elem, Data):\n",
    "            return Batch.from_data_list(batch)\n",
    "\n",
    "        raise TypeError('DataLoader found invalid type: {}'.format(type(elem)))\n",
    "\n",
    "    def __call__(self, cids):\n",
    "\n",
    "        tmp = [self.dataset.get_cid(int(cid)) for cid in cids]\n",
    "        return self.collate(tmp), self.get_masks(tmp)\n",
    "\n",
    "def get_attention_graph_data(data_generator, graph_data_path, mol_trunc_length):\n",
    "    root = osp.join(graph_data_path[:-len(osp.basename(graph_data_path))], 'graph-data/')\n",
    "    #graph_data_path = \"input/mol_graphs.zip\"\n",
    "    if not os.path.exists(root):\n",
    "        os.mkdir(root)\n",
    "\n",
    "    mg_data_tr = MoleculeGraphDataset(root, data_generator.training_cids, graph_data_path, data_generator)\n",
    "    graph_batcher_tr = AttentionGraphCollater(mg_data_tr, mol_trunc_length)\n",
    "\n",
    "    mg_data_val = MoleculeGraphDataset(root, data_generator.validation_cids, graph_data_path, data_generator)\n",
    "    graph_batcher_val = AttentionGraphCollater(mg_data_val, mol_trunc_length)\n",
    "\n",
    "    mg_data_test = MoleculeGraphDataset(root, data_generator.test_cids, graph_data_path, data_generator)\n",
    "    graph_batcher_test = AttentionGraphCollater(mg_data_test, mol_trunc_length)\n",
    "\n",
    "    return graph_batcher_tr, graph_batcher_val, graph_batcher_test\n",
    "\n",
    "\n",
    "def get_attention_dataloader(data_generator, params):\n",
    "\n",
    "    training_set = AttentionDataset(data_generator.generate_examples_train, len(data_generator.training_cids))\n",
    "    validation_set = AttentionDataset(data_generator.generate_examples_val, len(data_generator.validation_cids))\n",
    "    test_set = AttentionDataset(data_generator.generate_examples_test, len(data_generator.test_cids))\n",
    "\n",
    "    training_generator = DataLoader(training_set,\n",
    "                                    **params)\n",
    "    validation_generator = DataLoader(validation_set,\n",
    "                                      **params)\n",
    "    test_generator = DataLoader(test_set, **params)\n",
    "\n",
    "\n",
    "    return training_generator, validation_generator, test_generator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uGh12ZozsUvf"
   },
   "source": [
    "#### 3.4.3.3 Training code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OQoR7spNQn-o"
   },
   "source": [
    "The following implementation sets up and runs model training for three different architectures (MLP, GCN, Attention). We have implemented the following:\n",
    "- **Implementation of Ablation Study**: The script includes an ablation study for changing the loss function, allowing experimentation with different loss computations to evaluate their impact on model performance.\n",
    "\n",
    "- **Debugging and Enhancements**: Significant effort was spent debugging the original code provided by the authors. Notably, modifications were made to ensure proper output of attention weights in the Attention model. The `AttentionModifier` class was introduced to fix a bug and modify the behavior of the attention mechanism to output weights necessary for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vKZ3KYOhN0-i"
   },
   "outputs": [],
   "source": [
    "#@title Implementation of Model Training and Associated Ablation Option\n",
    "\n",
    "# from losses import contrastive_loss, negative_sampling_contrastive_loss, naive_loss\n",
    "# from models import MLPModel, GCNModel, AttentionModel\n",
    "# from dataloaders import get_dataloader, GenerateData, get_graph_data, get_attention_graph_data, GenerateDataAttention, get_attention_dataloader\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Run Text2Mol')\n",
    "# parser.add_argument('--data', metavar='data', type=str,\n",
    "#                     help='directory where data is located')\n",
    "# parser.add_argument('--output_path', metavar='output_path', type=str,\n",
    "#                     help='directory where data is located')\n",
    "# parser.add_argument('--model', type=str, default='MLP', nargs='?',\n",
    "#                     help=\"model type from 'MLP, 'GCN', 'Attention'\")\n",
    "# parser.add_argument('--mol_trunc_length', type=int, nargs='?', default=512,\n",
    "#                     help='Molecule truncation length.')\n",
    "# parser.add_argument('--text_trunc_length', type=int, nargs='?', default=256,\n",
    "#                     help='Text truncation length.')\n",
    "# parser.add_argument('--num_warmup_steps', type=int, nargs='?', default=1000,\n",
    "#                     help='Number of warmup steps.')\n",
    "# parser.add_argument('--epochs', type=int, default=40,\n",
    "#                     help='Number of epochs to train model.')\n",
    "# parser.add_argument('--batch_size', type=int, default=32,\n",
    "#                     help='Size of data batch.')\n",
    "# parser.add_argument('--lr', type=float, nargs='?', default=1e-4,\n",
    "#                     help='learning rate')\n",
    "# parser.add_argument('--bert_lr', type=float, nargs='?', default=3e-5,\n",
    "#                     help='Size of data batch.')\n",
    "# parser.add_argument('--normalization_layer_removal', type=bool, nargs='?', default=False,\n",
    "#                     help='True or False')\n",
    "# parser.add_argument('--max_pool', type=bool, nargs='?', default=False,\n",
    "#                     help='True or False')\n",
    "# parser.add_argument('--hidden_layer_removal', type=bool, nargs='?', default=False,\n",
    "#                     help='True or False')\n",
    "# parser.add_argument('--conv_layer_removal', type=bool, nargs='?', default=False,\n",
    "#                     help='True or False')\n",
    "# parser.add_argument('--add_dropout', type=bool, nargs='?', default=False,\n",
    "#                     help='True or False')\n",
    "# parser.add_argument('--change_loss', type=bool, nargs='?', default=False,\n",
    "#                     help='True or False')\n",
    "# parser.add_argument('--text_length_ablation', type=str, nargs='?', default=\"none\",\n",
    "#                     help='long, short or none')\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# data_path = args.data\n",
    "# output_path = args.output_path\n",
    "# MODEL = args.model\n",
    "\n",
    "# BATCH_SIZE = args.batch_size\n",
    "# epochs = args.epochs\n",
    "\n",
    "# init_lr = args.lr\n",
    "# bert_lr = args.bert_lr\n",
    "# num_warmup_steps = args.num_warmup_steps\n",
    "# text_trunc_length = args.text_trunc_length\n",
    "\n",
    "# mol_trunc_length = args.mol_trunc_length #attention model only\n",
    "\n",
    "# sample = args.sample\n",
    "\n",
    "# path_token_embs = osp.join(data_path, \"token_embedding_dict.npy\")\n",
    "# path_train = osp.join(data_path, \"training.txt\")\n",
    "# path_val = osp.join(data_path, \"val.txt\")\n",
    "# path_test = osp.join(data_path, \"test.txt\")\n",
    "# path_molecules = osp.join(data_path, \"ChEBI_definitions_substructure_corpus.cp\")\n",
    "\n",
    "# graph_data_path = osp.join(data_path, \"mol_graphs.zip\")\n",
    "\n",
    "# ablation_option = AblationOption(args.normalization_layer_removal, args.max_pool, args.hidden_layer_removal, args.conv_layer_removal, args.add_dropout, args.change_loss, args.text_length_ablation)\n",
    "\n",
    "# if MODEL == \"MLP\":\n",
    "#     gd = GenerateData(text_trunc_length, path_train, path_val, path_test, path_molecules, path_token_embs, ablation_option.text_length_ablation)\n",
    "\n",
    "#     # Parameters\n",
    "#     params = {'batch_size': BATCH_SIZE,\n",
    "#             'num_workers': 4}\n",
    "\n",
    "#     training_generator, validation_generator, test_generator = get_dataloader(gd, params)\n",
    "\n",
    "#     model = MLPModel(ninp = 768, nhid = 600, nout = 300, ablation_option = ablation_option)\n",
    "\n",
    "# elif MODEL == \"GCN\":\n",
    "#     gd = GenerateData(text_trunc_length, path_train, path_val, path_test, path_molecules, path_token_embs, ablation_option.text_length_ablation)\n",
    "\n",
    "#     # Parameters\n",
    "#     params = {'batch_size': BATCH_SIZE,\n",
    "#             'num_workers': 4}\n",
    "\n",
    "#     training_generator, validation_generator, test_generator = get_dataloader(gd, params)\n",
    "\n",
    "#     graph_batcher_tr, graph_batcher_val, graph_batcher_test = get_graph_data(gd, graph_data_path)\n",
    "\n",
    "#     model = GCNModel(num_node_features=graph_batcher_tr.dataset.num_node_features, ninp = 768, nhid = 600, nout = 300, graph_hidden_channels = 600, ablation_option = ablation_option)\n",
    "\n",
    "\n",
    "# elif MODEL == \"Attention\":\n",
    "#     gd = GenerateDataAttention(text_trunc_length, path_train, path_val, path_test, path_molecules, path_token_embs, ablation_option.text_length_ablation)\n",
    "\n",
    "#     # Parameters\n",
    "#     params = {'batch_size': BATCH_SIZE,\n",
    "#             'num_workers': 4}\n",
    "\n",
    "#     training_generator, validation_generator, test_generator = get_attention_dataloader(gd, params)\n",
    "\n",
    "#     graph_batcher_tr, graph_batcher_val, graph_batcher_test = get_attention_graph_data(gd, graph_data_path, mol_trunc_length)\n",
    "\n",
    "#     model = AttentionModel(num_node_features=graph_batcher_tr.dataset.num_node_features, ninp = 768, nout = 300, nhead = 8, nhid = 512, nlayers = 3,\n",
    "#         graph_hidden_channels = 768, mol_trunc_length=mol_trunc_length, temp=0.07)\n",
    "\n",
    "# bert_params = list(model.text_transformer_model.parameters())\n",
    "\n",
    "# optimizer = optim.Adam([\n",
    "#                 {'params': model.other_params},\n",
    "#                 {'params': bert_params, 'lr': bert_lr}\n",
    "#             ], lr=init_lr)\n",
    "\n",
    "# num_training_steps = epochs * len(training_generator) - num_warmup_steps\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps = num_warmup_steps, num_training_steps = num_training_steps)\n",
    "\n",
    "\n",
    "# device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# print(device)\n",
    "\n",
    "# if MODEL == \"Attention\":\n",
    "#     tmp = model.set_device(device)\n",
    "# else:\n",
    "#     tmp = model.to(device)\n",
    "\n",
    "\n",
    "# train_losses = []\n",
    "# val_losses = []\n",
    "\n",
    "# train_acc = []\n",
    "# val_acc = []\n",
    "\n",
    "# if not os.path.exists(output_path):\n",
    "#   os.mkdir(output_path)\n",
    "\n",
    "# # Loop over epochs\n",
    "# for epoch in range(epochs):\n",
    "#     # Training\n",
    "\n",
    "#     start_time = time.time()\n",
    "#     running_loss = 0.0\n",
    "#     running_acc = 0.0\n",
    "#     model.train()\n",
    "#     for i, d in enumerate(training_generator):\n",
    "#         batch, labels = d\n",
    "#         # Transfer to GPU\n",
    "\n",
    "#         text_mask = batch['text']['attention_mask'].bool()\n",
    "\n",
    "#         text = batch['text']['input_ids'].to(device)\n",
    "#         text_mask = text_mask.to(device)\n",
    "#         molecule = batch['molecule']['mol2vec'].float().to(device)\n",
    "\n",
    "#         if MODEL == \"MLP\":\n",
    "#             text_out, chem_out = model(text, molecule, text_mask)\n",
    "\n",
    "#             loss = contrastive_loss(text_out, chem_out).to(device)\n",
    "#             running_loss += loss.item()\n",
    "#         elif MODEL == \"GCN\":\n",
    "#             graph_batch = graph_batcher_tr(d[0]['molecule']['cid']).to(device)\n",
    "#             text_out, chem_out = model(text, graph_batch, text_mask)\n",
    "\n",
    "#             loss = contrastive_loss(text_out, chem_out).to(device)\n",
    "#             running_loss += loss.item()\n",
    "#         elif MODEL == \"Attention\":\n",
    "#             graph_batch, molecule_mask = graph_batcher_tr(d[0]['molecule']['cid'])\n",
    "#             graph_batch = graph_batch.to(device)\n",
    "#             molecule_mask = molecule_mask.to(device)\n",
    "#             labels = labels.float().to(device)\n",
    "#             text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n",
    "\n",
    "#             loss, pred = negative_sampling_contrastive_loss(text_out, chem_out, labels)\n",
    "#             if torch.isnan(loss): raise ValueError('Loss is NaN.')\n",
    "\n",
    "#             running_loss += loss.item()\n",
    "#             running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) == labels.cpu().detach().numpy()) / labels.shape[0]\n",
    "\n",
    "\n",
    "#         running_loss += loss.item()\n",
    "#         optimizer.zero_grad()\n",
    "#         loss.backward()\n",
    "#         optimizer.step()\n",
    "\n",
    "#         scheduler.step()\n",
    "\n",
    "#         if (i+1) % 100 == 0: print(i+1, \"batches trained. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
    "#     train_losses.append(running_loss / (i+1))\n",
    "#     train_acc.append(running_acc / (i+1))\n",
    "\n",
    "#     print(\"Epoch\", epoch+1, \"training loss:\\t\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
    "#     print(\"Training Accuracy:\", train_acc[-1])\n",
    "\n",
    "\n",
    "\n",
    "#     # Validation\n",
    "#     model.eval()\n",
    "#     with torch.set_grad_enabled(False):\n",
    "#         start_time = time.time()\n",
    "#         running_loss = 0.0\n",
    "#         running_acc = 0.0\n",
    "#         for i, d in enumerate(validation_generator):\n",
    "#             batch, labels = d\n",
    "#             # Transfer to GPU\n",
    "\n",
    "#             text_mask = batch['text']['attention_mask'].bool()\n",
    "\n",
    "#             text = batch['text']['input_ids'].to(device)\n",
    "#             text_mask = text_mask.to(device)\n",
    "#             molecule = batch['molecule']['mol2vec'].float().to(device)\n",
    "\n",
    "#             if MODEL == \"MLP\":\n",
    "#                 text_out, chem_out = model(text, molecule, text_mask)\n",
    "\n",
    "#                 loss = contrastive_loss(text_out, chem_out).to(device)\n",
    "#                 if ablation_option.change_loss:\n",
    "#                     loss = naive_loss(text_out, chem_out).to(device)\n",
    "#                 running_loss += loss.item()\n",
    "#             elif MODEL == \"GCN\":\n",
    "#                 graph_batch = graph_batcher_val(d[0]['molecule']['cid']).to(device)\n",
    "#                 text_out, chem_out = model(text, graph_batch, text_mask)\n",
    "\n",
    "#                 loss = contrastive_loss(text_out, chem_out).to(device)\n",
    "#                 running_loss += loss.item()\n",
    "#             elif MODEL == \"Attention\":\n",
    "#                 graph_batch, molecule_mask = graph_batcher_val(d[0]['molecule']['cid'])\n",
    "#                 graph_batch = graph_batch.to(device)\n",
    "#                 molecule_mask = molecule_mask.to(device)\n",
    "#                 labels = labels.float().to(device)\n",
    "#                 text_out, chem_out = model(text, graph_batch, text_mask, molecule_mask)\n",
    "\n",
    "#                 loss, pred = negative_sampling_contrastive_loss(text_out, chem_out, labels)\n",
    "#                 running_loss += loss.item()\n",
    "#                 running_acc += np.sum((pred.squeeze().cpu().detach().numpy() > 0) == labels.cpu().detach().numpy()) / labels.shape[0]\n",
    "\n",
    "#             if (i+1) % 100 == 0: print(i+1, \"batches eval. Avg loss:\\t\", running_loss / (i+1), \". Avg ms/step =\", 1000*(time.time()-start_time)/(i+1))\n",
    "\n",
    "#         val_losses.append(running_loss / (i+1))\n",
    "#         val_acc.append(running_acc / (i+1))\n",
    "\n",
    "\n",
    "#         min_loss = np.min(val_losses)\n",
    "#         if val_losses[-1] == min_loss:\n",
    "#             torch.save(model.state_dict(), output_path + 'weights_pretrained.{epoch:02d}-{min_loss:.2f}.pt'.format(epoch = epoch+1, min_loss = min_loss))\n",
    "\n",
    "#     print(\"Epoch\", epoch+1, \"validation loss:\\t\", running_loss / (i+1), \". Time =\", (time.time()-start_time), \"seconds.\")\n",
    "#     if MODEL == \"Attention\": print(\"Validation Accuracy:\", val_acc[-1])\n",
    "\n",
    "\n",
    "# torch.save(model.state_dict(), output_path + \"final_weights.\"+str(epochs)+\".pt\")\n",
    "\n",
    "\n",
    "# cids_train = np.array([])\n",
    "# cids_val = np.array([])\n",
    "# cids_test = np.array([])\n",
    "# chem_embeddings_train = np.array([])\n",
    "# text_embeddings_train = np.array([])\n",
    "# chem_embeddings_val = np.array([])\n",
    "# text_embeddings_val = np.array([])\n",
    "# chem_embeddings_test = np.array([])\n",
    "# text_embeddings_test = np.array([])\n",
    "\n",
    "# if MODEL != \"Attention\": #Store embeddings:\n",
    "#     def get_emb(d, graph_batcher = None):\n",
    "#         with torch.no_grad():\n",
    "#             cid = np.array([d['cid']])\n",
    "#             text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
    "\n",
    "#             text = torch.Tensor(d['input']['text']['input_ids']).long().reshape(1,-1).to(device)\n",
    "#             molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
    "\n",
    "#             if MODEL == \"MLP\":\n",
    "#                 text_emb, chem_emb = model(text, molecule, text_mask)\n",
    "#             elif MODEL == \"GCN\":\n",
    "#                 graph_batch = graph_batcher([d['input']['molecule']['cid']]).to(device)\n",
    "#                 graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
    "#                 text_emb, chem_emb = model(text, graph_batch, text_mask)\n",
    "\n",
    "#             chem_emb = chem_emb.cpu().numpy()\n",
    "#             text_emb = text_emb.cpu().numpy()\n",
    "\n",
    "#         return cid, chem_emb, text_emb\n",
    "\n",
    "#     for i, d in enumerate(gd.generate_examples_train()):\n",
    "\n",
    "#         if MODEL == \"MLP\":\n",
    "#             cid, chem_emb, text_emb = get_emb(d)\n",
    "#         elif MODEL == \"GCN\":\n",
    "#             cid, chem_emb, text_emb = get_emb(d, graph_batcher_tr)\n",
    "\n",
    "#         cids_train = np.concatenate((cids_train, cid)) if cids_train.size else cid\n",
    "#         chem_embeddings_train = np.concatenate((chem_embeddings_train, chem_emb)) if chem_embeddings_train.size else chem_emb\n",
    "#         text_embeddings_train = np.concatenate((text_embeddings_train, text_emb)) if text_embeddings_train.size else text_emb\n",
    "\n",
    "#         if (i+1) % 1000 == 0: print(i+1, \"embeddings processed\")\n",
    "\n",
    "\n",
    "#     print(\"Training Embeddings done:\", cids_train.shape, chem_embeddings_train.shape)\n",
    "\n",
    "#     for d in gd.generate_examples_val():\n",
    "\n",
    "#         if MODEL == \"MLP\":\n",
    "#             cid, chem_emb, text_emb = get_emb(d)\n",
    "#         elif MODEL == \"GCN\":\n",
    "#             cid, chem_emb, text_emb = get_emb(d, graph_batcher_val)\n",
    "\n",
    "#         cids_val = np.concatenate((cids_val, cid)) if cids_val.size else cid\n",
    "#         chem_embeddings_val = np.concatenate((chem_embeddings_val, chem_emb)) if chem_embeddings_val.size else chem_emb\n",
    "#         text_embeddings_val = np.concatenate((text_embeddings_val, text_emb)) if text_embeddings_val.size else text_emb\n",
    "\n",
    "#     print(\"Validation Embeddings done:\", cids_val.shape, chem_embeddings_val.shape)\n",
    "\n",
    "#     for d in gd.generate_examples_test():\n",
    "\n",
    "#         if MODEL == \"MLP\":\n",
    "#             cid, chem_emb, text_emb = get_emb(d)\n",
    "#         elif MODEL == \"GCN\":\n",
    "#             cid, chem_emb, text_emb = get_emb(d, graph_batcher_test)\n",
    "\n",
    "#         cids_test = np.concatenate((cids_test, cid)) if cids_test.size else cid\n",
    "#         chem_embeddings_test = np.concatenate((chem_embeddings_test, chem_emb)) if chem_embeddings_test.size else chem_emb\n",
    "#         text_embeddings_test = np.concatenate((text_embeddings_test, text_emb)) if text_embeddings_test.size else text_emb\n",
    "\n",
    "#     print(\"Test Embeddings done:\", cids_test.shape, chem_embeddings_test.shape)\n",
    "\n",
    "#     emb_path = osp.join(output_path, \"embeddings/\")\n",
    "#     if not os.path.exists(emb_path):\n",
    "#         os.mkdir(emb_path)\n",
    "#     np.save(emb_path+\"cids_train.npy\", cids_train)\n",
    "#     np.save(emb_path+\"cids_val.npy\", cids_val)\n",
    "#     np.save(emb_path+\"cids_test.npy\", cids_test)\n",
    "#     np.save(emb_path+\"chem_embeddings_train.npy\", chem_embeddings_train)\n",
    "#     np.save(emb_path+\"chem_embeddings_val.npy\", chem_embeddings_val)\n",
    "#     np.save(emb_path+\"chem_embeddings_test.npy\", chem_embeddings_test)\n",
    "#     np.save(emb_path+\"text_embeddings_train.npy\", text_embeddings_train)\n",
    "#     np.save(emb_path+\"text_embeddings_val.npy\", text_embeddings_val)\n",
    "#     np.save(emb_path+\"text_embeddings_test.npy\", text_embeddings_test)\n",
    "\n",
    "# else: #Save association rules\n",
    "#     #Extract attention:\n",
    "#     last_decoder = model.text_transformer_decoder.layers[-1]\n",
    "\n",
    "#     mha_weights = {}\n",
    "#     def get_activation(name):\n",
    "#         def hook(model, input, output):\n",
    "#             mha_weights[cid] = output[1].cpu().detach().numpy()\n",
    "#         return hook\n",
    "\n",
    "\n",
    "#     handle = last_decoder.multihead_attn.register_forward_hook(get_activation(''))\n",
    "\n",
    "#     # Add the AttentionModifier class to fix the bug in the original code.\n",
    "#     # Turn on the required flags to save and output the attention weights.\n",
    "#     class AttentionModifier:\n",
    "#         def __init__(self, module):\n",
    "#             self.original_forward = module.forward\n",
    "#             self._patch_forward(module)\n",
    "\n",
    "#         def _patch_forward(self, module):\n",
    "#             def modified_forward(*args, **kwargs):\n",
    "#                 # Set attention-related kwargs to ensure proper handling\n",
    "#                 kwargs['need_weights'] = True\n",
    "#                 kwargs['average_attn_weights'] = True\n",
    "#                 return self.original_forward(*args, **kwargs)\n",
    "#             module.forward = modified_forward\n",
    "\n",
    "#     attention_modifier = AttentionModifier(model.text_transformer_decoder.layers[-1].multihead_attn)\n",
    "\n",
    "#     #Go through data to actually get the rules\n",
    "#     for i,d in enumerate(gd.generate_examples_train()):\n",
    "\n",
    "#         batch = d['input']\n",
    "\n",
    "#         cid = d['cid']\n",
    "#         text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
    "\n",
    "#         text = torch.Tensor(batch['text']['input_ids']).long().reshape(1,-1).to(device)\n",
    "#         graph_batch, molecule_mask = graph_batcher_tr([batch['molecule']['cid']])\n",
    "#         graph_batch = graph_batch.to(device)\n",
    "#         molecule_mask = molecule_mask.to(device)\n",
    "#         graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
    "\n",
    "#         out = model(text, graph_batch, text_mask, molecule_mask)\n",
    "\n",
    "#         #for memory reasons\n",
    "#         if cid in mha_weights:\n",
    "#             mol_length = graph_batch.x.shape[0]\n",
    "#             text_input = gd.text_tokenizer(gd.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                             max_length=gd.text_trunc_length - 1)\n",
    "#             text_length = np.sum(text_input['attention_mask'])\n",
    "\n",
    "#             mha_weights[cid] = mha_weights[cid][:text_length, 0, :mol_length]\n",
    "#         else:\n",
    "#             print(f\"Skipping cid {cid} as attention weights are not available.\")\n",
    "\n",
    "#         if (i+1) % 1000 == 0: print(\"Training sample\", i+1, \"attention extracted.\")\n",
    "\n",
    "#     for i,d in enumerate(gd.generate_examples_val()):\n",
    "\n",
    "#         batch = d['input']\n",
    "\n",
    "#         cid = d['cid']\n",
    "#         text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
    "\n",
    "#         text = torch.Tensor(batch['text']['input_ids']).long().reshape(1,-1).to(device)\n",
    "#         graph_batch, molecule_mask = graph_batcher_val([batch['molecule']['cid']])\n",
    "#         graph_batch = graph_batch.to(device)\n",
    "#         molecule_mask = molecule_mask.to(device)\n",
    "#         graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
    "\n",
    "\n",
    "#         out = model(text, graph_batch, text_mask, molecule_mask)\n",
    "\n",
    "#         #for memory reasons\n",
    "#         mol_length = graph_batch.x.shape[0]\n",
    "#         text_input = gd.text_tokenizer(gd.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                             max_length=gd.text_trunc_length - 1)\n",
    "#         text_length = np.sum(text_input['attention_mask'])\n",
    "#         mha_weights[cid] = mha_weights[cid][:text_length, 0, :mol_length]\n",
    "\n",
    "\n",
    "#         if (i+1) % 1000 == 0: print(\"Validation sample\", i+1, \"attention extracted.\")\n",
    "\n",
    "#     for i,d in enumerate(gd.generate_examples_test()):\n",
    "\n",
    "#         batch = d['input']\n",
    "\n",
    "#         cid = d['cid']\n",
    "#         text_mask = torch.Tensor(batch['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
    "\n",
    "#         text = torch.Tensor(batch['text']['input_ids']).long().reshape(1,-1).to(device)\n",
    "#         graph_batch, molecule_mask = graph_batcher_test([batch['molecule']['cid']])\n",
    "#         graph_batch = graph_batch.to(device)\n",
    "#         molecule_mask = molecule_mask.to(device)\n",
    "#         graph_batch.edge_index = graph_batch.edge_index.reshape((2,-1))\n",
    "\n",
    "\n",
    "#         out = model(text, graph_batch, text_mask, molecule_mask)\n",
    "\n",
    "#         #for memory reasons\n",
    "#         mol_length = graph_batch.x.shape[0]\n",
    "#         text_input = gd.text_tokenizer(gd.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                             max_length=gd.text_trunc_length - 1)\n",
    "#         text_length = np.sum(text_input['attention_mask'])\n",
    "#         mha_weights[cid] = mha_weights[cid][:text_length, 0, :mol_length]\n",
    "\n",
    "\n",
    "#         if (i+1) % 1000 == 0: print(\"Test sample\", i+1, \"attention extracted.\")\n",
    "\n",
    "\n",
    "#     with open(osp.join(output_path, \"mha_weights.pkl\"), 'wb') as fp:\n",
    "#         pickle.dump(mha_weights, fp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "M2ul7JvUbpLX"
   },
   "source": [
    "### 3.4.4 Demonstration of the Training Process Using a Sample Subset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XjtMT0knZYW2"
   },
   "source": [
    "In the following, we demonstrate the training of one epoch of the MLP model using a sample of training dataset that contains 6 records, a validation dataset that contains 3 records and a test dataset that contains 3 records."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XuzYS78bGeU4",
    "outputId": "cfce6869-ff73-4ddf-9dd7-b49ce820bc8f"
   },
   "outputs": [],
   "source": [
    "# @title Demo of Model Training Process with a Subset of Samples\n",
    "!mkdir test_output\n",
    "%cd /content/tar_files/\n",
    "!python3 code/main_sample.py --data data --output_path test_output/ --model MLP --epochs 1 --batch_size 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Gy-PVG4xTL4"
   },
   "source": [
    "## 3.5 Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R94t71hbxVg9"
   },
   "source": [
    "### 3.5.1 Evaluation Metrics Descriptions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FSehTKC3vV9"
   },
   "source": [
    "We are mainly using two metrics to evaluate the models: Mean Rank and Mean Reciprocal Rank (MRR).  Mean rank measures the average rank of relevant molecules in the list of retrieved molecules from the text descriptions. A lower mean rank indicates a better performance. MRR is calculated as the average of reciprocal ranks across all the records. A higher MRR indicates a better performance.\n",
    "\n",
    "$$\n",
    "Mean Rank = \\frac{1}{n} \\sum_{i=1}^{n} \\ R_i\n",
    "$$\n",
    "\n",
    "$$\n",
    "MRR = \\frac{1}{n} \\sum_{i=1}^{n} \\frac{1}{R_i}\n",
    "$$\n",
    "\n",
    "\n",
    "<p align = \"center\"> (Edwards et al., 2021)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HEG9vAIxxwKO"
   },
   "source": [
    "### 3.5.2 Evaluation Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKKpCK1IXiHw"
   },
   "source": [
    "#### 3.5.2.1 Baseline Models Evaluation\n",
    "\n",
    "This script implements a performance evaluation framework for baseline MLP and GCN models by computing the ranking metrics of molecule-text pair embeddings across training, validation, and test datasets. Utilizing cosine similarity, it systematically calculates and reports detailed ranking statistics such as mean rank, hit rates at specified thresholds, and Mean Reciprocal Rank (MRR)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pp-np8DV8Lga"
   },
   "outputs": [],
   "source": [
    "# @title Implementation of Baseline MLP and GCN Model Ranker\n",
    "\n",
    "#Note: There is a very slight variance in the results which arises because the molecule encoder cannot distinguish things like isotopes.\n",
    "# parser = argparse.ArgumentParser(description='Ensemble of Text2mol embeddings.')\n",
    "# parser.add_argument('dir', metavar='directory', type=str, nargs=1,\n",
    "#                     help='directory where embeddings are located')\n",
    "# parser.add_argument('--train', action='store_const', const=True, help=\"calculate training split ranks\")\n",
    "# parser.add_argument('--val', action='store_const', const=True, help=\"calculate validation split ranks\")\n",
    "# parser.add_argument('--test', action='store_const', const=True, help=\"calculate test split ranks\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# dir = args.dir[0]\n",
    "\n",
    "\n",
    "# cids_train = np.load(osp.join(dir, \"cids_train.npy\"), allow_pickle=True)\n",
    "# cids_val = np.load(osp.join(dir, \"cids_val.npy\"), allow_pickle=True)\n",
    "# cids_test = np.load(osp.join(dir, \"cids_test.npy\"), allow_pickle=True)\n",
    "\n",
    "# text_embeddings_train = np.load(osp.join(dir, \"text_embeddings_train.npy\"))\n",
    "# text_embeddings_val = np.load(osp.join(dir, \"text_embeddings_val.npy\"))\n",
    "# text_embeddings_test = np.load(osp.join(dir, \"text_embeddings_test.npy\"))\n",
    "\n",
    "# chem_embeddings_train = np.load(osp.join(dir, \"chem_embeddings_train.npy\"))\n",
    "# chem_embeddings_val = np.load(osp.join(dir, \"chem_embeddings_val.npy\"))\n",
    "# chem_embeddings_test = np.load(osp.join(dir, \"chem_embeddings_test.npy\"))\n",
    "\n",
    "# print('Loaded embeddings')\n",
    "\n",
    "\n",
    "# #combine all splits:\n",
    "\n",
    "# all_text_embbedings = np.concatenate((text_embeddings_train, text_embeddings_val, text_embeddings_test), axis = 0)\n",
    "# all_mol_embeddings = np.concatenate((chem_embeddings_train, chem_embeddings_val, chem_embeddings_test), axis = 0)\n",
    "\n",
    "# all_cids = np.concatenate((cids_train, cids_val, cids_test), axis = 0)\n",
    "\n",
    "\n",
    "# n_train = len(cids_train)\n",
    "# n_val = len(cids_val)\n",
    "# n_test = len(cids_test)\n",
    "# n = n_train + n_val + n_test\n",
    "\n",
    "# offset_val = n_train\n",
    "# offset_test = n_train + n_val\n",
    "\n",
    "# #Create efficient cosine calculator\n",
    "# def memory_efficient_similarity_matrix_custom(func, embedding1, embedding2, chunk_size = 1000):\n",
    "#     rows = embedding1.shape[0]\n",
    "\n",
    "#     num_chunks = int(np.ceil(rows / chunk_size))\n",
    "\n",
    "#     for i in range(num_chunks):\n",
    "#         end_chunk = (i+1)*(chunk_size) if (i+1)*(chunk_size) < rows else rows #account for smaller chunk at end...\n",
    "#         yield func(embedding1[i*chunk_size:end_chunk,:], embedding2)\n",
    "\n",
    "# text_chem_cos = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_train, all_mol_embeddings)\n",
    "# text_chem_cos_val = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_val, all_mol_embeddings)\n",
    "# text_chem_cos_test = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_test, all_mol_embeddings)\n",
    "\n",
    "\n",
    "# #Calculate Ranks:\n",
    "# if args.train:\n",
    "#     tr_avg_ranks = np.zeros((n_train, n))\n",
    "# if args.val:\n",
    "#     val_avg_ranks = np.zeros((n_val, n))\n",
    "# if args.test:\n",
    "#     test_avg_ranks = np.zeros((n_test, n))\n",
    "\n",
    "# ranks_train = []\n",
    "# ranks_val = []\n",
    "# ranks_test = []\n",
    "\n",
    "# def get_ranks(text_chem_cos, ranks_avg, offset, split= \"\"):\n",
    "#     ranks_tmp = []\n",
    "#     j = 0 #keep track of all loops\n",
    "#     for l, emb in enumerate(text_chem_cos):\n",
    "#         for k in range(emb.shape[0]):\n",
    "#             cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#             ranks = np.argsort(cid_locs)\n",
    "\n",
    "#             ranks_avg[j,:] = ranks_avg[j,:] + ranks\n",
    "\n",
    "#             rank = ranks[j+offset] + 1\n",
    "#             ranks_tmp.append(rank)\n",
    "\n",
    "\n",
    "#             j += 1\n",
    "#             if j % 1000 == 0: print(j, split+\" processed\")\n",
    "\n",
    "#     return np.array(ranks_tmp)\n",
    "\n",
    "# def print_ranks(ranks, split):\n",
    "\n",
    "#     print(split+\" Model:\")\n",
    "#     print(\"Mean rank:\", np.mean(ranks))\n",
    "#     print(\"Hits at 1:\", np.mean(ranks <= 1))\n",
    "#     print(\"Hits at 10:\", np.mean(ranks <= 10))\n",
    "#     print(\"Hits at 100:\", np.mean(ranks <= 100))\n",
    "#     print(\"Hits at 500:\", np.mean(ranks <= 500))\n",
    "#     print(\"Hits at 1000:\", np.mean(ranks <= 1000))\n",
    "\n",
    "#     print(\"MRR:\", np.mean(1/ranks))\n",
    "#     print()\n",
    "\n",
    "\n",
    "# if args.train:\n",
    "#     ranks_tmp = get_ranks(text_chem_cos, tr_avg_ranks, offset=0, split=\"train\")\n",
    "#     print_ranks(ranks_tmp, split=\"Training\")\n",
    "#     ranks_train = ranks_tmp\n",
    "\n",
    "# if args.val:\n",
    "#     ranks_tmp = get_ranks(text_chem_cos_val, val_avg_ranks, offset=offset_val, split=\"val\")\n",
    "#     print_ranks(ranks_tmp, split=\"Validation\")\n",
    "#     ranks_val = ranks_tmp\n",
    "\n",
    "# if args.test:\n",
    "#     ranks_tmp = get_ranks(text_chem_cos_test, test_avg_ranks, offset=offset_test, split=\"test\")\n",
    "#     print_ranks(ranks_tmp, split=\"Test\")\n",
    "#     ranks_test = ranks_tmp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBiZ1iDBYV_2"
   },
   "source": [
    "#### 3.5.2.2 Ensemble Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yv1CQhryYizW"
   },
   "source": [
    "This script integrates multiple model embeddings to evaluate ensemble strategies for ranking molecule-text pairs, employing cosine similarity across multiple data splits (training, validation, test). The script includes sorting mechanisms to ensure consistent ordering across different model outputs before performing evaluations.\n",
    "\n",
    "Two key additional experiments that we conducted in ensemble strategies are highlighted:\n",
    "\n",
    "- **Maximum Rank Ensemble**: An experiment where the ensemble rank is determined by taking the maximum rank across models instead of summing ranks. This approach assesses the performance under the most pessimistic scenario where the ensemble's prediction is only as good as the least accurate model's prediction for each case. It provides insights into the lower-bound of the ensemble.\n",
    "\n",
    "- **Weighted Rank Average Ensemble**: An experiment where ranks are averaged based on the inverse of their Mean Reciprocal Rank (MRR), but without reranking the averaged scores. This method weights the contribution of each model according to its overall performance, aiming to leverage the strengths of models with higher accuracy without altering their relative ranking positions.\n",
    "\n",
    "These experiments are designed to explore alternative ensemble methods that may explore the predictive performance by leveraging different aspects of individual model strengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sKjZQb9_YmYr"
   },
   "outputs": [],
   "source": [
    "#@title Implementation of the Ensemble Ranker and Additional Experiments of Ensemble Strategies\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Ensemble of Text2mol embeddings.')\n",
    "# parser.add_argument('dirs', metavar='directories', type=str, nargs='+',\n",
    "#                     help='all directories where embeddings are located')\n",
    "# parser.add_argument('--train', action='store_const', const=True, help=\"calculate training split ranks\")\n",
    "# parser.add_argument('--val', action='store_const', const=True, help=\"calculate validation split ranks\")\n",
    "# parser.add_argument('--test', action='store_const', const=True, help=\"calculate test split ranks\")\n",
    "\n",
    "# args = parser.parse_args()\n",
    "# dirs = list(args.dirs)\n",
    "\n",
    "# '''\n",
    "# inputs = \"../softmax_CLIP/embeddings\"\n",
    "# dir1 = osp.join(inputs, \"GCN1/\")\n",
    "# dir2 = osp.join(inputs, \"GCN2/\")\n",
    "# dir3 = osp.join(inputs, \"GCN3/\")\n",
    "# dir4 = osp.join(inputs, \"MLP1/\")\n",
    "# dir5 = osp.join(inputs, \"MLP2/\")\n",
    "# dir6 = osp.join(inputs, \"MLP3/\")\n",
    "# dirs = [dir1, dir2, dir3, dir4, dir5, dir6]\n",
    "# '''\n",
    "\n",
    "# NUM_MODELS = len(dirs)\n",
    "\n",
    "\n",
    "# text_embeddings_train = []\n",
    "# text_embeddings_val = []\n",
    "# text_embeddings_test = []\n",
    "# chem_embeddings_train = []\n",
    "# chem_embeddings_val = []\n",
    "# chem_embeddings_test = []\n",
    "\n",
    "# cids_train = []\n",
    "# cids_val = []\n",
    "# cids_test = []\n",
    "\n",
    "# for i, dir in enumerate(dirs):\n",
    "\n",
    "#     cids_train.append(np.load(osp.join(dir, \"cids_train.npy\"), allow_pickle=True))\n",
    "#     cids_val.append(np.load(osp.join(dir, \"cids_val.npy\"), allow_pickle=True))\n",
    "#     cids_test.append(np.load(osp.join(dir, \"cids_test.npy\"), allow_pickle=True))\n",
    "\n",
    "#     text_embeddings_train.append(np.load(osp.join(dir, \"text_embeddings_train.npy\")))\n",
    "#     text_embeddings_val.append(np.load(osp.join(dir, \"text_embeddings_val.npy\")))\n",
    "#     text_embeddings_test.append(np.load(osp.join(dir, \"text_embeddings_test.npy\")))\n",
    "\n",
    "#     chem_embeddings_train.append(np.load(osp.join(dir, \"chem_embeddings_train.npy\")))\n",
    "#     chem_embeddings_val.append(np.load(osp.join(dir, \"chem_embeddings_val.npy\")))\n",
    "#     chem_embeddings_test.append(np.load(osp.join(dir, \"chem_embeddings_test.npy\")))\n",
    "\n",
    "#     print('Loaded embedding from model', i+1)\n",
    "\n",
    "# print('Loaded embeddings')\n",
    "\n",
    "# #Reorder (this is very important):\n",
    "\n",
    "# for i in range(1, NUM_MODELS):\n",
    "#     tmp = cids_train[i].tolist()\n",
    "#     indexes = [tmp.index(i) for i in cids_train[0]]\n",
    "#     tmp = cids_val[i].tolist()\n",
    "#     indexes_val = [tmp.index(i) for i in cids_val[0]]\n",
    "#     tmp = cids_test[i].tolist()\n",
    "#     indexes_test = [tmp.index(i) for i in cids_test[0]]\n",
    "\n",
    "#     cids_train[i] = cids_train[i][indexes]\n",
    "#     cids_val[i] = cids_val[i][indexes_val]\n",
    "#     cids_test[i] = cids_test[i][indexes_test]\n",
    "\n",
    "#     text_embeddings_train[i] = text_embeddings_train[i][indexes]\n",
    "#     text_embeddings_val[i] = text_embeddings_val[i][indexes_val]\n",
    "#     text_embeddings_test[i] = text_embeddings_test[i][indexes_test]\n",
    "\n",
    "#     chem_embeddings_train[i] = chem_embeddings_train[i][indexes]\n",
    "#     chem_embeddings_val[i] = chem_embeddings_val[i][indexes_val]\n",
    "#     chem_embeddings_test[i] = chem_embeddings_test[i][indexes_test]\n",
    "\n",
    "#     print('Embeddings {} reordered'.format(i+1))\n",
    "\n",
    "# print('Sorted embeddings')\n",
    "\n",
    "# #combine all splits:\n",
    "# all_text_embbedings = []\n",
    "# all_mol_embeddings = []\n",
    "# for i in range(NUM_MODELS):\n",
    "#     all_text_embbedings.append(np.concatenate((text_embeddings_train[i], text_embeddings_val[i], text_embeddings_test[i]), axis = 0))\n",
    "#     all_mol_embeddings.append(np.concatenate((chem_embeddings_train[i], chem_embeddings_val[i], chem_embeddings_test[i]), axis = 0))\n",
    "\n",
    "# all_cids = np.concatenate((cids_train[0], cids_val[0], cids_test[0]), axis = 0)\n",
    "\n",
    "\n",
    "# n_train = len(cids_train[0])\n",
    "# n_val = len(cids_val[0])\n",
    "# n_test = len(cids_test[0])\n",
    "# n = n_train + n_val + n_test\n",
    "\n",
    "# offset_val = n_train\n",
    "# offset_test = n_train + n_val\n",
    "\n",
    "\n",
    "# #I wrote a multithreaded version of the cosine similarity for something else. I can upload it if needed.\n",
    "\n",
    "# #Create efficient cosine calculator\n",
    "# def memory_efficient_similarity_matrix_custom(func, embedding1, embedding2, chunk_size = 1000):\n",
    "#     rows = embedding1.shape[0]\n",
    "\n",
    "#     num_chunks = int(np.ceil(rows / chunk_size))\n",
    "\n",
    "#     for i in range(num_chunks):\n",
    "#         end_chunk = (i+1)*(chunk_size) if (i+1)*(chunk_size) < rows else rows #account for smaller chunk at end...\n",
    "#         yield func(embedding1[i*chunk_size:end_chunk,:], embedding2)\n",
    "\n",
    "# text_chem_cos = []\n",
    "# text_chem_cos_val = []\n",
    "# text_chem_cos_test = []\n",
    "# for i in range(NUM_MODELS):\n",
    "#     text_chem_cos.append(memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_train[i], all_mol_embeddings[i]))\n",
    "#     text_chem_cos_val.append(memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_val[i], all_mol_embeddings[i]))\n",
    "#     text_chem_cos_test.append(memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_test[i], all_mol_embeddings[i]))\n",
    "\n",
    "# #Calculate Ranks:\n",
    "# if args.train:\n",
    "#     tr_avg_ranks = np.zeros((n_train, n))\n",
    "# if args.val:\n",
    "#     val_avg_ranks = np.zeros((n_val, n))\n",
    "# if args.test:\n",
    "#     test_avg_ranks = np.zeros((n_test, n))\n",
    "\n",
    "# ranks_train = []\n",
    "# ranks_val = []\n",
    "# ranks_test = []\n",
    "\n",
    "# def get_ranks(text_chem_cos, ranks_avg, offset, split= \"\"):\n",
    "#     ranks_tmp = []\n",
    "#     j = 0 #keep track of all loops\n",
    "#     for l, emb in enumerate(text_chem_cos):\n",
    "#         for k in range(emb.shape[0]):\n",
    "#             cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#             ranks = np.argsort(cid_locs)\n",
    "\n",
    "#             # Additional experiment 1: change rank sum to maximum rank. add the next line.\n",
    "#             # ranks_avg[j,:] = np.maximum(ranks_avg[j,:], ranks)\n",
    "#             ranks_avg[j,:] = ranks_avg[j,:] + ranks\n",
    "\n",
    "#             rank = ranks[j+offset] + 1\n",
    "#             ranks_tmp.append(rank)\n",
    "\n",
    "\n",
    "#             j += 1\n",
    "#             if j % 1000 == 0: print(j, split+\" processed\")\n",
    "\n",
    "#     return np.array(ranks_tmp)\n",
    "\n",
    "# def print_ranks(ranks, model_num, split):\n",
    "\n",
    "#     print(split+\" Model {}:\".format(model_num))\n",
    "#     print(\"Mean rank:\", np.mean(ranks))\n",
    "#     print(\"Hits at 1:\", np.mean(ranks <= 1))\n",
    "#     print(\"Hits at 10:\", np.mean(ranks <= 10))\n",
    "#     print(\"Hits at 100:\", np.mean(ranks <= 100))\n",
    "#     print(\"Hits at 500:\", np.mean(ranks <= 500))\n",
    "#     print(\"Hits at 1000:\", np.mean(ranks <= 1000))\n",
    "\n",
    "#     print(\"MRR:\", np.mean(1/ranks))\n",
    "#     print()\n",
    "\n",
    "\n",
    "# for i in range(NUM_MODELS):\n",
    "#     if args.train:\n",
    "#         ranks_tmp = get_ranks(text_chem_cos[i], tr_avg_ranks, offset=0, split=\"train\")\n",
    "#         print_ranks(ranks_tmp, i+1, split=\"Training\")\n",
    "#         ranks_train.append(ranks_tmp)\n",
    "\n",
    "#     if args.val:\n",
    "#         ranks_tmp = get_ranks(text_chem_cos_val[i], val_avg_ranks, offset=offset_val, split=\"val\")\n",
    "#         print_ranks(ranks_tmp, i+1, split=\"Validation\")\n",
    "#         ranks_val.append(ranks_tmp)\n",
    "\n",
    "#     if args.test:\n",
    "#         ranks_tmp = get_ranks(text_chem_cos_test[i], test_avg_ranks, offset=offset_test, split=\"test\")\n",
    "#         print_ranks(ranks_tmp, i+1, split=\"Test\")\n",
    "#         ranks_test.append(ranks_tmp)\n",
    "\n",
    "\n",
    "# #Process ensemble:\n",
    "# if args.train:\n",
    "#     sorted = np.argsort(tr_avg_ranks)\n",
    "#     new_tr_ranks = np.diag(np.argsort(sorted)) + 1\n",
    "\n",
    "#     print_ranks(new_tr_ranks, \"e\", split=\"Training Ensemble\")\n",
    "\n",
    "# if args.val:\n",
    "#     sorted = np.argsort(val_avg_ranks)\n",
    "#     val_final_ranks = np.argsort(sorted) + 1\n",
    "#     new_val_ranks = np.diag(val_final_ranks[:,offset_val:offset_test])\n",
    "\n",
    "#     print_ranks(new_val_ranks, \"e\", split=\"Validation Ensemble\")\n",
    "\n",
    "# if args.test:\n",
    "#     sorted = np.argsort(test_avg_ranks)\n",
    "#     test_final_ranks = np.argsort(sorted) + 1\n",
    "#     new_test_ranks = np.diag(test_final_ranks[:,offset_test:])\n",
    "\n",
    "#     print_ranks(new_test_ranks, \"e\", split=\"Test Ensemble\")\n",
    "\n",
    "# #  Additional experiment 2: Weighted rank average ensemble\n",
    "# # Calculate ensemble using weighted rank average\n",
    "# #def calculate_ensemble_weighted_rank_average(ranks_list, weights):\n",
    "#     #weighted_ranks = [ranks * weight for ranks, weight in zip(ranks_list, weights)]\n",
    "#     #ensemble_ranks = np.sum(weighted_ranks, axis=0)\n",
    "#     #return ensemble_ranks\n",
    "\n",
    "# # Process ensemble using weighted rank average\n",
    "# #if args.train:\n",
    "#     #train_mrr_list = [np.mean(1 / ranks) for ranks in ranks_train]\n",
    "#     #train_weights = [mrr / sum(train_mrr_list) for mrr in train_mrr_list]\n",
    "#     #ensemble_train_ranks = calculate_ensemble_weighted_rank_average(ranks_train, train_weights)\n",
    "#     #print_ranks(ensemble_train_ranks, 'Weighted Rank Average Ensemble', split=\"Training\")\n",
    "\n",
    "# #if args.val:\n",
    "#     #val_mrr_list = [np.mean(1 / ranks) for ranks in ranks_val]\n",
    "#     #val_weights = [mrr / sum(val_mrr_list) for mrr in val_mrr_list]\n",
    "#     #ensemble_val_ranks = calculate_ensemble_weighted_rank_average(ranks_val, val_weights)\n",
    "#     #print_ranks(ensemble_val_ranks, 'Weighted Rank Average Ensemble', split=\"Validation\")\n",
    "\n",
    "# #if args.test:\n",
    "#     #test_mrr_list = [np.mean(1 / ranks) for ranks in ranks_test]\n",
    "#     #test_weights = [mrr / sum(test_mrr_list) for mrr in test_mrr_list]\n",
    "#     #ensemble_test_ranks = calculate_ensemble_weighted_rank_average(ranks_test, test_weights)\n",
    "#     #print_ranks(ensemble_test_ranks, 'Weighted Rank Average Ensemble', split=\"Test\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gKYn9di2Y3tQ"
   },
   "source": [
    "#### 3.5.2.3 Cross-modal Attention Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zp4VemhT4D9a"
   },
   "source": [
    "We extracted the weights of the final layer of the attention model and used it to produce the new assocation rules - denoted by AR(a,b) in the formula below. These new association rules were subsequently integrated with the existing association rules from the MLP model to formulate an updated ranking for the molecule embeddings."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2QR3B4lFw_3d"
   },
   "source": [
    "$$\n",
    "S (a, b) = \\alpha cos(a,b) + (1 - \\alpha) AR(a,b)\n",
    "$$\n",
    "\n",
    "<p align = \"center\"> (Edwards et al., 2021)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9eA4hZLxZMKQ"
   },
   "outputs": [],
   "source": [
    "#@title Implementation of Attention Model Ranker\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Run attention MRR calculation')\n",
    "# parser.add_argument('--weights_dir', metavar='weights_dir', type=str,\n",
    "#                     help='directory where weights is located')\n",
    "# parser.add_argument('--embeddings_dir', metavar='embeddings_dir', type=str,\n",
    "#                     help='directory where embeddings is located')\n",
    "# args = parser.parse_args()\n",
    "# weights_path = args.weights_dir\n",
    "# embeddings_path = args.embeddings_dir\n",
    "\n",
    "# class GenerateData():\n",
    "#   def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n",
    "#     self.path_train = path_train\n",
    "#     self.path_val = path_val\n",
    "#     self.path_test = path_test\n",
    "#     self.path_molecules = path_molecules\n",
    "#     self.path_token_embs = path_token_embs\n",
    "\n",
    "#     self.mol_trunc_length = 512\n",
    "#     self.text_trunc_length = 256\n",
    "\n",
    "#     self.prep_text_tokenizer()\n",
    "\n",
    "#     self.load_substructures()\n",
    "\n",
    "#     self.batch_size = 32\n",
    "\n",
    "#     self.store_descriptions()\n",
    "\n",
    "#   def load_substructures(self):\n",
    "#     self.molecule_sentences = {}\n",
    "#     self.molecule_tokens = {}\n",
    "\n",
    "#     total_tokens = set()\n",
    "#     self.max_mol_length = 0\n",
    "#     with open(self.path_molecules) as f:\n",
    "#       for line in f:\n",
    "#         spl = line.split(\":\")\n",
    "#         cid = spl[0]\n",
    "#         tokens = spl[1].strip()\n",
    "#         self.molecule_sentences[cid] = tokens\n",
    "#         t = tokens.split()\n",
    "#         total_tokens.update(t)\n",
    "#         size = len(t)\n",
    "#         if size > self.max_mol_length: self.max_mol_length = size\n",
    "\n",
    "#     self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
    "\n",
    "#   def prep_text_tokenizer(self):\n",
    "#     self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "#   def store_descriptions(self):\n",
    "#     self.descriptions = {}\n",
    "\n",
    "#     self.mols = {}\n",
    "\n",
    "#     self.training_cids = []\n",
    "#     #get training set cids...\n",
    "#     with open(self.path_train) as f:\n",
    "#       reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "#       for n, line in enumerate(reader):\n",
    "#         self.descriptions[line['cid']] = line['desc']\n",
    "#         self.mols[line['cid']] = line['mol2vec']\n",
    "#         self.training_cids.append(line['cid'])\n",
    "\n",
    "#     self.validation_cids = []\n",
    "#     #get validation set cids...\n",
    "#     with open(self.path_val) as f:\n",
    "#       reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "#       for n, line in enumerate(reader):\n",
    "#         self.descriptions[line['cid']] = line['desc']\n",
    "#         self.mols[line['cid']] = line['mol2vec']\n",
    "#         self.validation_cids.append(line['cid'])\n",
    "\n",
    "#     self.test_cids = []\n",
    "#     with open(self.path_test) as f:\n",
    "#       reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "#       for n, line in enumerate(reader):\n",
    "#         self.descriptions[line['cid']] = line['desc']\n",
    "#         self.mols[line['cid']] = line['mol2vec']\n",
    "#         self.test_cids.append(line['cid'])\n",
    "\n",
    "#   #transformers can't take array with full attention so have to pad a 0...\n",
    "#   def padarray(self, A, size, value=0):\n",
    "#       t = size - len(A)\n",
    "#       return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n",
    "\n",
    "\n",
    "#   def generate_examples_train(self):\n",
    "#     \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "#     np.random.shuffle(self.training_cids)\n",
    "\n",
    "#     for cid in self.training_cids:\n",
    "#       label = np.random.randint(2)\n",
    "#       rand_cid = np.random.choice(self.training_cids)\n",
    "#       if label:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "#       else:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "\n",
    "#       text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "#       text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "#       yield {\n",
    "#           'cid': cid,\n",
    "#           'input': {\n",
    "#               'text': {\n",
    "#                 'input_ids': text_ids,\n",
    "#                 'attention_mask': text_mask,\n",
    "#               },\n",
    "#               'molecule' : {\n",
    "#                     'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "#                     'cid' : cid\n",
    "#               },\n",
    "#           },\n",
    "#           'label': label\n",
    "#       }\n",
    "\n",
    "\n",
    "#   def generate_examples_val(self):\n",
    "#     \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "#     np.random.shuffle(self.validation_cids)\n",
    "\n",
    "#     for cid in self.validation_cids:\n",
    "#       label = np.random.randint(2)\n",
    "#       rand_cid = np.random.choice(self.validation_cids)\n",
    "#       if label:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "#       else:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "\n",
    "\n",
    "#       text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "#       text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "#       yield {\n",
    "#           'cid': cid,\n",
    "#           'input': {\n",
    "#               'text': {\n",
    "#                 'input_ids': text_ids,\n",
    "#                 'attention_mask': text_mask,\n",
    "#               },\n",
    "#               'molecule' : {\n",
    "#                     'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "#                     'cid' : cid\n",
    "#               },\n",
    "#           },\n",
    "#           'label': label\n",
    "#       }\n",
    "\n",
    "#   def generate_examples_test(self):\n",
    "#     \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "#     np.random.shuffle(self.test_cids)\n",
    "\n",
    "#     for cid in self.test_cids:\n",
    "#       label = np.random.randint(2)\n",
    "#       rand_cid = np.random.choice(self.test_cids)\n",
    "#       if label:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "#       else:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "\n",
    "\n",
    "#       text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "#       text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "#       yield {\n",
    "#           'cid': cid,\n",
    "#           'input': {\n",
    "#               'text': {\n",
    "#                 'input_ids': text_ids,\n",
    "#                 'attention_mask': text_mask,\n",
    "#               },\n",
    "#               'molecule' : {\n",
    "#                     'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "#                     'cid' : cid\n",
    "#               },\n",
    "#           },\n",
    "#           'label': label\n",
    "#       }\n",
    "\n",
    "# graph_data_path = \"data/mol_graphs.zip\"\n",
    "# dir_emb = embeddings_path\n",
    "# mounted_path_token_embs = \"data/token_embedding_dict.npy\"\n",
    "# mounted_path_train = \"data/training.txt\"\n",
    "# mounted_path_val = \"data/val.txt\"\n",
    "# mounted_path_test = \"data/test.txt\"\n",
    "# mounted_path_molecules = \"data/ChEBI_definitions_substructure_corpus.cp\"\n",
    "# gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n",
    "\n",
    "# cids_train1 = np.load(dir_emb + \"cids_train.npy\", allow_pickle=True)\n",
    "# cids_val1 = np.load(dir_emb + \"cids_val.npy\", allow_pickle=True)\n",
    "# cids_test1 = np.load(dir_emb + \"cids_test.npy\", allow_pickle=True)\n",
    "# chem_embeddings_train1 = np.load(dir_emb + \"chem_embeddings_train.npy\")\n",
    "# chem_embeddings_val1 = np.load(dir_emb + \"chem_embeddings_val.npy\")\n",
    "# chem_embeddings_test1 = np.load(dir_emb + \"chem_embeddings_test.npy\")\n",
    "# text_embeddings_train1 = np.load(dir_emb + \"text_embeddings_train.npy\")\n",
    "# text_embeddings_val1 = np.load(dir_emb + \"text_embeddings_val.npy\")\n",
    "# text_embeddings_test1 = np.load(dir_emb + \"text_embeddings_test.npy\")\n",
    "\n",
    "\n",
    "# all_chem_embbedings1 = np.concatenate((chem_embeddings_train1, chem_embeddings_val1, chem_embeddings_test1), axis = 0)\n",
    "\n",
    "# cids_all = np.concatenate((cids_train1, cids_val1, cids_test1), axis = 0)\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def memory_efficient_similarity_matrix_custom(func, embedding1, embedding2, chunk_size = 1000):\n",
    "#     rows = embedding1.shape[0]\n",
    "\n",
    "#     num_chunks = int(np.ceil(rows / chunk_size))\n",
    "\n",
    "#     for i in range(num_chunks):\n",
    "#         end_chunk = (i+1)*(chunk_size) if (i+1)*(chunk_size) < rows else rows #account for smaller chunk at end...\n",
    "#         yield func(embedding1[i*chunk_size:end_chunk,:], embedding2)\n",
    "\n",
    "# #Calculate mean rank, hits at ten\n",
    "\n",
    "# def dot_product(a, b):\n",
    "#   return np.dot(a, b.T)\n",
    "\n",
    "# sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# compose = lambda a,b: sigmoid(dot_product(a,b))\n",
    "\n",
    "# text_chem_cos1 = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_train1, all_chem_embbedings1)\n",
    "# text_chem_cos_val1 = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_val1, all_chem_embbedings1)\n",
    "# text_chem_cos_test1 = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_test1, all_chem_embbedings1)\n",
    "\n",
    "# n_train = len(cids_train1)\n",
    "# n_val = len(cids_val1)\n",
    "# n_test = len(cids_test1)\n",
    "# n = n_train + n_val + n_test\n",
    "\n",
    "# offset_val = n_train\n",
    "# offset_test = n_train + n_val\n",
    "\n",
    "# num_top = 10\n",
    "# top_cids1 = {}\n",
    "# top_cids_val1 = {}\n",
    "# top_cids_test1 = {}\n",
    "# scores_val1 = {}\n",
    "# scores_test1 = {}\n",
    "\n",
    "# ranks1 = []\n",
    "# j = 0 #keep track of all loops\n",
    "# for i, emb in enumerate(text_chem_cos1):\n",
    "#     for k in range(emb.shape[0]):\n",
    "#         cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#         ranks = np.argsort(cid_locs) #rank is actually double argsort...\n",
    "\n",
    "#         top_cids1[cids_train1[j]] = [cids_all[loc] for loc in cid_locs[:num_top]]\n",
    "\n",
    "#         rank = ranks[j] + 1\n",
    "#         ranks1.append(rank)\n",
    "\n",
    "\n",
    "#         j += 1\n",
    "#         if (j) % 1000 == 0: print((j), \"train processed.\")\n",
    "\n",
    "# ranks1 = np.array(ranks1)\n",
    "\n",
    "# print()\n",
    "# print(\"MLP Training Mean rank:\", np.mean(ranks1))\n",
    "# print(\"MLP Hits at 1:\", np.mean(ranks1 <= 1))\n",
    "# print(\"MLP Hits at 10:\", np.mean(ranks1 <= 10))\n",
    "# print(\"MLP Hits at 100:\", np.mean(ranks1 <= 100))\n",
    "# print(\"MLP Hits at 500:\", np.mean(ranks1 <= 500))\n",
    "# print(\"MLP Hits at 1000:\", np.mean(ranks1 <= 1000))\n",
    "\n",
    "# print(\"MLP Trainng MRR:\", np.mean(1/np.array(ranks1)))\n",
    "\n",
    "# ranks_val1 = []\n",
    "# j = 0 #keep track of all loops\n",
    "# for i, emb in enumerate(text_chem_cos_val1):\n",
    "#     for k in range(emb.shape[0]):\n",
    "#         cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#         ranks = np.argsort(cid_locs) #rank is actually double argsort...\n",
    "\n",
    "#         scores = np.sort(emb[k,:])[::-1]\n",
    "\n",
    "#         top_cids_val1[cids_val1[j]] = [cids_all[loc] for loc in cid_locs[:num_top]]\n",
    "#         scores_val1[cids_val1[j]] = scores[:num_top]\n",
    "\n",
    "#         rank = ranks[j+offset_val] + 1\n",
    "#         ranks_val1.append(rank)\n",
    "\n",
    "#         j += 1\n",
    "#         if (j) % 1000 == 0: print((j), \"val processed.\")\n",
    "\n",
    "\n",
    "# ranks_val1 = np.array(ranks_val1)\n",
    "\n",
    "# print()\n",
    "# print(\"MLP Val Mean rank:\", np.mean(ranks_val1))\n",
    "# print(\"MLP Hits at 1:\", np.mean(ranks_val1 <= 1))\n",
    "# print(\"MLP Hits at 10:\", np.mean(ranks_val1 <= 10))\n",
    "# print(\"MLP Hits at 100:\", np.mean(ranks_val1 <= 100))\n",
    "# print(\"MLP Hits at 500:\", np.mean(ranks_val1 <= 500))\n",
    "# print(\"MLP Hits at 1000:\", np.mean(ranks_val1 <= 1000))\n",
    "\n",
    "# print(\"MLP Validation MRR:\", np.mean(1/ranks_val1))\n",
    "\n",
    "\n",
    "# ranks_test1 = []\n",
    "# j = 0 #keep track of all loops\n",
    "# for i, emb in enumerate(text_chem_cos_test1):\n",
    "#     for k in range(emb.shape[0]):\n",
    "#         cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#         ranks = np.argsort(cid_locs) #rank is actually double argsort...\n",
    "\n",
    "#         scores = np.sort(emb[k,:])[::-1]\n",
    "\n",
    "#         top_cids_test1[cids_test1[j]] = [cids_all[loc] for loc in cid_locs[:num_top]]\n",
    "#         scores_test1[cids_test1[j]] = scores[:num_top]\n",
    "\n",
    "#         rank = ranks[j+offset_test] + 1\n",
    "#         ranks_test1.append(rank)\n",
    "\n",
    "#         j += 1\n",
    "#         if (j) % 1000 == 0: print((j), \"test processed.\")\n",
    "\n",
    "\n",
    "# ranks_test1 = np.array(ranks_test1)\n",
    "\n",
    "# print()\n",
    "# print(\"MLP Test Mean rank:\", np.mean(ranks_test1))\n",
    "# print(\"MLP Hits at 1:\", np.mean(ranks_test1 <= 1))\n",
    "# print(\"MLP Hits at 10:\", np.mean(ranks_test1 <= 10))\n",
    "# print(\"MLP Hits at 100:\", np.mean(ranks_test1 <= 100))\n",
    "# print(\"MLP Hits at 500:\", np.mean(ranks_test1 <= 500))\n",
    "# print(\"MLP Hits at 1000:\", np.mean(ranks_test1 <= 1000))\n",
    "\n",
    "# print(\"MLP Test MRR:\", np.mean(1/ranks_test1))\n",
    "\n",
    "\n",
    "# # get attention rules\n",
    "# with open(f'{weights_path}/mha_weights.pkl', 'rb') as f:\n",
    "#     mha_weights = pickle.load(f)\n",
    "\n",
    "\n",
    "# all_mol_tokens = set()\n",
    "# all_text_tokens = set()\n",
    "\n",
    "# import zipfile\n",
    "# archive = zipfile.ZipFile(graph_data_path, 'r')\n",
    "\n",
    "# for i, cid in enumerate(mha_weights):\n",
    "#   attn_weights = mha_weights[cid]\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   gfile = archive.open(cid + '.graph').read().decode('ascii')\n",
    "#   mol_tokens = {}\n",
    "#   idx = False\n",
    "#   for line in gfile.split('\\n'):\n",
    "#     line = line.strip()\n",
    "#     if line == 'idx to identifier:':\n",
    "#       idx = True\n",
    "#       continue\n",
    "#     if idx and len(line) != 0:\n",
    "#       id, idf = line.split(\" \")\n",
    "#       mol_tokens[id] = idf\n",
    "\n",
    "#   mol_tokens = list(mol_tokens.values())\n",
    "\n",
    "#   all_mol_tokens.update(mol_tokens)\n",
    "#   all_text_tokens.update(text_tokens)\n",
    "\n",
    "# mol_token_ids = {}\n",
    "# text_token_ids = {}\n",
    "\n",
    "# mol_token_ids_rev = {}\n",
    "# text_token_ids_rev = {}\n",
    "# for i, k in enumerate(all_mol_tokens):\n",
    "#   mol_token_ids[k] = i\n",
    "#   mol_token_ids_rev[i] = k\n",
    "# for i, k in enumerate(all_text_tokens):\n",
    "#   text_token_ids[k] = i\n",
    "#   text_token_ids_rev[i] = k\n",
    "\n",
    "# support = np.zeros((len(all_text_tokens), len(all_mol_tokens)))\n",
    "# conf = np.zeros((len(all_text_tokens), len(all_mol_tokens)))\n",
    "\n",
    "# for i, cid in enumerate(mha_weights):\n",
    "#   # print('cid:', cid)\n",
    "#   if cid in gt.validation_cids or cid in gt.test_cids: continue\n",
    "#   attn_weights = mha_weights[cid]\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   gfile = archive.open(cid + '.graph').read().decode('ascii')\n",
    "#   mol_tokens = {}\n",
    "#   idx = False\n",
    "#   for line in gfile.split('\\n'):\n",
    "#     line = line.strip()\n",
    "#     if line == 'idx to identifier:':\n",
    "#       idx = True\n",
    "#       continue\n",
    "#     if idx and len(line) != 0:\n",
    "#       id, idf = line.split(\" \")\n",
    "#       mol_tokens[id] = idf\n",
    "#   mol_tokens = list(mol_tokens.values())\n",
    "\n",
    "#   if len(mol_tokens) > gt.mol_trunc_length: mol_tokens = mol_tokens[:gt.mol_trunc_length]\n",
    "\n",
    "#   for j, text in enumerate(text_tokens):\n",
    "#     for k, molt in enumerate(mol_tokens):\n",
    "# #      support[text_token_ids[text], mol_token_ids[molt]] += attn_weights[j,k] #* mol_length # mol_length to normalize\n",
    "#       if k >= attn_weights.shape[1]:\n",
    "#             break\n",
    "#        # fixing the bug of IndexError. attn_weights has a shape of (1, X), iterating through j will cause the index out of bound.\n",
    "#       support[text_token_ids[text], mol_token_ids[molt]] += attn_weights[0, k]\n",
    "\n",
    "#   if (i+1) % 1000 == 0: print(i+1)\n",
    "\n",
    "# print(\"Support calculation finished.\")\n",
    "\n",
    "# for j, text in enumerate(all_text_tokens):\n",
    "#   if np.sum(support[text_token_ids[text], :]) == 0:\n",
    "#     conf[text_token_ids[text], :] = 0.0\n",
    "#   else:\n",
    "#     conf[text_token_ids[text], :] = support[text_token_ids[text], :] / np.sum(support[text_token_ids[text], :])\n",
    "\n",
    "#   if (j+1) % 1000 == 0: print(j+1)\n",
    "\n",
    "# print(\"Confidence calculation finished.\")\n",
    "\n",
    "# from itertools import combinations, chain\n",
    "\n",
    "\n",
    "# def all_subsets(ss):#skip empty set\n",
    "#     return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))\n",
    "\n",
    "\n",
    "# def generate_rules(text_tokens, mol_tokens):\n",
    "#   candidates = set()\n",
    "\n",
    "#   text_subs = [frozenset([text_token_ids[j] for j in i]) for i in combinations(text_tokens, 1)]\n",
    "#   mol_subs = [frozenset([mol_token_ids[j] for j in i]) for i in combinations(mol_tokens, 1)]\n",
    "\n",
    "#   rules = []\n",
    "\n",
    "#   for t in text_subs:\n",
    "#     for m in mol_subs:\n",
    "#       rules.append((t, m))\n",
    "\n",
    "#   return rules\n",
    "\n",
    "\n",
    "# def ar_score(text_cid, mol_cid, top_num=10):\n",
    "\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[text_cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   gfile = archive.open(mol_cid + '.graph').read().decode('ascii')\n",
    "#   mol_tokens = {}\n",
    "#   idx = False\n",
    "#   for line in gfile.split('\\n'):\n",
    "#     line = line.strip()\n",
    "#     if line == 'idx to identifier:':\n",
    "#       idx = True\n",
    "#       continue\n",
    "#     if idx and len(line) != 0:\n",
    "#       id, idf = line.split(\" \")\n",
    "#       mol_tokens[id] = idf\n",
    "#   mol_tokens = list(mol_tokens.values())\n",
    "\n",
    "#   rules = generate_rules(text_tokens, mol_tokens)\n",
    "\n",
    "#   tmp = np.array([conf[list(r[0])[0], list(r[1])[0]] for r in rules])\n",
    "\n",
    "#   mx = np.min((top_num, len(tmp)))\n",
    "#   top_confs = -np.partition(-tmp, mx-1)[:mx]\n",
    "\n",
    "\n",
    "#   return np.mean(top_confs)\n",
    "\n",
    "# # calculate ar_scores using alpha = 0\n",
    "\n",
    "# alpha = 0.0\n",
    "# print(f'alpha={alpha}')\n",
    "\n",
    "\n",
    "# ar_scores = np.zeros((len(top_cids_val1), num_top))\n",
    "\n",
    "# new_ranks_val = []\n",
    "# for i, cid in enumerate(top_cids_val1):\n",
    "\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   score = np.zeros((num_top))\n",
    "#   for j, cid2 in enumerate(top_cids_val1[cid]):\n",
    "#     gfile = archive.open(cid + '.graph').read().decode('ascii')\n",
    "#     mol_tokens = {}\n",
    "#     idx = False\n",
    "#     for line in gfile.split('\\n'):\n",
    "#       line = line.strip()\n",
    "#       if line == 'idx to identifier:':\n",
    "#         idx = True\n",
    "#         continue\n",
    "#       if idx and len(line) != 0:\n",
    "#         id, idf = line.split(\" \")\n",
    "#         mol_tokens[id] = idf\n",
    "#     mol_tokens = list(mol_tokens.values())\n",
    "\n",
    "#     tmp = ar_score(cid, cid2)\n",
    "#     ar_scores[i,j] = tmp\n",
    "#     score[j] = alpha * scores_val1[cid][j] + (1 - alpha) * tmp\n",
    "\n",
    "#   try:\n",
    "#     old_loc = top_cids_val1[cid].index(cid)\n",
    "\n",
    "#     sorted = np.argsort(-score, kind='stable')\n",
    "\n",
    "#     new_rank = np.where(sorted == old_loc)[0][0] + 1\n",
    "\n",
    "#   except ValueError:\n",
    "#     new_rank = ranks_val1[i]\n",
    "\n",
    "#   new_ranks_val.append(new_rank)\n",
    "\n",
    "\n",
    "#   if (i+1) % 200 == 0: print(i+1)\n",
    "\n",
    "# new_ranks_val = np.array(new_ranks_val)\n",
    "\n",
    "\n",
    "# # alpha 0 - 101\n",
    "# x = np.linspace(0.0,1,101)\n",
    "# MRRs = []\n",
    "# hits1 = []\n",
    "# hits10 = []\n",
    "\n",
    "\n",
    "# for n in x:\n",
    "\n",
    "#   alpha = n\n",
    "#   # print(\"alpha:\", alpha)\n",
    "#   hits_at_one = 0\n",
    "#   hits_at_ten = 0\n",
    "#   hits_at_100 = 0\n",
    "\n",
    "#   tmp_ranks = []\n",
    "#   for i, cid in enumerate(top_cids_val1):\n",
    "\n",
    "#     score = np.zeros((num_top))\n",
    "#     for j, cid2 in enumerate(top_cids_val1[cid]):\n",
    "#       # tmp = ar_score(cid, cid2)\n",
    "#       # ar_scores[i,j] = tmp\n",
    "#       score[j] = alpha * scores_val1[cid][j] + (1 - alpha) * ar_scores[i,j]\n",
    "\n",
    "#     try:\n",
    "#       old_loc = top_cids_val1[cid].index(cid)\n",
    "\n",
    "#       sorted = np.argsort(-score, kind='stable')\n",
    "\n",
    "#       new_rank = np.where(sorted == old_loc)[0][0] + 1\n",
    "\n",
    "#     except ValueError:\n",
    "#       new_rank = ranks_val1[i]\n",
    "\n",
    "#     tmp_ranks.append(new_rank)\n",
    "\n",
    "#     if new_rank <= 1:\n",
    "#         hits_at_one += 1\n",
    "#     if new_rank <= 10:\n",
    "#         hits_at_ten += 1\n",
    "#     if new_rank <= 100:\n",
    "#         hits_at_100 += 1\n",
    "#   print(\"alpha:\", alpha, \", MRR:\", np.mean(1/np.array(tmp_ranks)))\n",
    "\n",
    "#   MRRs.append(np.mean(1/np.array(tmp_ranks)))\n",
    "#   hits1.append(hits_at_one/cids_val1.size)\n",
    "#   hits10.append(hits_at_ten/cids_val1.size)\n",
    "\n",
    "# print(\"Val Mean rank:\", np.mean(tmp_ranks))\n",
    "# print(\"Hits at 1:\", hits_at_one/cids_val1.size)\n",
    "# print(\"Hits at 10:\", hits_at_ten/cids_val1.size)\n",
    "# print(\"Hits at 100:\", hits_at_100/cids_val1.size)\n",
    "\n",
    "# print(\"Validation MRR:\", np.mean(1/np.array(tmp_ranks)))\n",
    "\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "\n",
    "# alpha = x[np.argmax(MRRs)]\n",
    "# print(f'alpha: {alpha}')\n",
    "\n",
    "# ar_scores_test = np.zeros((len(top_cids_test1), num_top))\n",
    "\n",
    "# new_ranks_test = []\n",
    "# for i, cid in enumerate(top_cids_test1):\n",
    "\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   score = np.zeros((num_top))\n",
    "#   for j, cid2 in enumerate(top_cids_test1[cid]):\n",
    "#     gfile = archive.open(cid + '.graph').read().decode('ascii')\n",
    "#     mol_tokens = {}\n",
    "#     idx = False\n",
    "#     for line in gfile.split('\\n'):\n",
    "#       line = line.strip()\n",
    "#       if line == 'idx to identifier:':\n",
    "#         idx = True\n",
    "#         continue\n",
    "#       if idx and len(line) != 0:\n",
    "#         id, idf = line.split(\" \")\n",
    "#         mol_tokens[id] = idf\n",
    "#     mol_tokens = list(mol_tokens.values())\n",
    "\n",
    "#     tmp = ar_score(cid, cid2)\n",
    "#     ar_scores_test[i,j] = tmp\n",
    "#     score[j] = alpha * scores_test1[cid][j] + (1 - alpha) * tmp\n",
    "\n",
    "#   try:\n",
    "#     old_loc = top_cids_test1[cid].index(cid)\n",
    "\n",
    "#     sorted = np.argsort(-score, kind='stable')\n",
    "\n",
    "#     new_rank = np.where(sorted == old_loc)[0][0] + 1\n",
    "\n",
    "#   except ValueError:\n",
    "#     new_rank = ranks_test1[i]\n",
    "\n",
    "#   new_ranks_test.append(new_rank)\n",
    "\n",
    "\n",
    "#   if (i+1) % 200 == 0: print(i+1)\n",
    "\n",
    "# new_ranks_test = np.array(new_ranks_test)\n",
    "\n",
    "# print()\n",
    "# print(\"Test Mean rank:\", np.mean(new_ranks_test))\n",
    "# print(\"Hits at 1:\", np.mean(new_ranks_test <= 1))\n",
    "# print(\"Hits at 10:\", np.mean(new_ranks_test <= 10))\n",
    "# print(\"Hits at 100:\", np.mean(new_ranks_test <= 100))\n",
    "\n",
    "# print(\"Test MRR:\", np.mean(1/np.array(new_ranks_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-v8ekaEjaHiV"
   },
   "source": [
    "#### 3.5.2.4 FPGrowth Model Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qn-zsMC4aZwu"
   },
   "source": [
    "The following script implements an association rule mining approach using the FP-Growth algorithm to uncover frequent patterns within text and molecular tokens, but it does not directly calculate cross-modal associations between text and molecule tokens. Instead, it explores intramodal relationships by generating rules within each modality separately, analyzing how often certain text or molecular token combinations occur together within the training dataset, thereby identifying prevalent substructures or linguistic patterns that could inform the model's predictions. This method aids in understanding the internal dynamics of each data type but lacks a mechanism to link these insights directly across the text and molecular modalities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h5LDDJNbaTiN"
   },
   "outputs": [],
   "source": [
    "#@title Implementation of FPGrowth Association Rules Mining and Ranker\n",
    "\n",
    "# parser = argparse.ArgumentParser(description='Run FPGrowth MRR calculation')\n",
    "# #parser.add_argument('--weights_dir', metavar='weights_dir', type=str,\n",
    "# #                    help='directory where weights is located')\n",
    "# parser.add_argument('--embeddings_dir', metavar='embeddings_dir', type=str,\n",
    "#                     help='directory where embeddings is located')\n",
    "# args = parser.parse_args()\n",
    "# #weights_path = args.weights_dir\n",
    "# embeddings_path = args.embeddings_dir\n",
    "\n",
    "# class GenerateData():\n",
    "#   def __init__(self, path_train, path_val, path_test, path_molecules, path_token_embs):\n",
    "#     self.path_train = path_train\n",
    "#     self.path_val = path_val\n",
    "#     self.path_test = path_test\n",
    "#     self.path_molecules = path_molecules\n",
    "#     self.path_token_embs = path_token_embs\n",
    "\n",
    "#     self.mol_trunc_length = 512\n",
    "#     self.text_trunc_length = 256\n",
    "\n",
    "#     self.prep_text_tokenizer()\n",
    "\n",
    "#     self.load_substructures()\n",
    "\n",
    "#     self.batch_size = 32\n",
    "\n",
    "#     self.store_descriptions()\n",
    "\n",
    "#   def load_substructures(self):\n",
    "#     self.molecule_sentences = {}\n",
    "#     self.molecule_tokens = {}\n",
    "\n",
    "#     total_tokens = set()\n",
    "#     self.max_mol_length = 0\n",
    "#     with open(self.path_molecules) as f:\n",
    "#       for line in f:\n",
    "#         spl = line.split(\":\")\n",
    "#         cid = spl[0]\n",
    "#         tokens = spl[1].strip()\n",
    "#         self.molecule_sentences[cid] = tokens\n",
    "#         t = tokens.split()\n",
    "#         total_tokens.update(t)\n",
    "#         size = len(t)\n",
    "#         if size > self.max_mol_length: self.max_mol_length = size\n",
    "\n",
    "#     self.token_embs = np.load(self.path_token_embs, allow_pickle = True)[()]\n",
    "\n",
    "#   def prep_text_tokenizer(self):\n",
    "#     self.text_tokenizer = BertTokenizerFast.from_pretrained(\"allenai/scibert_scivocab_uncased\")\n",
    "\n",
    "\n",
    "#   def store_descriptions(self):\n",
    "#     self.descriptions = {}\n",
    "\n",
    "#     self.mols = {}\n",
    "\n",
    "\n",
    "\n",
    "#     self.training_cids = []\n",
    "#     #get training set cids...\n",
    "#     with open(self.path_train) as f:\n",
    "#       reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "#       for n, line in enumerate(reader):\n",
    "#         self.descriptions[line['cid']] = line['desc']\n",
    "#         self.mols[line['cid']] = line['mol2vec']\n",
    "#         self.training_cids.append(line['cid'])\n",
    "\n",
    "#     self.validation_cids = []\n",
    "#     #get validation set cids...\n",
    "#     with open(self.path_val) as f:\n",
    "#       reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "#       for n, line in enumerate(reader):\n",
    "#         self.descriptions[line['cid']] = line['desc']\n",
    "#         self.mols[line['cid']] = line['mol2vec']\n",
    "#         self.validation_cids.append(line['cid'])\n",
    "\n",
    "#     self.test_cids = []\n",
    "#     with open(self.path_test) as f:\n",
    "#       reader = csv.DictReader(f, delimiter=\"\\t\", quoting=csv.QUOTE_NONE, fieldnames = ['cid', 'mol2vec', 'desc'])\n",
    "#       for n, line in enumerate(reader):\n",
    "#         self.descriptions[line['cid']] = line['desc']\n",
    "#         self.mols[line['cid']] = line['mol2vec']\n",
    "#         self.test_cids.append(line['cid'])\n",
    "\n",
    "#   #transformers can't take array with full attention so have to pad a 0...\n",
    "#   def padarray(self, A, size, value=0):\n",
    "#       t = size - len(A)\n",
    "#       return np.pad(A, pad_width=(0, t), mode='constant', constant_values = value)\n",
    "\n",
    "\n",
    "#   def generate_examples_train(self):\n",
    "#     \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "#     np.random.shuffle(self.training_cids)\n",
    "\n",
    "#     for cid in self.training_cids:\n",
    "#       label = np.random.randint(2)\n",
    "#       rand_cid = np.random.choice(self.training_cids)\n",
    "#       if label:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "#       else:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "\n",
    "#       text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "#       text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "#       yield {\n",
    "#           'cid': cid,\n",
    "#           'input': {\n",
    "#               'text': {\n",
    "#                 'input_ids': text_ids,\n",
    "#                 'attention_mask': text_mask,\n",
    "#               },\n",
    "#               'molecule' : {\n",
    "#                     'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "#                     'cid' : cid\n",
    "#               },\n",
    "#           },\n",
    "#           'label': label\n",
    "#       }\n",
    "\n",
    "\n",
    "#   def generate_examples_val(self):\n",
    "#     \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "#     np.random.shuffle(self.validation_cids)\n",
    "\n",
    "#     for cid in self.validation_cids:\n",
    "#       label = np.random.randint(2)\n",
    "#       rand_cid = np.random.choice(self.validation_cids)\n",
    "#       if label:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "#       else:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "\n",
    "\n",
    "#       text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "#       text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "#       yield {\n",
    "#           'cid': cid,\n",
    "#           'input': {\n",
    "#               'text': {\n",
    "#                 'input_ids': text_ids,\n",
    "#                 'attention_mask': text_mask,\n",
    "#               },\n",
    "#               'molecule' : {\n",
    "#                     'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "#                     'cid' : cid\n",
    "#               },\n",
    "#           },\n",
    "#           'label': label\n",
    "#       }\n",
    "\n",
    "#   def generate_examples_test(self):\n",
    "#     \"\"\"Yields examples.\"\"\"\n",
    "\n",
    "#     np.random.shuffle(self.test_cids)\n",
    "\n",
    "#     for cid in self.test_cids:\n",
    "#       label = np.random.randint(2)\n",
    "#       rand_cid = np.random.choice(self.test_cids)\n",
    "#       if label:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "#       else:\n",
    "#         text_input = self.text_tokenizer(self.descriptions[rand_cid], truncation=True, max_length=self.text_trunc_length - 1,\n",
    "#                                         padding='max_length', return_tensors = 'np')\n",
    "\n",
    "\n",
    "#       text_ids = self.padarray(text_input['input_ids'].squeeze(), self.text_trunc_length)\n",
    "#       text_mask = self.padarray(text_input['attention_mask'].squeeze(), self.text_trunc_length)\n",
    "\n",
    "#       yield {\n",
    "#           'cid': cid,\n",
    "#           'input': {\n",
    "#               'text': {\n",
    "#                 'input_ids': text_ids,\n",
    "#                 'attention_mask': text_mask,\n",
    "#               },\n",
    "#               'molecule' : {\n",
    "#                     'mol2vec' : np.fromstring(self.mols[cid], sep = \" \"),\n",
    "#                     'cid' : cid\n",
    "#               },\n",
    "#           },\n",
    "#           'label': label\n",
    "#       }\n",
    "\n",
    "# graph_data_path = \"data/mol_graphs.zip\"\n",
    "# dir_emb = embeddings_path\n",
    "# mounted_path_token_embs = \"data/token_embedding_dict.npy\"\n",
    "# mounted_path_train = \"data/training.txt\"\n",
    "# mounted_path_val = \"data/val.txt\"\n",
    "# mounted_path_test = \"data/test.txt\"\n",
    "# mounted_path_molecules = \"data/ChEBI_definitions_substructure_corpus.cp\"\n",
    "# gt = GenerateData(mounted_path_train, mounted_path_val, mounted_path_test, mounted_path_molecules, mounted_path_token_embs)\n",
    "\n",
    "# cids_train1 = np.load(dir_emb + \"cids_train.npy\", allow_pickle=True)\n",
    "# cids_val1 = np.load(dir_emb + \"cids_val.npy\", allow_pickle=True)\n",
    "# cids_test1 = np.load(dir_emb + \"cids_test.npy\", allow_pickle=True)\n",
    "# chem_embeddings_train1 = np.load(dir_emb + \"chem_embeddings_train.npy\")\n",
    "# chem_embeddings_val1 = np.load(dir_emb + \"chem_embeddings_val.npy\")\n",
    "# chem_embeddings_test1 = np.load(dir_emb + \"chem_embeddings_test.npy\")\n",
    "# text_embeddings_train1 = np.load(dir_emb + \"text_embeddings_train.npy\")\n",
    "# text_embeddings_val1 = np.load(dir_emb + \"text_embeddings_val.npy\")\n",
    "# text_embeddings_test1 = np.load(dir_emb + \"text_embeddings_test.npy\")\n",
    "\n",
    "\n",
    "# all_chem_embbedings1 = np.concatenate((chem_embeddings_train1, chem_embeddings_val1, chem_embeddings_test1), axis = 0)\n",
    "\n",
    "# cids_all = np.concatenate((cids_train1, cids_val1, cids_test1), axis = 0)\n",
    "\n",
    "# from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "# def memory_efficient_similarity_matrix_custom(func, embedding1, embedding2, chunk_size = 1000):\n",
    "#     rows = embedding1.shape[0]\n",
    "\n",
    "#     num_chunks = int(np.ceil(rows / chunk_size))\n",
    "\n",
    "#     for i in range(num_chunks):\n",
    "#         end_chunk = (i+1)*(chunk_size) if (i+1)*(chunk_size) < rows else rows #account for smaller chunk at end...\n",
    "#         yield func(embedding1[i*chunk_size:end_chunk,:], embedding2)\n",
    "\n",
    "# #Calculate mean rank, hits at ten\n",
    "\n",
    "# def dot_product(a, b):\n",
    "#   return np.dot(a, b.T)\n",
    "\n",
    "# sigmoid = lambda x: 1 / (1 + np.exp(-x))\n",
    "\n",
    "# compose = lambda a,b: sigmoid(dot_product(a,b))\n",
    "\n",
    "# text_chem_cos1 = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_train1, all_chem_embbedings1)\n",
    "# text_chem_cos_val1 = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_val1, all_chem_embbedings1)\n",
    "# text_chem_cos_test1 = memory_efficient_similarity_matrix_custom(cosine_similarity, text_embeddings_test1, all_chem_embbedings1)\n",
    "\n",
    "# n_train = len(cids_train1)\n",
    "# n_val = len(cids_val1)\n",
    "# n_test = len(cids_test1)\n",
    "# n = n_train + n_val + n_test\n",
    "\n",
    "# offset_val = n_train\n",
    "# offset_test = n_train + n_val\n",
    "\n",
    "# num_top = 10\n",
    "# top_cids1 = {}\n",
    "# top_cids_val1 = {}\n",
    "# top_cids_test1 = {}\n",
    "# scores_val1 = {}\n",
    "# scores_test1 = {}\n",
    "\n",
    "# ranks1 = []\n",
    "# j = 0 #keep track of all loops\n",
    "# for i, emb in enumerate(text_chem_cos1):\n",
    "#     for k in range(emb.shape[0]):\n",
    "#         cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#         ranks = np.argsort(cid_locs) #rank is actually double argsort...\n",
    "\n",
    "#         top_cids1[cids_train1[j]] = [cids_all[loc] for loc in cid_locs[:num_top]]\n",
    "\n",
    "#         rank = ranks[j] + 1\n",
    "#         ranks1.append(rank)\n",
    "\n",
    "\n",
    "#         j += 1\n",
    "#         if (j) % 1000 == 0: print((j), \"train processed.\")\n",
    "\n",
    "# ranks1 = np.array(ranks1)\n",
    "\n",
    "# print()\n",
    "# print(\"MLP Training Mean rank:\", np.mean(ranks1))\n",
    "# print(\"MLP Hits at 1:\", np.mean(ranks1 <= 1))\n",
    "# print(\"MLP Hits at 10:\", np.mean(ranks1 <= 10))\n",
    "# print(\"MLP Hits at 100:\", np.mean(ranks1 <= 100))\n",
    "# print(\"MLP Hits at 500:\", np.mean(ranks1 <= 500))\n",
    "# print(\"MLP Hits at 1000:\", np.mean(ranks1 <= 1000))\n",
    "\n",
    "# print(\"MLP Trainng MRR:\", np.mean(1/np.array(ranks1)))\n",
    "\n",
    "# ranks_val1 = []\n",
    "# j = 0 #keep track of all loops\n",
    "# for i, emb in enumerate(text_chem_cos_val1):\n",
    "#     for k in range(emb.shape[0]):\n",
    "#         cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#         ranks = np.argsort(cid_locs) #rank is actually double argsort...\n",
    "\n",
    "#         scores = np.sort(emb[k,:])[::-1]\n",
    "\n",
    "#         top_cids_val1[cids_val1[j]] = [cids_all[loc] for loc in cid_locs[:num_top]]\n",
    "#         scores_val1[cids_val1[j]] = scores[:num_top]\n",
    "\n",
    "#         rank = ranks[j+offset_val] + 1\n",
    "#         ranks_val1.append(rank)\n",
    "\n",
    "#         j += 1\n",
    "#         if (j) % 1000 == 0: print((j), \"val processed.\")\n",
    "\n",
    "\n",
    "# ranks_val1 = np.array(ranks_val1)\n",
    "\n",
    "# print()\n",
    "# print(\"MLP Val Mean rank:\", np.mean(ranks_val1))\n",
    "# print(\"MLP Hits at 1:\", np.mean(ranks_val1 <= 1))\n",
    "# print(\"MLP Hits at 10:\", np.mean(ranks_val1 <= 10))\n",
    "# print(\"MLP Hits at 100:\", np.mean(ranks_val1 <= 100))\n",
    "# print(\"MLP Hits at 500:\", np.mean(ranks_val1 <= 500))\n",
    "# print(\"MLP Hits at 1000:\", np.mean(ranks_val1 <= 1000))\n",
    "\n",
    "# print(\"MLP Validation MRR:\", np.mean(1/ranks_val1))\n",
    "\n",
    "\n",
    "# ranks_test1 = []\n",
    "# j = 0 #keep track of all loops\n",
    "# for i, emb in enumerate(text_chem_cos_test1):\n",
    "#     for k in range(emb.shape[0]):\n",
    "#         cid_locs = np.argsort(emb[k,:])[::-1]\n",
    "#         ranks = np.argsort(cid_locs) #rank is actually double argsort...\n",
    "\n",
    "#         scores = np.sort(emb[k,:])[::-1]\n",
    "\n",
    "#         top_cids_test1[cids_test1[j]] = [cids_all[loc] for loc in cid_locs[:num_top]]\n",
    "#         scores_test1[cids_test1[j]] = scores[:num_top]\n",
    "\n",
    "#         rank = ranks[j+offset_test] + 1\n",
    "#         ranks_test1.append(rank)\n",
    "\n",
    "#         j += 1\n",
    "#         if (j) % 1000 == 0: print((j), \"test processed.\")\n",
    "\n",
    "\n",
    "# ranks_test1 = np.array(ranks_test1)\n",
    "\n",
    "# print()\n",
    "# print(\"MLP Test Mean rank:\", np.mean(ranks_test1))\n",
    "# print(\"MLP Hits at 1:\", np.mean(ranks_test1 <= 1))\n",
    "# print(\"MLP Hits at 10:\", np.mean(ranks_test1 <= 10))\n",
    "# print(\"MLP Hits at 100:\", np.mean(ranks_test1 <= 100))\n",
    "# print(\"MLP Hits at 500:\", np.mean(ranks_test1 <= 500))\n",
    "# print(\"MLP Hits at 1000:\", np.mean(ranks_test1 <= 1000))\n",
    "\n",
    "# print(\"MLP Test MRR:\", np.mean(1/ranks_test1))\n",
    "\n",
    "\n",
    "# from collections import defaultdict\n",
    "\n",
    "# all_mol_tokens = set()\n",
    "# all_text_tokens = set()\n",
    "\n",
    "# import zipfile\n",
    "# archive = zipfile.ZipFile(graph_data_path, 'r')\n",
    "\n",
    "# for i, cid in enumerate(gt.training_cids):\n",
    "#   #text_input = gt.text_tokenizer(gt.descriptions_train[cid], truncation=True, padding = 'max_length',\n",
    "#   #                                  max_length=gt.text_trunc_length - 1)\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   mol_length = len(gt.molecule_sentences[cid].split())\n",
    "#   mol_tokens = ['[CLS]']\n",
    "#   mol_tokens.extend(gt.molecule_sentences[cid].split()[:mol_length])\n",
    "\n",
    "#   all_mol_tokens.update(mol_tokens)\n",
    "#   all_text_tokens.update(text_tokens)\n",
    "\n",
    "# mol_token_ids = defaultdict(lambda : -1)\n",
    "# text_token_ids = defaultdict(lambda : -1)\n",
    "\n",
    "# mol_token_ids_rev = {}\n",
    "# text_token_ids_rev = {}\n",
    "# for i, k in enumerate(all_mol_tokens):\n",
    "#   mol_token_ids[k] = i\n",
    "#   mol_token_ids_rev[i] = k\n",
    "# for i, k in enumerate(all_text_tokens):\n",
    "#   text_token_ids[k] = i\n",
    "#   text_token_ids_rev[i] = k\n",
    "\n",
    "# #Create database\n",
    "\n",
    "# database = []\n",
    "\n",
    "# for cid in gt.training_cids:\n",
    "#   if cid in gt.validation_cids: continue\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "#   text_tokens = text_tokens[1:-1] #skip [CLS], [SEP]\n",
    "\n",
    "#   mol_length = len(gt.molecule_sentences[cid].split())\n",
    "#   mol_tokens = gt.molecule_sentences[cid].split()[:mol_length]\n",
    "\n",
    "#   all_tokens = []\n",
    "#   all_tokens.extend(text_tokens)\n",
    "#   all_tokens.extend(mol_tokens)\n",
    "\n",
    "#   database.append(all_tokens)\n",
    "\n",
    "# #!pip install mlxtend==0.17.0\n",
    "\n",
    "# from mlxtend.frequent_patterns import fpgrowth\n",
    "\n",
    "# import sys\n",
    "# sys.setrecursionlimit(1500)\n",
    "\n",
    "# from mlxtend.preprocessing import TransactionEncoder\n",
    "# import pandas as pd\n",
    "\n",
    "# te = TransactionEncoder()\n",
    "# te_ary = te.fit(database).transform(database)\n",
    "# df = pd.DataFrame(te_ary, columns=te.columns_)\n",
    "\n",
    "# fp = fpgrowth(df, min_support=0.3, use_colnames=True)\n",
    "\n",
    "# from mlxtend.frequent_patterns import association_rules\n",
    "\n",
    "# rules = association_rules(fp, metric=\"confidence\", min_threshold=0.99)\n",
    "\n",
    "# pd.set_option('display.max_columns', 4)\n",
    "\n",
    "# print(rules[['antecedents', 'consequents', 'support', 'confidence']])\n",
    "\n",
    "# #based off FP-growth for 1->1\n",
    "# normal_support = np.zeros((len(all_text_tokens), len(all_mol_tokens)))\n",
    "# normal_conf = np.zeros((len(all_text_tokens), len(all_mol_tokens)))\n",
    "\n",
    "# #Create database\n",
    "\n",
    "# for i, cid in enumerate(gt.training_cids):\n",
    "\n",
    "#   if cid in gt.validation_cids or cid in gt.test_cids: continue\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   gfile = archive.open(cid + '.graph').read().decode('ascii')\n",
    "#   mol_tokens = {}\n",
    "#   idx = False\n",
    "#   for line in gfile.split('\\n'):\n",
    "#     line = line.strip()\n",
    "#     if line == 'idx to identifier:':\n",
    "#       idx = True\n",
    "#       continue\n",
    "#     if idx and len(line) != 0:\n",
    "#       id, idf = line.split(\" \")\n",
    "#       mol_tokens[id] = idf\n",
    "\n",
    "#   mol_tokens = list(mol_tokens.values())\n",
    "\n",
    "#   if len(mol_tokens) > gt.mol_trunc_length: mol_tokens = mol_tokens[:gt.mol_trunc_length]\n",
    "\n",
    "#   for j, text in enumerate(text_tokens):\n",
    "#     for k, molt in enumerate(mol_tokens):\n",
    "#       normal_support[text_token_ids[text], mol_token_ids[molt]] += 1\n",
    "\n",
    "\n",
    "#   if (i+1) % 1000 == 0: print(i+1)\n",
    "\n",
    "# print(\"Support calculation finished.\")\n",
    "\n",
    "# for j, text in enumerate(all_text_tokens):\n",
    "#   normal_conf[text_token_ids[text], :] = normal_support[text_token_ids[text], :] / np.sum(normal_support[text_token_ids[text], :])\n",
    "\n",
    "#   if (j+1) % 1000 == 0: print(j+1)\n",
    "\n",
    "# print(\"Confidence calculation finished.\")\n",
    "\n",
    "# from itertools import combinations, chain\n",
    "\n",
    "\n",
    "# def all_subsets(ss):#skip empty set\n",
    "#     return chain(*map(lambda x: combinations(ss, x), range(1, len(ss)+1)))\n",
    "\n",
    "\n",
    "# def generate_rules(text_tokens, mol_tokens):\n",
    "#   candidates = set()\n",
    "\n",
    "#   text_subs = [frozenset([text_token_ids[j] for j in i]) for i in combinations(text_tokens, 1)]\n",
    "#   mol_subs = [frozenset([mol_token_ids[j] for j in i]) for i in combinations(mol_tokens, 1)]\n",
    "\n",
    "#   rules = []\n",
    "\n",
    "#   for t in text_subs:\n",
    "#     for m in mol_subs:\n",
    "#       rules.append((t, m))\n",
    "\n",
    "#   return rules\n",
    "\n",
    "\n",
    "# def ar_score(text_cid, mol_cid, top_num=10):\n",
    "\n",
    "#   text_input = gt.text_tokenizer(gt.descriptions[text_cid], truncation=True, padding = 'max_length',\n",
    "#                                     max_length=gt.text_trunc_length - 1)\n",
    "#   text_length = np.sum(text_input['attention_mask'])\n",
    "#   text_tokens = gt.text_tokenizer.convert_ids_to_tokens(text_input['input_ids'][:text_length])\n",
    "\n",
    "#   gfile = archive.open(mol_cid + '.graph').read().decode('ascii')\n",
    "#   mol_tokens = {}\n",
    "#   idx = False\n",
    "#   for line in gfile.split('\\n'):\n",
    "#     line = line.strip()\n",
    "#     if line == 'idx to identifier:':\n",
    "#       idx = True\n",
    "#       continue\n",
    "#     if idx and len(line) != 0:\n",
    "#       id, idf = line.split(\" \")\n",
    "#       mol_tokens[id] = idf\n",
    "#   mol_tokens = list(mol_tokens.values())\n",
    "\n",
    "#   rules = generate_rules(text_tokens, mol_tokens)\n",
    "\n",
    "#   tmp = np.array([normal_conf[list(r[0])[0], list(r[1])[0]] for r in rules])\n",
    "\n",
    "\n",
    "#   mx = np.min((top_num, len(tmp)))\n",
    "#   top_confs = -np.partition(-tmp, mx-1)[:mx]\n",
    "\n",
    "#   return np.mean(top_confs)\n",
    "\n",
    "\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "\n",
    "# alpha = 0.0\n",
    "\n",
    "\n",
    "# hits_at_one = 0\n",
    "# hits_at_ten = 0\n",
    "# hits_at_100 = 0\n",
    "\n",
    "# ar_scores = np.zeros((len(top_cids_val1), num_top))\n",
    "\n",
    "# new_ranks_val = []\n",
    "# for i, cid in enumerate(top_cids_val1):\n",
    "\n",
    "#   score = np.zeros((num_top))\n",
    "#   for j, cid2 in enumerate(top_cids_val1[cid]):\n",
    "#     tmp = ar_score(cid, cid2)\n",
    "#     ar_scores[i,j] = tmp\n",
    "#     score[j] = alpha * scores_val1[cid][j] + (1 - alpha) * tmp\n",
    "#   try:\n",
    "#     old_loc = top_cids_val1[cid].index(cid)\n",
    "\n",
    "#     sorted = np.argsort(-score, kind='stable')\n",
    "\n",
    "#     new_rank = np.where(sorted == old_loc)[0][0] + 1\n",
    "\n",
    "#   except ValueError:\n",
    "#     new_rank = ranks_val1[i]\n",
    "\n",
    "#   new_ranks_val.append(new_rank)\n",
    "\n",
    "#   if new_rank <= 1:\n",
    "#       hits_at_one += 1\n",
    "#   if new_rank <= 10:\n",
    "#       hits_at_ten += 1\n",
    "#   if new_rank <= 100:\n",
    "#       hits_at_100 += 1\n",
    "\n",
    "#   if (i+1) % 200 == 0: print(i+1)\n",
    "\n",
    "# print()\n",
    "# print(\"Val Mean rank:\", np.mean(new_ranks_val))\n",
    "# print(\"Hits at 1:\", hits_at_one/cids_val1.size)\n",
    "# print(\"Hits at 10:\", hits_at_ten/cids_val1.size)\n",
    "# print(\"Hits at 100:\", hits_at_100/cids_val1.size)\n",
    "\n",
    "# print(\"Validation MRR:\", np.mean(1/np.array(new_ranks_val)))\n",
    "\n",
    "\n",
    "# x = np.linspace(0.0,1,101)\n",
    "# MRRs = []\n",
    "# hits1 = []\n",
    "# hits10 = []\n",
    "\n",
    "\n",
    "# for n in x:\n",
    "#   alpha = n\n",
    "\n",
    "#   hits_at_one = 0\n",
    "#   hits_at_ten = 0\n",
    "#   hits_at_100 = 0\n",
    "\n",
    "#   tmp_ranks = []\n",
    "#   for i, cid in enumerate(top_cids_val1):\n",
    "\n",
    "#     score = np.zeros((num_top))\n",
    "#     for j, cid2 in enumerate(top_cids_val1[cid]):\n",
    "#       score[j] = alpha * scores_val1[cid][j] + (1 - alpha) * ar_scores[i,j]\n",
    "\n",
    "#     try:\n",
    "#       old_loc = top_cids_val1[cid].index(cid)\n",
    "\n",
    "#       sorted = np.argsort(-score, kind='stable')\n",
    "\n",
    "#       new_rank = np.where(sorted == old_loc)[0][0] + 1\n",
    "\n",
    "#     except ValueError:\n",
    "#       new_rank = ranks_val1[i]\n",
    "\n",
    "#     tmp_ranks.append(new_rank)\n",
    "\n",
    "#     if new_rank <= 1:\n",
    "#         hits_at_one += 1\n",
    "#     if new_rank <= 10:\n",
    "#         hits_at_ten += 1\n",
    "#     if new_rank <= 100:\n",
    "#         hits_at_100 += 1\n",
    "\n",
    "#   MRRs.append(np.mean(1/np.array(tmp_ranks)))\n",
    "#   hits1.append(hits_at_one/cids_val1.size)\n",
    "#   hits10.append(hits_at_ten/cids_val1.size)\n",
    "\n",
    "# print(\"Val Mean rank:\", np.mean(tmp_ranks))\n",
    "# print(\"Hits at 1:\", hits_at_one/cids_val1.size)\n",
    "# print(\"Hits at 10:\", hits_at_ten/cids_val1.size)\n",
    "# print(\"Hits at 100:\", hits_at_100/cids_val1.size)\n",
    "\n",
    "# print(\"Validation MRR:\", np.mean(1/np.array(tmp_ranks)))\n",
    "\n",
    "\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "\n",
    "# alpha = 0.0\n",
    "\n",
    "\n",
    "# hits_at_one = 0\n",
    "# hits_at_ten = 0\n",
    "# hits_at_100 = 0\n",
    "\n",
    "# ar_scores_test = np.zeros((len(top_cids_test1), num_top))\n",
    "\n",
    "# new_ranks_test = []\n",
    "# for i, cid in enumerate(top_cids_test1):\n",
    "\n",
    "#   score = np.zeros((num_top))\n",
    "#   for j, cid2 in enumerate(top_cids_test1[cid]):\n",
    "\n",
    "#     tmp = ar_score(cid, cid2)\n",
    "#     ar_scores_test[i,j] = tmp\n",
    "#     score[j] = alpha * scores_test1[cid][j] + (1 - alpha) * tmp\n",
    "#   try:\n",
    "#     old_loc = top_cids_test1[cid].index(cid)\n",
    "\n",
    "#     sorted = np.argsort(-score, kind='stable')\n",
    "\n",
    "#     new_rank = np.where(sorted == old_loc)[0][0] + 1\n",
    "\n",
    "#   except ValueError:\n",
    "#     new_rank = ranks_test1[i]\n",
    "\n",
    "#   new_ranks_test.append(new_rank)\n",
    "\n",
    "#   if new_rank <= 1:\n",
    "#       hits_at_one += 1\n",
    "#   if new_rank <= 10:\n",
    "#       hits_at_ten += 1\n",
    "#   if new_rank <= 100:\n",
    "#       hits_at_100 += 1\n",
    "\n",
    "#   if (i+1) % 200 == 0: print(i+1)\n",
    "\n",
    "# print()\n",
    "# print(\"Test Mean rank:\", np.mean(new_ranks_test))\n",
    "# print(\"Hits at 1:\", hits_at_one/cids_test1.size)\n",
    "# print(\"Hits at 10:\", hits_at_ten/cids_test1.size)\n",
    "# print(\"Hits at 100:\", hits_at_100/cids_test1.size)\n",
    "\n",
    "# print(\"Test MRR:\", np.mean(1/np.array(new_ranks_test)))\n",
    "\n",
    "\n",
    "# import operator\n",
    "# from collections import defaultdict\n",
    "\n",
    "# first = np.argmax(MRRs)\n",
    "# last = len(MRRs) - np.argmax(MRRs[::-1])\n",
    "\n",
    "# alpha = (x[first] + x[last])/2\n",
    "# print(alpha)\n",
    "\n",
    "# hits_at_one = 0\n",
    "# hits_at_ten = 0\n",
    "# hits_at_100 = 0\n",
    "\n",
    "# new_ranks_test = []\n",
    "# for i, cid in enumerate(top_cids_test1):\n",
    "\n",
    "\n",
    "#   score = np.zeros((num_top))\n",
    "#   for j, cid2 in enumerate(top_cids_test1[cid]):\n",
    "\n",
    "#     score[j] = alpha * scores_test1[cid][j] + (1 - alpha) * ar_scores_test[i,j]\n",
    "#   try:\n",
    "#     old_loc = top_cids_test1[cid].index(cid)\n",
    "\n",
    "#     sorted = np.argsort(-score, kind='stable')\n",
    "\n",
    "#     new_rank = np.where(sorted == old_loc)[0][0] + 1\n",
    "\n",
    "#   except ValueError:\n",
    "#     new_rank = ranks_test1[i]\n",
    "\n",
    "#   new_ranks_test.append(new_rank)\n",
    "\n",
    "#   if new_rank <= 1:\n",
    "#       hits_at_one += 1\n",
    "#   if new_rank <= 10:\n",
    "#       hits_at_ten += 1\n",
    "#   if new_rank <= 100:\n",
    "#       hits_at_100 += 1\n",
    "\n",
    "#   if (i+1) % 200 == 0: print(i+1)\n",
    "\n",
    "# print()\n",
    "# print(\"Test Mean rank:\", np.mean(new_ranks_test))\n",
    "# print(\"Hits at 1:\", hits_at_one/cids_test1.size)\n",
    "# print(\"Hits at 10:\", hits_at_ten/cids_test1.size)\n",
    "# print(\"Hits at 100:\", hits_at_100/cids_test1.size)\n",
    "\n",
    "# print(\"Test MRR:\", np.mean(1/np.array(new_ranks_test)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GBmutTdyebE"
   },
   "source": [
    "## 3.6 Loading Pretrained MLP1 Model Checkpoint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bl6Qiqkw_2Ll"
   },
   "source": [
    "In the following, we demonstrate loading the pretrained MLP model checkpoint to generate the text embeddings for model evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BNTiB3GvmEiB",
    "outputId": "eb2c070b-eeaa-4047-d35a-640d2e719199"
   },
   "outputs": [],
   "source": [
    "#@title Data Loader Initialization\n",
    "%cd \"/content/tar_files/code\"\n",
    "from dataloaders import GenerateData\n",
    "\n",
    "path_train = training_data\n",
    "path_val = val_data\n",
    "path_test = test_data\n",
    "path_molecules = \"/content/tar_files/data/ChEBI_definitions_substructure_corpus.cp\"\n",
    "path_token_embs = \"/content/tar_files/data/token_embedding_dict.npy\"\n",
    "text_trunc_length = 256\n",
    "text_length_ablation = \"none\"\n",
    "gd = GenerateData(text_trunc_length, path_train, path_val, path_test, path_molecules, path_token_embs, text_length_ablation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yTpUo_btjZyd",
    "outputId": "88b9beec-8244-4c84-e6ed-0d2d7fa3630e"
   },
   "outputs": [],
   "source": [
    "# @title Load the MLP1 Model Checkpoint and Generate Test Embeddings Using the Checkpoint\n",
    "\n",
    "# generate embeddings using model checkpoint and then calculate metrics\n",
    "!mkdir \"/content/test_embeddings/\"\n",
    "\n",
    "CHECKPOINT = '/content/tar_files/2024_4_8_epoch40_sample100_mlp1/test_outputfinal_weights.40.pt'\n",
    "\n",
    "cids_test = np.array([])\n",
    "chem_embeddings_test = np.array([])\n",
    "text_embeddings_test = np.array([])\n",
    "ablation_option = AblationOption(\n",
    "    normalization_layer_removal=False,\n",
    "    max_pool=False,\n",
    "    hidden_layer_removal=False,\n",
    "    conv_layer_removal=False,\n",
    "    add_dropout=False,\n",
    "    change_loss=False,\n",
    "    text_length_ablation='none'\n",
    ")\n",
    "\n",
    "model = MLPModel(ninp=768, nhid=600, nout=300, ablation_option=ablation_option)\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    model.load_state_dict(torch.load(CHECKPOINT))\n",
    "else:\n",
    "    model.load_state_dict(torch.load(CHECKPOINT, map_location=torch.device('cpu')))\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model.eval()\n",
    "\n",
    "print(device)\n",
    "\n",
    "tmp = model.to(device)\n",
    "\n",
    "\n",
    "def get_emb(d, graph_batcher = None):\n",
    "    with torch.no_grad():\n",
    "        cid = np.array([d['cid']])\n",
    "        text_mask = torch.Tensor(d['input']['text']['attention_mask']).bool().reshape(1,-1).to(device)\n",
    "\n",
    "        text = torch.Tensor(d['input']['text']['input_ids']).long().reshape(1,-1).to(device)\n",
    "        molecule = torch.Tensor(d['input']['molecule']['mol2vec']).float().reshape(1,-1).to(device)\n",
    "\n",
    "        text_emb, chem_emb = model(text, molecule, text_mask)\n",
    "\n",
    "        chem_emb = chem_emb.cpu().numpy()\n",
    "        text_emb = text_emb.cpu().numpy()\n",
    "\n",
    "    return cid, chem_emb, text_emb\n",
    "\n",
    "for d in gd.generate_examples_test():\n",
    "    cid, chem_emb, text_emb = get_emb(d)\n",
    "    cids_test = np.concatenate((cids_test, cid)) if cids_test.size else cid\n",
    "    chem_embeddings_test = np.concatenate((chem_embeddings_test, chem_emb)) if chem_embeddings_test.size else chem_emb\n",
    "    text_embeddings_test = np.concatenate((text_embeddings_test, text_emb)) if text_embeddings_test.size else text_emb\n",
    "\n",
    "print(\"Test Embeddings done:\", cids_test.shape, chem_embeddings_test.shape)\n",
    "\n",
    "\n",
    "np.save(\"/content/test_embeddings/cids_test.npy\", cids_test)\n",
    "np.save(\"/content/test_embeddings/chem_embeddings_test.npy\", chem_embeddings_test)\n",
    "np.save(\"/content/test_embeddings/text_embeddings_test.npy\", text_embeddings_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcBfHq536_aM"
   },
   "source": [
    "---\n",
    "# <font color=blue>4. RESULTS</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gX6bCcZNuxmz"
   },
   "source": [
    "We successfully reproduced the original experiments, achieving similar model performance at epoch 15 as reported in the original paper. However, upon extending the training duration to 40 epochs, our results surpassed the reported metrics by approximately 25% across all models.\n",
    "\n",
    "Our findings partially support *Hypothesis 1*, indicating that while the integrity of certain model architectures and hyperparameters are essential for optimal performance, not all components contribute equally. Our systematic ablation studies demonstrate that only specific structures within the model are critical in enhancing model performance, for example, normalization layers. Additionally, our findings underscore the importance of carefully selecting hyperparameters and loss functions, as evidenced by the impact of different options, e.g., changing loss function to `naive_loss`.\n",
    "\n",
    "Our experiments with ensembles of MLP and GCN models demonstrated a consistent improvement in model performance metrics, including MRR, Hits@1 and mean ranks. These results robustly support *Hypothesis 2*, confirming that an ensemble of multiple MLP and GCN models yields improvements compared to individual models operating in isolation.\n",
    "\n",
    "A natural followup question would be \"why the ensemble strategy work?\". In exploring beyond the original study, we conducted additional experiments involving different ensemble strategies. These modifications, including using max rank and weighted rank average, explored the lower-bound of the original ensemble strategy and its inner reranking mechanism.\n",
    "\n",
    "Additional studies included segmenting text by length, which revealed that longer texts generally improve performance. Given the lower general Hits@1 rate observed in the original study, instead of direct query, we utilized t-SNE visualizations of embeddings to delve deeper into the shared semantic spaces between text and molecules, providing nuanced insights into why specific text descriptions correlate with certain molecular structures.\n",
    "\n",
    "Our attempt to replicate the integration of the attention layer resulted in a lesser improvement in the performance of the MLP1 model compared to the findings reported in the original paper. This disparity is likely attributed to the already heightened performance of the MLP1 baseline, which was achieved through extensive training over 40 epochs. Consequently, the marginal enhancements from the attention layer's generated rules may have been constrained, as these rules are derived from the same dataset.\n",
    "\n",
    "In order to delve into the 'pure' impact of the attention layer, we conducted additional experiment to compare the GCN baseline model with the attention layer trained with transformer decoder and GCN, isolating the impact of the attention layer to assess its direct effect on model performance. These comprehensive experiments help pinpoint crucial elements that influence or enhance model efficacy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3YIZ-bDN331i"
   },
   "source": [
    "## 4.1 Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hr3-zPW47OGY"
   },
   "source": [
    "### 4.1.1 MLP Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KIk19QuH-tYM"
   },
   "source": [
    "####4.1.1.1 MLP1 Model Checkpoint Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ebuvAQ5N422H",
    "outputId": "49c48801-0751-4806-bde7-377c8176bc45"
   },
   "outputs": [],
   "source": [
    "# @title Pretrained MLP1 Model: Evaluation Results\n",
    "%cd /content\n",
    "!python3 tar_files/code/ranker_test.py tar_files/2024_4_8_epoch40_sample100_mlp1/embeddings test_embeddings --test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Eu7YxGmphx0"
   },
   "source": [
    "The original study reported an average MRR of 0.370 for the baseline MLP model on the validation set. In contrast, our experiments consistently demonstrated a higher average MRR of approximately 0.50 on the validation set. To explore this difference further, we analyzed the loss curves associated with the experiment.\n",
    "\n",
    "Note that before generating test embeddings with the MLP1 model checkpoint, the dataloader initializes with shuffled data. This may lead to minor variations in model evaluation results, as the test set encountered may differ from the initial evaluation. Therefore, these variations should not be mistaken for irreproducibility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hFbqHR1l-1XB"
   },
   "source": [
    "####4.1.1.2 Loss of the MLP Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jbcPHCc2xxtv"
   },
   "outputs": [],
   "source": [
    "# @title Pretrained MLP1 Model: Loss vs. Epochs\n",
    "def ReadModelTrainingLog(filename):\n",
    "    res = pd.read_csv(filename)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "HVOAQ9t3x5s8",
    "outputId": "62c48a70-e28b-4418-c5cd-87f6b9e86648"
   },
   "outputs": [],
   "source": [
    "filename3 = '/content/tar_files/training loss_validation loss_computation_time/mlp1.csv'\n",
    "\n",
    "res = ReadModelTrainingLog(filename3)\n",
    "plt.plot(res['Epoch'], res[['Train Loss', 'Valid Loss']])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Figure 3. MLP Training and Validation Losses by Epoch')\n",
    "plt.legend(['Train Loss', 'Valid Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9UUrrf4upprm"
   },
   "source": [
    "As illustrated in Figure 3, both training and validation losses show a significant decline initially, followed by a gradual decrease as the number of epochs increases. To investigate potential overfitting, we monitored the validation loss, which did not increase with additional epochs. Early stopping trials at epochs 15, 20, and 25 yielded average MRR of 0.38, 0.42, and 0.45, respectively (See 4.4.2). The performance at epoch 15 aligns closely with the results reported in the original paper. However, continued losses over validation set beyond epoch 15 suggest that overfitting is unlikely."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lfycR9AZ5AqX"
   },
   "source": [
    "#### 4.1.1.3 All MLP Models Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BQOwSOuU9C9z"
   },
   "source": [
    "As shown in Table 2, all three MLP models achieved MRR values slightly over 0.50 on the test set, with scores ranging from 0.501 to 0.507. Despite a Hits@10 rate of approximately 83%, it's crucial to note that the Hits@1 rate stands at around 34%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MeerBi8T7ulR"
   },
   "source": [
    "### 4.1.2 GCN Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qXrUk1tj_Fuf"
   },
   "source": [
    "####4.1.2.1 Loss of the GCN Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "xV0y8QfgqGQL",
    "outputId": "30c91c85-cbb9-4e35-b33a-add22184fba5"
   },
   "outputs": [],
   "source": [
    "# @title Pretrained GCN1 Model: Loss vs. Epochs\n",
    "filename4 = '/content/tar_files/training loss_validation loss_computation_time/gcn1.csv'\n",
    "\n",
    "res = ReadModelTrainingLog(filename4)\n",
    "plt.plot(res['Epoch'], res[['Train Loss', 'Valid Loss']])\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Figure 4. GCN Training and Validation Losses by Epoch')\n",
    "plt.legend(['Train Loss', 'Valid Loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B9EHl-Z9qJk3"
   },
   "source": [
    "The loss patterns observed in GCN models are analogous to those in MLP models, characterized by a rapid initial decline followed by a more gradual decrease as the number of epochs increases. Additionally, the validation loss for GCN models also decreases over epochs, albeit at a slower rate compared to the training loss."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MUNLBHrF_Bok"
   },
   "source": [
    "####4.1.2.1 All GCN Models Evaluation Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8s0jXbRlpxhG"
   },
   "source": [
    "As illustrated in Table 2, baseline GCN models exhibit marginally lower performance compared to MLP models, with the mean rank, MRR, Hits@1, and Hits@10 on the test set all recording lower values for GCNs. The test MRR for GCN models ranges between 0.486 to 0.502."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9J6pqtLl76zS"
   },
   "source": [
    "## 4.2 Ensembles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3C_ge2oa9hWG"
   },
   "source": [
    "### 4.2.1 MLP-ensembles, GCN-ensembles and All-ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MPyCR1R_hSP2"
   },
   "outputs": [],
   "source": [
    "# @title Evaluation Results for All Single and Ensemble MLP and GCN Models\n",
    "import pandas as pd\n",
    "\n",
    "def ReadModelResult(filename):\n",
    "    res = pd.read_csv(filename, index_col=0)\n",
    "    res.columns = [\n",
    "        'Mean rank',\n",
    "        'Hits at 1',\n",
    "        'Hits at 10',\n",
    "        'Hits at 100',\n",
    "        'Hits at 500',\n",
    "        'Hits at 1000',\n",
    "        'MRR',\n",
    "    ]\n",
    "    res.index = [\n",
    "        'Training',\n",
    "        'Validation',\n",
    "        'Test',\n",
    "    ]\n",
    "    return res\n",
    "\n",
    "def GetModelMetricsForTable(metrics, row_name):\n",
    "    res = metrics.loc[['Training', 'Test'], ['Mean rank', 'MRR', 'Hits at 1', 'Hits at 10']]\n",
    "    res = res.stack().to_frame().T\n",
    "    res.columns = ['_'.join(map(str, col)).strip() for col in res.columns.values]\n",
    "    res.reset_index(drop=True, inplace=True)\n",
    "    res.index = [row_name]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rj2jCdCOozBB"
   },
   "outputs": [],
   "source": [
    "def GenerateTable(model_result_file_map):\n",
    "    table = None\n",
    "    for model_name, file_name in model_result_file_map.items():\n",
    "        res = ReadModelResult(file_name)\n",
    "        metrics = GetModelMetricsForTable(res, model_name)\n",
    "\n",
    "        if table is None:\n",
    "            table = metrics\n",
    "        else:\n",
    "            table = pd.concat([table, metrics])\n",
    "    return table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 462
    },
    "id": "zQlIg7JKlWhn",
    "outputId": "653f75b5-07b3-495c-c503-8cc9a08f2760"
   },
   "outputs": [],
   "source": [
    "model_result_file_map_2 = {\n",
    "    'MLP1': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "    'MLP2': '/content/tar_files/model evaluation results/mlp2_eval.csv',\n",
    "    'MLP3': '/content/tar_files/model evaluation results/mlp3_eval.csv',\n",
    "    'GCN1': '/content/tar_files/model evaluation results/gcn1_eval.csv',\n",
    "    'GCN2': '/content/tar_files/model evaluation results/gcn2_eval.csv',\n",
    "    'GCN3': '/content/tar_files/model evaluation results/gcn3_eval.csv',\n",
    "    'MLP-Ensemble': '/content/tar_files/model evaluation results/ensemble_mlp1_mlp2_mlp3_eval.csv',\n",
    "    'GCN-Ensemble': '/content/tar_files/model evaluation results/ensemble_gcn1_gcn2_gcn3_eval.csv',\n",
    "    'All-Emsemble': '/content/tar_files/model evaluation results/ensemble_mlp1_mlp2_mlp3_gcn1_gcn2_gcn3_eval.csv',\n",
    "}\n",
    "\n",
    "table2 = GenerateTable(model_result_file_map_2)\n",
    "display(HTML('<h3>Table 2. Evaluation Results for Single and Ensemble MLP and GCN Models </h3>'))\n",
    "table2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jb-NHaiAqaeB"
   },
   "source": [
    "Shown in Table 2, we evaluated that the ensemble of all three MLP models, as well as the ensemble of all three GCN models, both demonstrated improved performance compared to any individual model. Moreover, the all-ensemble configuration, which combines three MLPs and three GCN models, achieved the highest performance metrics, recording a mean rank of 16.38 and an MRR of 0.593 in our study. These findings corroborate the primary assertion in the original article, that ensembles of MLP or GCN models consistently outperform their single-component counterparts, supporting Hypothesis 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 331
    },
    "id": "nBGJM-p7J7t2",
    "outputId": "efe23c4d-3e61-49f7-b80f-a4b4e6998b77"
   },
   "outputs": [],
   "source": [
    "# @title Overview of MRRs on Test Sets for Baseline Models and Ensembles\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "table2['Test_MRR'].plot(kind='line', figsize=(12, 4), title='Figure 5. Comparison of MRRs on Test Sets Across Single and Ensemble Models ')\n",
    "plt.xlabel('Single and Ensemble Models')\n",
    "plt.ylabel('MRR on Test Set')\n",
    "plt.gca().spines[['top', 'right']].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-zOcjnChABiK"
   },
   "source": [
    "Note that Figure 5 illustrates that the all-ensemble strategy achieved a 22% higher MRR compared to the lowest MRR observed in the GCN1 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 335
    },
    "id": "hsEhY8MXJm1a",
    "outputId": "ff29eb60-24b1-4a5a-f8b9-c0bbb6c2d5a9"
   },
   "outputs": [],
   "source": [
    "# @title Overview of Mean Ranks on Test Sets for Baseline Models and Ensembles\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "table2['Test_Mean rank'].plot(kind='line', figsize=(12, 4), title='Figure 6. Comparison of Mean Ranks on Test Sets Across Single and Ensemble Models')\n",
    "plt.xlabel('Single and Ensemble Models')\n",
    "plt.ylabel('Mean Ranks on Test Sets')\n",
    "plt.gca().spines[['top', 'right']].set_visible(False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IgngDHdNA1uT"
   },
   "source": [
    "However, if we consider the mean rank of 16.37 for the all-ensemble model, it shows that, on average, the target molecule appear around the 16th position as shown in Figure 6."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uJvXTECL9t97"
   },
   "source": [
    "### 4.2.2 Hybrid Ensembles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W6J0Yt1Hqm-t"
   },
   "outputs": [],
   "source": [
    "# @title Variations of MRRs on Validation Sets For Different Combinations of MLP and GCN Architectures\n",
    "def GetValidationMRR(metrics):\n",
    "    return metrics.loc['Validation', 'MRR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yZ2JW1929BI_"
   },
   "outputs": [],
   "source": [
    "def GetTestMRR(metrics):\n",
    "    return metrics.loc['Test', 'MRR']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "xzlS9HvVqxKN",
    "outputId": "7bf689b1-eccb-4c21-d036-18409a9cf351"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "result = np.zeros([4,4])\n",
    "for mlp_n in range(0,4):\n",
    "    mlp_s = '_'.join(['mlp'+str(i) for i in range(1, mlp_n+1)])\n",
    "    for gcn_n in range(0, 4):\n",
    "        gcn_s = '_'.join(['gcn'+str(i) for i in range(1, gcn_n+1)])\n",
    "\n",
    "        s = '_'.join([mlp_s, gcn_s]).strip('_')\n",
    "\n",
    "        if mlp_n == 0 and gcn_n == 0:\n",
    "            continue\n",
    "        if mlp_n == 1 and gcn_n == 0:\n",
    "            file_name_7 = f'mlp1_eval.csv'\n",
    "        elif mlp_n == 0 and gcn_n == 1:\n",
    "            file_name_7 = f'gcn1_eval.csv'\n",
    "        else:\n",
    "            file_name_7 = f'ensemble_{s}_eval.csv'\n",
    "        file_name_7 = f'/content/tar_files/model evaluation results/{file_name_7}'\n",
    "\n",
    "        res = ReadModelResult(file_name_7)\n",
    "        mrr = GetValidationMRR(res)\n",
    "        #mrr = GetTestMRR(res)\n",
    "\n",
    "        result[mlp_n, gcn_n] = mrr\n",
    "\n",
    "im = plt.imshow(result, cmap='viridis')\n",
    "plt.colorbar(im)\n",
    "plt.xlabel('Number of MLP Models')\n",
    "plt.ylabel('Number of GCN Models')\n",
    "plt.title('Figure 7. Variations of MRRs on Validation Set For Different Combinations of Architectures')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xticks(np.arange(result.shape[1]), labels=np.arange(result.shape[1]))\n",
    "plt.yticks(np.arange(result.shape[0]), labels=np.arange(result.shape[0]))\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    for j in range(result.shape[1]):\n",
    "        plt.text(i, j, format(result[i, j], '.3f'), ha='center', va='center', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XSX08vNNqhFi"
   },
   "source": [
    "We further reproduced the advantageous effect of a hybrid ensemble consisting of MLP and GCN models compared to ensembles comprising solely MLP or GCN models. As depicted in Figure 7, the hybrid ensembles, for example, two MLP models and one GCN model, achieved a validation MRR of 0.558, surpassing the performance of the ensembles with only three MLPs or GCNs, which posted MRRs of 0.554 and 0.543, respectively. This finding lends additional support to Hypothesis 2, affirming that hybrid ensembles capitalize on the strengths of both model types."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 472
    },
    "id": "KIbAUPF1iriC",
    "outputId": "ed34fc8b-65fe-4760-cc28-e75a153a16fe"
   },
   "outputs": [],
   "source": [
    "# @title Variations of MRRs on Test Sets For Different Combinations of MLP and GCN Architectures\n",
    "result = np.zeros([4,4])\n",
    "for mlp_n in range(0,4):\n",
    "    mlp_s = '_'.join(['mlp'+str(i) for i in range(1, mlp_n+1)])\n",
    "    for gcn_n in range(0, 4):\n",
    "        gcn_s = '_'.join(['gcn'+str(i) for i in range(1, gcn_n+1)])\n",
    "\n",
    "        s = '_'.join([mlp_s, gcn_s]).strip('_')\n",
    "\n",
    "        if mlp_n == 0 and gcn_n == 0:\n",
    "            continue\n",
    "        if mlp_n == 1 and gcn_n == 0:\n",
    "            file_name_8 = f'mlp1_eval.csv'\n",
    "        elif mlp_n == 0 and gcn_n == 1:\n",
    "            file_name_8 = f'gcn1_eval.csv'\n",
    "        else:\n",
    "            file_name_8 = f'ensemble_{s}_eval.csv'\n",
    "        file_name_8 = f'/content/tar_files/model evaluation results/{file_name_8}'\n",
    "\n",
    "        res = ReadModelResult(file_name_8)\n",
    "        mrr = GetTestMRR(res)\n",
    "\n",
    "        result[mlp_n, gcn_n] = mrr\n",
    "\n",
    "im = plt.imshow(result, cmap='winter')\n",
    "plt.colorbar(im)\n",
    "plt.xlabel('Number of MLP Models')\n",
    "plt.ylabel('Number of GCN Models')\n",
    "plt.title('Figure 8. Variations of MRRs on Test Set For Different Combinations of Architectures')\n",
    "plt.gca().invert_yaxis()\n",
    "\n",
    "plt.xticks(np.arange(result.shape[1]), labels=np.arange(result.shape[1]))\n",
    "plt.yticks(np.arange(result.shape[0]), labels=np.arange(result.shape[0]))\n",
    "\n",
    "for i in range(result.shape[0]):\n",
    "    for j in range(result.shape[1]):\n",
    "        plt.text(i, j, format(result[i, j], '.3f'), ha='center', va='center', color='black')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tEUHXEbIDTMP"
   },
   "source": [
    "\n",
    "\n",
    "We extended the model evaluation to the test set. Unlike the validation set, test set is used to evaluate the model performance after all training and tuning have been completed. Shown in Figure 8, the all ensemble model achieved the highest MRR of 0.593 on the test set, indicating the model's performance over unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g8b5tnaPwPgI"
   },
   "source": [
    "## 4.3  Cross-modal Attention Rerank"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eQOm922amYq1"
   },
   "source": [
    "### 4.3.1 Baseline MLP Model with Cross-modal Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 489
    },
    "id": "al-91GYG2QjQ",
    "outputId": "723417d9-96ef-480b-d639-086d434dd570"
   },
   "outputs": [],
   "source": [
    "# @title Finding the Optimal Alpha: MRRs on Validation Set vs. Alpha values\n",
    "filename_9 = '/content/tar_files/training loss_validation loss_computation_time/mlp_attention.csv'\n",
    "\n",
    "res = ReadModelTrainingLog(filename_9)\n",
    "plt.plot(res['alpha'], res[['MRR']])\n",
    "plt.xlabel('Alpha')\n",
    "plt.ylabel('MRR')\n",
    "plt.axvline(0.97, linestyle='dashed', color='r')\n",
    "plt.title('Figure 9. Validation data MRR for alpha values from 0 to 1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aucOrUGf9xjI"
   },
   "source": [
    "Upon evaluation on the validation set, we found that an alpha value of 0.97 yielded the highest MRR, as depicted in Figure 9. This indicates that the rules generated by the attention model contributed minimally (3%) to enhancing the performance of the original MLP model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "TSUj9f-9xRkq",
    "outputId": "d8321f2b-f65b-4452-a98c-1f1b3b359c73"
   },
   "outputs": [],
   "source": [
    "# @title MLP1 vs. MLP1 with Cross-modal Attention Layer\n",
    "model_result_file_map_attn = {\n",
    "    'MLP1': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "    'MLP1+Attention': '/content/tar_files/model evaluation results/mlp_attention_eval.csv',\n",
    "}\n",
    "\n",
    "table3 = GenerateTable(model_result_file_map_attn)\n",
    "display(HTML(\"<h3>Table 3. Evaluation Results for Cross-modal Attention</h3>\"))\n",
    "table3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oBbmUYn8deG"
   },
   "source": [
    "\n",
    "In comparison to MLP1, the incorporation of the attention layer led to a marginal enhancement of 0.27% over MRR on the test set (0.505 compared to 0.504). This contrasts with the 0.8% improvement observed in the original paper (0.375 compared to 0.372)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hKQdERQQlyTe"
   },
   "source": [
    "### 4.3.2 Baseline MLP Model with FPGrowth Association Rules"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "id": "PPUgSYjVm4Qm",
    "outputId": "8a40400f-0e34-4ce6-f236-5ec37f2fc02a"
   },
   "outputs": [],
   "source": [
    "#@title MLP1 vs. MLP1 with FPGrowth-mind Association Rules\n",
    "model_result_file_map_fpgrowth = {\n",
    "    'MLP1': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "    'MLP1+FPGrowth': '/content/tar_files/model evaluation results/mlp_fpgrowth_eval.csv',\n",
    "}\n",
    "\n",
    "table4 = GenerateTable(model_result_file_map_fpgrowth)\n",
    "display(HTML(\"<h3>Table 4. Evaluation Results for Cross-modal Attention</h3>\"))\n",
    "table4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iYoCopVEGPQ2"
   },
   "source": [
    "Similarly, we noted that the MLP1 model, when integrated with the FPGrowth algorithm to mine association rules, marginally increased the test MRR by 0.02%. This observation is consistent with the findings in the original paper, where the FPGrowth algorithm demonstrated a slight improvement, though less significant than that achieved with the cross-modal attention layer.\n",
    "\n",
    "Overall, our results lend weak support to Hypothesis 3, albeit to a much lesser extent than anticipated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8EAWAy_LwHlV"
   },
   "source": [
    "## 4.4  Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8bQA9ZQQH3_A"
   },
   "source": [
    "To assess Hypothesis 1, which posits that all components of the model architectures are essential and that hyperparameters are tuned to achieve optimal performance, such that any modifications would lead to a decline in model efficacy, we designed and conducted comprehensive ablation studies. The results are presented in Table 5-9."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "R02YtG0lp_2b"
   },
   "source": [
    "### 4.4.1 Impact of Model Architectural Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 478
    },
    "id": "4zWgMvCKqe2h",
    "outputId": "0b7a7fc5-fdda-4b1a-c4a4-550821deb976"
   },
   "outputs": [],
   "source": [
    "# @title Impacts of Architectural Modifications on MLP1 Model\n",
    "model_result_file_map_5 = {\n",
    "    'MLP1 Baseline': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "    'MLP1 Ablation: remove one molecule encoder hidden layer': '/content/tar_files/model evaluation results/ablation_remove_molecule_encoder_hidden_layer_2_mlp_eval.csv',\n",
    "    'MLP1 Ablation: remove all nomralization layers': '/content/tar_files/model evaluation results/ablation_remove_normalization_layers_mlp_eval.csv',\n",
    "    'MLP1 Ablation: add dropout rate of 0.5': '/content/tar_files/model evaluation results/ablation_add_dropout_rate_0.5_mlp_eval.csv'\n",
    "}\n",
    "\n",
    "table5 = GenerateTable(model_result_file_map_5)\n",
    "display(HTML(\"<h3>Table 5. Evaluation Results for Ablation Studies: MLP Model Architectural Modifications</h3>\"))\n",
    "table5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hL6U7MbTqpa3"
   },
   "source": [
    "\n",
    "\n",
    "In terms of model architecture, shown in Table 5, using the MLP1 model as a baseline, the removal of normalization layers resulted in the most significant performance deterioration, with the Test MRR plummeting from 0.504 to 0.332. Conversely, introducing a dropout rate of 0.5 post-ReLU activation led to a marginal decrease in performance. However, eliminating the middle hidden layer in the molecule encoder did not significantly affect the model's performance.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "KWba3IGusNUP",
    "outputId": "0cad165c-e377-44a8-c59e-8dbc4e9a0506"
   },
   "outputs": [],
   "source": [
    "# @title Impacts of Architectural Modifications on GCN1 Model\n",
    "model_result_file_map_6 = {\n",
    "    'GCN1 Baseline': '/content/tar_files/model evaluation results/gcn1_eval.csv',\n",
    "    'GCN1 Ablation: remove one convolutional layer': '/content/tar_files/model evaluation results/ablation_remove_convolutional_layer_2_gcn_eval.csv',\n",
    "    'GCN1 Ablation: change to maximum pooling': '/content/tar_files/model evaluation results/ablation_max_pool_gcn_eval.csv'\n",
    "}\n",
    "\n",
    "table6 = GenerateTable(model_result_file_map_6)\n",
    "display(HTML(\"<h3>Table 6. Evaluation Results for Ablation Studies: GCN Model Architectural Modifications</h3>\"))\n",
    "table6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9N927IJQIiuJ"
   },
   "source": [
    "Similarly, shown in Table 6, removing the middle convolutional layer from the GCN model or substituting mean pooling with max pooling led to slight improvements, with MRR increasing to 0.490 and 0.493 respectively."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vk1HEh8ytW_N"
   },
   "source": [
    "### 4.4.2 Impact of Hyperparameters Modifications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 378
    },
    "id": "L_hlQc7jpEu1",
    "outputId": "9398a44c-bd96-4f62-f348-98fc7b256b6c"
   },
   "outputs": [],
   "source": [
    "# @title Impacts of Changing Hyperparameters During Model Training\n",
    "model_result_file_map_7 = {\n",
    "    'MLP1 Baseline': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "    'MLP1 Ablation: reduce batch size': '/content/tar_files/model evaluation results/ablation_reduce_batch_size_16_mlp_eval.csv',\n",
    "    'MLP1 Ablation: reduce learning rates': '/content/tar_files/model evaluation results/ablation_reduce_learning_rate_mlp_eval.csv'\n",
    "}\n",
    "\n",
    "table7 = GenerateTable(model_result_file_map_7)\n",
    "display(HTML(\"<h3>Table 7. Evaluation Results for Ablation Studies: Hyperparameters Modifications</h3>\"))\n",
    "table7"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "S9-Dg_T-IvMv"
   },
   "source": [
    "\n",
    "Regarding hyperparameters, shown in Table 7, a half reduction in batch size from 32 to 16 and halving the learning rate from 1e-4 to 5e-5 both resulted in decreased performance for the MLP model, with test MRRs dropping from 0.504 to 0.467 and 0.479 respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 322
    },
    "id": "6eako9C7C24y",
    "outputId": "7bad55b9-e6eb-4f1a-c9ea-0154e2df7b87"
   },
   "outputs": [],
   "source": [
    "#@title Investigate the Relations Between Early Stopping and Model Evaluation Results\n",
    "model_result_file_map_8 = {\n",
    "    'MLP epoch15': '/content/tar_files/model evaluation results/mlp_epoch15_eval.csv',\n",
    "    'MLP epoch20': '/content/tar_files/model evaluation results/mlp_epoch20_eval.csv',\n",
    "    'MLP epoch25': '/content/tar_files/model evaluation results/mlp_epoch25_eval.csv',\n",
    "    'MLP epoch40': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "}\n",
    "\n",
    "table8 = GenerateTable(model_result_file_map_8)\n",
    "display(HTML('<h3>Table 8. Dose-response Relations Between Model Evaluation Results and Training Epoches </h3>'))\n",
    "table8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDPMlYn4Jmb1"
   },
   "source": [
    "To further explore the influence of epoch count on model performance, we investigated models that were halted early at epochs 15, 20, and 25. We observed that as the number of epochs increased, so did the test MRR, suggesting that early stopping may not be advantageous for this model's training. Notably, our results at epoch 15 were closely aligned with those reported in the original article."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TdRvCLGkuh2i"
   },
   "source": [
    "### 4.4.3 Impact of Learning Objective Modification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 277
    },
    "id": "aygf9vb6vCkE",
    "outputId": "2887be71-f9a1-484e-c585-f5410ff12746"
   },
   "outputs": [],
   "source": [
    "# @title Impact of Changing Loss\n",
    "model_result_file_map_9 = {\n",
    "    'MLP1 Baseline': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "    'MLP1 Ablation: change loss function': '/content/tar_files/model evaluation results/ablation_change_loss_function_eval.csv'\n",
    "}\n",
    "\n",
    "table9 = GenerateTable(model_result_file_map_9)\n",
    "display(HTML(\"<h3>Table 9. Evaluation Results for Ablation Studies: Changing Loss Function</h3>\"))\n",
    "table9"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dnflTl36JM3d"
   },
   "source": [
    "For the Symmetric Contrastive Loss Function used in the training of the MLP and GCN models, changing the symmetric loss function to a naive loss function that only accounts for the cross-entropy loss between the text embeddings and the molecule embeddings reduced the performance of the MLP model.\n",
    "\n",
    "In summary, the ablation studies partially supported Hypothesis 1. Not all components and hyperparameters in the original model configuration were critical to achieving optimal performance, indicating that some elements of the original design could be modified without detrimental effects on overall model effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhzkYilKBojr"
   },
   "source": [
    "## 4.5 Experiments Beyond The Original Article"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rdOxQoTdLYpd"
   },
   "source": [
    "In our pursuit to expand upon the original study, we have conducted four additional experiments to probe deeper into various significant aspects of model behavior. These experiments aimed to elucidate the mechanics behind ensemble methods, such as max rank and weighted rank average, and to explore the boundaries and reranking capabilities inherent in these strategies. Additionally, we investigated the effects of text length on model performance and employed t-SNE visualizations to explore the semantic relationships between text and molecular structures. A focused study on the attention layer's role  over baseline GCN model also allowed us to isolate and evaluate its specific contribution to model accuracy. These explorations are designed to provide a foundational understanding of the critical factors that enhance or influence model performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mk_kFjNyCAgn"
   },
   "source": [
    "### 4.5.1 Ensemble Stategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 291
    },
    "id": "cMbjLHTpqFf7",
    "outputId": "374a28cc-dc98-4219-91cd-e6e49bba08e5"
   },
   "outputs": [],
   "source": [
    "# @title Exploring Different Ensemble Strategies\n",
    "model_result_file_map_10 = {\n",
    "    'MLP1+GCN1': '/content/tar_files/model evaluation results/ensemble_mlp1_gcn1_eval.csv',\n",
    "    'Ensemble max rank': '/content/tar_files/model evaluation results/additional_experiment_ensemble_max_rank_eval.csv',\n",
    "    'Ensemble weighted rank average': '/content/tar_files/model evaluation results/additional_experiment_ensemble_weighted_rank_average_eval.csv',\n",
    "}\n",
    "\n",
    "table10 = GenerateTable(model_result_file_map_10)\n",
    "display(HTML(\"<h3>Table 10. Evaluation Results for Different Ensemble Strategies</h3>\"))\n",
    "table10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aUf1N7eCqw7F"
   },
   "source": [
    "* **Max Rank Ensemble:**\n",
    "The Max Rank Ensemble strategy selects the highest rank from the individual model predictions, focusing on the most pessimistic outcomes. This approach allows us to examine the lower bounds of the ensemble model's performance.\n",
    "\n",
    "* **Weighted Rank Average Ensemble:**\n",
    "The Weighted Rank Average Ensemble calculates the average ranks of model predictions, assigning greater weight to models with higher MRR. This method, however, does not re-rank the results after computing the weighted averages, which means it does not incorporate positional adjustments based on the aggregated scores. This approach intends to investigate the impact of relative positioning for accurate retrieval performance.\n",
    "\n",
    "Shown in Table 5, the lower bound of ensemble model performance was marginally lower than the original version. However, without reranking, the Weighted Rank Average resulted in a significantly lower test MRR. This finding partially supports Hypothesis 1, suggesting that the original strategy—integrating mean rank and relative rank sorting—remains the most effective among those evaluated."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YwxSD4qyn5yL"
   },
   "source": [
    "### 4.5.2 Text Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 343
    },
    "id": "969ZSXaNoA3L",
    "outputId": "56244c1d-5db8-4e86-9224-f7f3c993fd7b"
   },
   "outputs": [],
   "source": [
    "# @title Assessing Model Performance with Different Text Lengths\n",
    "model_result_file_map_11 = {\n",
    "    'MLP1': '/content/tar_files/model evaluation results/mlp1_eval.csv',\n",
    "    'MLP1 (Text length > 300)': '/content/tar_files/model evaluation results/additional_experiment_text_length_long_eval.csv',\n",
    "    'MLP1 (Text length <= 300)': '/content/tar_files/model evaluation results/additional_experiment_text_length_short_eval.csv',\n",
    "}\n",
    "\n",
    "table11 = GenerateTable(model_result_file_map_11)\n",
    "display(HTML(\"<h3>Table 11. Evaluation Results for Different Text Lengths</h3>\"))\n",
    "table11"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y9lYjbuforGs"
   },
   "source": [
    "* Text Length > 300\n",
    "Text length > 300 is trained on the data with text descriptions longer than 300\n",
    "\n",
    "* Text Length <= 300\n",
    "Text length > 300 is trained on the data with text descriptions shorter or equal to 300\n",
    "\n",
    "As shown in Table 11, longer texts (text length >300) had on par performance with the original MLP model, whereas shorter texts had worse performance. As indicated in Figure 1, 300 is around the median text lengths of the texual descriptions in the original dataset. This suggests that the longer texts model (length > 300) have similar sample sizes to the shorter texts model, and the impact of sample sizes on the model performance is minimal. The worse performance of the shorter texts model is likely because longer descriptions are less noisy and tend to be more informative. (Edwards et al., 2021)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BZUM1uHrx5s_"
   },
   "source": [
    "### 4.5.3 Embedding Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "T5hLc94TyEGd"
   },
   "source": [
    "Given the low Hits@1 rates across all the models, instead of reproducing the qualitative results of the original article, we use embedding visualization extracted from MLP1 model to further investigate quanlitative model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "id": "D08jeBgJy2O-",
    "outputId": "9065fce0-9873-4d81-aada-7e24758b7579"
   },
   "outputs": [],
   "source": [
    "# @title Test Embedding Visualization with t-SNE\n",
    "\n",
    "# Load the text embeddings and CIDs\n",
    "text_embeddings_train = np.load('/content/tar_files/2024_4_8_epoch40_sample100_mlp1/embeddings/text_embeddings_train.npy')\n",
    "cids_train = np.load('/content/tar_files/2024_4_8_epoch40_sample100_mlp1/embeddings/cids_train.npy')\n",
    "\n",
    "# Select the top 20 embeddings and their corresponding CIDs\n",
    "top_n = 100\n",
    "selected_embeddings = text_embeddings_train[:top_n]\n",
    "selected_cids = cids_train[:top_n]\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the selected embeddings\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(selected_embeddings)\n",
    "\n",
    "# Create a colormap with a unique color for each CID\n",
    "unique_cids = np.unique(selected_cids)\n",
    "num_cids = len(unique_cids)\n",
    "cmap = ListedColormap(plt.cm.rainbow(np.linspace(0, 1, num_cids)))\n",
    "\n",
    "# Create a dictionary to map each CID to its corresponding color\n",
    "cid_color_map = {cid: cmap(i) for i, cid in enumerate(unique_cids)}\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create a scatter plot for each CID with its corresponding color\n",
    "for cid in unique_cids:\n",
    "    mask = selected_cids == cid\n",
    "    ax.scatter(embeddings_tsne[mask, 0], embeddings_tsne[mask, 1], c=[cid_color_map[cid]], label=str(cid))\n",
    "\n",
    "    # Annotate each point with its CID label\n",
    "    for i in range(len(selected_cids)):\n",
    "        if selected_cids[i] == cid:\n",
    "            if str(cid) == '24817':\n",
    "                ax.annotate(str(cid), (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
    "                            textcoords=\"offset points\", xytext=(5, 5), ha='right', va='bottom', fontsize=8, color='red')\n",
    "            elif str(cid) == '18854':\n",
    "                ax.annotate(str(cid), (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
    "                            textcoords=\"offset points\", xytext=(-5, -5), ha='right', va='top', fontsize=8, color='blue')\n",
    "            else:\n",
    "                ax.annotate(str(cid), (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
    "                            textcoords=\"offset points\", xytext=(0, 5), ha='center', fontsize=6)\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_xlabel('t-SNE Dimension 1')\n",
    "ax.set_ylabel('t-SNE Dimension 2')\n",
    "ax.set_title('Figure 10. t-SNE Visualization of Top 100 Text Embeddings')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G4iRHBAQPkJg"
   },
   "source": [
    "As depicted in Figure 10, among the top 100 text embeddings, CID 24817 (marked in red) and CID 18854 (marked in blue) form a cluster that is notably closer within the semantic embedding space compared to the text descriptions of other CIDs.\n",
    "\n",
    "[PubChem CID 24817 URL](https://pubchem.ncbi.nlm.nih.gov/compound/24817) (PubChem, 2024) shows that the text description of CID 24817 Zirconium tetrachloride is 'Zirconium tetrachloride is a zirconium coordination entity comprising four chlorine atoms bound to a central zirconium atom. It has a role as a catalyst. It is a zirconium coordination entity and an inorganic chloride.'\n",
    "\n",
    "[PubChem CID 18854 URL](https://pubchem.ncbi.nlm.nih.gov/compound/18854) (PubChem, 2024) shows that CID 18854 1,2,3,4-Tetrachlorobutane is described as '1,2,3,4-tetrachlorobutane is a chloroalkane that is butane substituted by chloro groups at positions 1,2,3 and 4. It has a role as a human metabolite. It is a chloroalkane and a volatile organic compound.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 665
    },
    "id": "U_CgyEKFyBc7",
    "outputId": "ea244da8-6910-4001-a172-659f541653ff"
   },
   "outputs": [],
   "source": [
    "# @title Molecule Embedding Visualization with t-SNE\n",
    "\n",
    "# Load the chemical embeddings and CIDs\n",
    "chem_embeddings_train = np.load('/content/tar_files/2024_4_8_epoch40_sample100_mlp1/embeddings/chem_embeddings_train.npy')\n",
    "cids_train = np.load('/content/tar_files/2024_4_8_epoch40_sample100_mlp1/embeddings/cids_train.npy')\n",
    "\n",
    "# Select the top 20 embeddings and their corresponding CIDs\n",
    "top_n = 100\n",
    "selected_embeddings = chem_embeddings_train[:top_n]\n",
    "selected_cids = cids_train[:top_n]\n",
    "\n",
    "# Apply t-SNE to reduce the dimensionality of the selected embeddings\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "embeddings_tsne = tsne.fit_transform(selected_embeddings)\n",
    "\n",
    "# Create a colormap with a unique color for each CID\n",
    "unique_cids = np.unique(selected_cids)\n",
    "num_cids = len(unique_cids)\n",
    "cmap = ListedColormap(plt.cm.rainbow(np.linspace(0, 1, num_cids)))\n",
    "\n",
    "# Create a dictionary to map each CID to its corresponding color\n",
    "cid_color_map = {cid: cmap(i) for i, cid in enumerate(unique_cids)}\n",
    "\n",
    "# Create a figure and axis\n",
    "fig, ax = plt.subplots(figsize=(10, 8))\n",
    "\n",
    "# Create a scatter plot for each CID with its corresponding color\n",
    "for cid in unique_cids:\n",
    "    mask = selected_cids == cid\n",
    "    ax.scatter(embeddings_tsne[mask, 0], embeddings_tsne[mask, 1], c=[cid_color_map[cid]], label=str(cid))\n",
    "    # Annotate each point with its CID label\n",
    "    for i in range(len(selected_cids)):\n",
    "        if selected_cids[i] == cid:\n",
    "            if str(cid) == '24817':\n",
    "                ax.annotate(str(cid), (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
    "                            textcoords=\"offset points\", xytext=(5, 5), ha='right', va='bottom', fontsize=8, color='red')\n",
    "            elif str(cid) == '18854':\n",
    "                ax.annotate(str(cid), (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
    "                            textcoords=\"offset points\", xytext=(-5, -5), ha='right', va='top', fontsize=8, color='blue')\n",
    "            else:\n",
    "                ax.annotate(str(cid), (embeddings_tsne[i, 0], embeddings_tsne[i, 1]),\n",
    "                            textcoords=\"offset points\", xytext=(0, 5), ha='center', fontsize=6)\n",
    "\n",
    "\n",
    "# Set the axis labels and title\n",
    "ax.set_xlabel('t-SNE Dimension 1')\n",
    "ax.set_ylabel('t-SNE Dimension 2')\n",
    "ax.set_title('Figure 11. t-SNE Visualization of Top 100 Molecule Embeddings')\n",
    "\n",
    "# Add a legend\n",
    "#ax.legend(title='CIDs')\n",
    "\n",
    "# Display the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDaZplRYi36z"
   },
   "source": [
    "As shown in Figure 11, unsurprisingly, CID 24817 (marked in red) and CID 18854 (marked in blue) molecules also form a tight cluster in the shared molecule embedding space, indicating close similarity across both domains."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRtIwcLSzm0q"
   },
   "source": [
    "Figure 12. Molecule Structure of CID 24817 and CID 18854, Retrieved from [PubChem CID 24817 URL](https://pubchem.ncbi.nlm.nih.gov/compound/24817) and [PubChem CID 18854 URL](https://pubchem.ncbi.nlm.nih.gov/compound/18854) Respectively\n",
    "\n",
    "\n",
    "![](https://storage.googleapis.com/598dhl_text2mol_team21/Images/Zirconium%20tetrachloride.png)\n",
    "\n",
    "CID 24817\n",
    "\n",
    "![](https://storage.googleapis.com/598dhl_text2mol_team21/Images/1%2C2%2C3%2C4-Tetrachlorobutane.png)\n",
    "\n",
    "CID 18854"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tjzyLWkHTblK"
   },
   "source": [
    "Upon analyzing their molecular structures, it is observed that both molecules feature four chlorine atoms surrounding the central atom(s). This structural similarity is reflected in the shared semantic embedding space, demonstrating that it effectively captures the parallels in both text descriptions and molecular substructures."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oZX4mgXgzQqp"
   },
   "source": [
    "### 4.5.4 Pure Effect of Attention Layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 259
    },
    "id": "SR3w57VMzlqL",
    "outputId": "9590ca56-e984-47c4-ee54-066518187172"
   },
   "outputs": [],
   "source": [
    "# @title Evaluating the Pure Effect of Attention Layer: GCN Baseline vs. GCN + Attention\n",
    "model_result_file_map_12 = {\n",
    "    'GCN1 Baseline': '/content/tar_files/model evaluation results/gcn1_eval.csv',\n",
    "    'GCN1 with Attention Layer': '/content/tar_files/model evaluation results/ablation_remove_convolutional_layer_2_gcn_eval.csv'\n",
    "}\n",
    "\n",
    "table12 = GenerateTable(model_result_file_map_12)\n",
    "display(HTML(\"<h3>Table 12. Evaluation Results for Pure Effect of Attention Layer</h3>\"))\n",
    "table12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y8hzfAnnWIvr"
   },
   "source": [
    "Using the attention layer on the GCN baseline allows for a more controlled comparison that isolates the impact of the attention mechanism within a consistent architectural framework. This approach avoids the architectural discrepancies that could arise when comparing the effects of an attention layer on different model types, such as MLP model, ensuring that any observed performance differences are genuinely attributable to the attention mechanism itself.\n",
    "\n",
    "Table 12 reveals a marginal increase in test MRR of 0.08%, which aligns with prior findings indicating only minimal support for Hypothesis 3. This suggests that the impact of cross-modal attention and association rules on MRR is likely limited in magnitude."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qH75TNU71eRH"
   },
   "source": [
    "---\n",
    "# <font color=blue>5. DISCUSSION</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fNewWGw_ESUr"
   },
   "source": [
    "## 5.1 Implications of the Experimental Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KLf4hDZe0xNJ"
   },
   "source": [
    "### 5.1.1 Reproducibility Assessment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U8qovr5XWF8"
   },
   "source": [
    "We successfully replicated the findings related to Hypothesis 1, 2 and 3 as reported in the original article.\n",
    "\n",
    "By employing a dose-response relationship analysis and implementing early stopping at epochs 15, 20, and 25, we observed a co-increasing relationship between the number of epochs (less than 40) and model performance. Although our results at epoch 15 align with the original article's performance, further investigation is required to identify factors contributing to the discrepancies observed, aside from the number of epochs.\n",
    "\n",
    "As shown in Table 2 and Table 13, despite higher Hits@10 rates, it's crucial to note that the Hits@1 rate consistently stands at around 30% in our findings. This lower performance at the first rank may not be sufficient for tasks that rely on high accuracy in the topmost predictions, indicating a potential limitation in the models' retrieval capabilities.\n",
    "\n",
    "For the discrepancies of model performances observed in the attention model, our hypothesis is that this discrepancy arises from the already elevated performance of the MLP1 baseline, stemming from extensive training over 40 epochs. Consequently, the modest gains from the attention layer's generated rules might be limited, as these rules derive from the same dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QM4EWF41yKIf"
   },
   "source": [
    "\n",
    "Table 13. Evaluation Results for Single and Ensemble MLP and GCN Models From The Original Article\n",
    "\n",
    "![](https://drive.google.com/uc?id=1euDqte6asdr0SZMff0np8-KBOWWqsfnb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x7YokFMT03Ux"
   },
   "source": [
    "### 5.1.2 Insights from Ablation Studies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eRIHWgKdq38N"
   },
   "source": [
    "\n",
    "\n",
    "Our comprehensive and systematic ablation studies revealed that not all components of the model architectures, the hyperparameters or the losses proposed in the original article contribute to optimal performance. This observation partially supports Hypothesis 1.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7kaulCgM1cY-"
   },
   "source": [
    "#### 5.1.2.1 Modifications on Model Architectures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HwIpsS54affS"
   },
   "source": [
    "Removing the normalization layer from an MLP model can significantly decrease performance because normalization stabilizes the learning process by reducing internal covariate shift, thereby ensuring that each layer’s inputs have zero mean and unit variance. This not only speeds up training but also helps in maintaining consistent scale across activations, preventing the gradient vanishing or exploding problems that are critical for deep networks.\n",
    "\n",
    "Adding a dropout rate of 0.5 likely reduces model performance because it excessively disables neurons during training, potentially leading to underfitting by preventing the model from learning detailed patterns in the data.\n",
    "\n",
    "However, the lack of significant performance change upon removing the middle hidden layer in the molecule encoder in MLP model or the middle convolutional layer in the GCN model suggests that these layers might not be critical for capturing the essential features required for the task. This could indicate redundancy within the network architecture, where the remaining layers are capable of compensating for the removed layer's function. Additionally, it may reflect that the model's capacity to encode and process molecular information effectively does not heavily rely on the depth provided by this specific layer.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gqohC_g61t5z"
   },
   "source": [
    "#### 5.1.2.2 Modifications on Hyperparameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uMdmxZ8aajP_"
   },
   "source": [
    "Reducing the batch size from 32 to 16 can lead to decreased performance because smaller batches often provide a less accurate estimate of the gradient. This results in noisier, less stable updates during training, which can hinder convergence to the optimal weights, especially in complex models. Similarly, halving the learning rate from 1e-4 to 5e-5 may cause the learning process to become too slow, potentially failing to converge within the allocated training epochs or getting stuck in suboptimal minima, thereby reducing overall performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iQojLddc1c6M"
   },
   "source": [
    "#### 5.1.2.3 Modifications on Loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Uy3T-_aAbuY0"
   },
   "source": [
    "Switching from symmetric contrastive loss to naive loss results in poorer outcomes because symmetric contrastive loss effectively minimizes the distance between similar pairs while maximizing the distance between dissimilar pairs, enhancing the discriminative power of the model. In contrast, naive loss, which may not explicitly account for the relationship between non-similar pairs, fails to enforce the same level of separation, leading to a less effective feature space for distinguishing between classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BSsMwjo519eC"
   },
   "source": [
    "### 5.1.3 Discussion of Findings from Additional Experiments Beyond the Original Paper"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qrmX_cPx2Vvq"
   },
   "source": [
    "#### 5.1.3.1 Ensemble Strategy Exploration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eEaJuyzBY-y3"
   },
   "source": [
    "Our experiments further confirmed that the ensemble strategy in the original article consistently outperformed any single model. This improvement can be attributed to the complementary strengths of MLP and GCN models, which excel at different tasks; their combination therefore yields greater benefits.\n",
    "\n",
    "Additionally, the use of relative rank within the ensemble contributed to this enhanced performance. The mean rank with sorting ensemble strategy performs better than weighted rank average method because it provides a robust average that can mitigate the influence of outliers or extreme values from individual model predictions. In addition, this approach sorts the predictions first, which ensures that each model's output has a proportional impact, avoiding the dominance of any single model's potentially skewed prediction. In contrast, weighted rank average can overly emphasize models based on their past performance, which might not always be indicative of future accuracy.\n",
    "\n",
    "Max rank ensemble simulates the lower-bound of ensemble strategy performance. Surprisingly, although it can depend too heavily on the single prediction, which might not be consistent across different data points, its performance was slightly worse than the original version. These findings support Hypothesis 2, affirming that ensemble strategies surpass the effectiveness of individual models, and the inherit sorting mechanism could be the essential contributing factor.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QavlzH2z2cx_"
   },
   "source": [
    "#### 5.1.3.2 Text Length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_TFpxc8ncKq0"
   },
   "source": [
    "Text descriptions longer than 300 characters tend to yield better results primarily because they contain more detailed information, allowing models to capture a richer set of features and contexts essential for accurate predictions. However, there remains a practical challenge: ideally, we would prefer models to achieve high accuracy with shorter texts to ensure efficiency and applicability in scenarios where verbose descriptions are unavailable. Addressing this requires further refining model architectures or employing techniques that can extract and leverage critical information from more concise texts effectively, maintaining performance without the necessity for lengthier inputs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7xrVJlow2ppi"
   },
   "source": [
    "#### 5.1.3.3 Embedding Visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z1zIeSWUc9pQ"
   },
   "source": [
    "t-SNE embedding visualizations offer significant benefits in understanding high-dimensional data by projecting it into a lower-dimensional space, making it possible to visually assess the clustering and separation of different data points. We have successfully conducted t-SNE embedding visualizations to examine both text and molecule embeddings within the shared semantic embedding space. This technique is particularly valuable for visualizing model performance and conducting qualitative analyses, especially useful when Hits@1 is low. By observing how closely related items cluster together, t-SNE helps in identifying patterns and potential areas for model improvement, providing intuitive insights that are not easily obtainable through traditional metrics alone."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H4xuoLwx2wgl"
   },
   "source": [
    "#### 5.1.3.4 Pure Effect of Attention Layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "29TEZJzmdczt"
   },
   "source": [
    "Integrating a transformer decoder attention layer on top of a GCN model for reranking offers a compelling approach to enhance retrieval capabilities by leveraging both structural graph information and contextual interdependencies within data. The attention mechanism facilitates a more nuanced understanding and processing of the relationships between nodes in a graph, effectively capturing complex patterns that a standard GCN might miss. By applying this attention-enhanced GCN model to rerank results, we can directly compare its performance with a baseline GCN to isolate and evaluate the distinct impact of the attention layer. This comparison is crucial for assessing how the attention layer modifies the embedding space and improves retrieval outcomes, especially in complex datasets where relational intricacies are pivotal for accurate predictions. Such a setup allows for a clear delineation of the attention layer's contributions, providing a pure measure of its effect on enhancing model performance over the traditional GCN framework."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-86QOIKqEZHf"
   },
   "source": [
    "## 5.2 What Was Easy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WNRhimQjq4qK"
   },
   "source": [
    "* Re-running the original authors' baseline MLP and GCN code required minimal effort, with only light debugging of `main.py` necessary to get the models operational.\n",
    "* The ease of execution was significantly aided by the extensive documentation and example runs provided, which facilitated the retrieval and utilization of various model checkpoints."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P7EW8iIUEmMm"
   },
   "source": [
    "## 5.3 What Was Difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K0rYdjkrq-hx"
   },
   "source": [
    "* Implementing the attention mechanism was challenging as the author's provided code was incomplete, particularly lacking components for handling association rules, which is essential for calculating evaluation metrics. Consequently, we had to adapt the code for weight extraction from the original paper, necessitated by recent updates in PyTorch. Additionally, we undertook the task of rewriting the code for the calculation of the evaluation metrics of the model with the incorporation of the new association rules extracted from the attention layer.\n",
    "* Training the models was considerably time-consuming, with each MLP model training for 4 hours, GCN 8 hours and Attention for over 10 hours.\n",
    "* The output data was cumbersome to manage since it was not formatted for straightforward storage or analysis, such as in a CSV format, thereby complicating data cleaning and manipulation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xV0qYUE-Eunu"
   },
   "source": [
    "## 5.4 Suggestions To The Author For Improving Reproducibility"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gO0l8HpirC9H"
   },
   "source": [
    "* To enhance reproducibility and potentially improve model performance, a deeper analysis of the impacts of various epochs, model architecture components, and hyperparameters is recommended. This approach could uncover additional opportunities for optimization and effectiveness in model design.\n",
    "\n",
    "* For ease of downstream analysis, it is advisable to standardize the storage of output results in a structured format, such as CSV files. This practice would simplify data handling and enable more straightforward reproducibility and analysis.\n",
    "\n",
    "* Consider expanding the dataset to include a more diverse and larger sample size with description lengths shorter than 20 words. Adding human-labeled or synthesized data could also improve model robustness and generalization across different datasets and applications.\n",
    "\n",
    "* Last but not least, code readability is always an important factor for anyone who wants to reproduce the article. Consider improve code readability by having test cases, meaningful variable names and in-line comments."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IXGkTSqCaN1i"
   },
   "source": [
    "## 5.5 Limitation\n",
    "\n",
    "However, a limitation of our approach is that we did not have sufficient time to conduct all the additional experiments we wanted to have, which may have provided further insights into the effectiveness and robustness of different model configurations and ensemble strategies. Text2Mol is certainly an interesting and challenging task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1phjkMqLCzee"
   },
   "source": [
    "---\n",
    "# <font color=blue>6. References</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SHMI2chl9omn"
   },
   "source": [
    "1.  **Citation to the original paper:**\n",
    "Edwards, C., Zhai, C., & Ji, H. (2021). Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries. In Proceedings of the 2021 Conference on Empirical Methods in Natural Language Processing, 595–607. Online and Punta Cana, Dominican Republic. Association for Computational Linguistics. Data retrieved from: https://aclanthology.org/2021.emnlp-main.47/.\n",
    "\n",
    "2. **Citation to the original paper’s GitHub repository:**\n",
    "Edwards, C.(2021). Text2Mol: Cross-Modal Molecule Retrieval with Natural Language Queries. Data retrieved from: https://github.com/cnedwards/text2mol.git.\n",
    "\n",
    "3. Beltagy, l., Lo, K., & Cohan, A. (2019). SciBERT: A pretrained language model for scientific text. In Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP), 3615– 3620. Hong Kong, China. Association for Computational Linguistics. Date retrieved from: https://aclanthology.org/D19-1371/.\n",
    "\n",
    "4. Rogers, D. & Hahn, M. (2010). Extendedconnectivity fingerprints. Journal of Chemical Information and Modeling, 50(5):742–754. Data retrieved from: https://pubs.acs.org/doi/10.1021/ci100050t.\n",
    "\n",
    "5. Jaeger, S., Fulle, S. & Turk, S. (2018). Mol2vec: unsupervised machine learning approach with chemical intuition. Journal of chemical information and modeling, 58(1):27–35. Date retrieved from: https://pubs.acs.org/doi/abs/10.1021/acs.jcim.7b00616.\n",
    "\n",
    "6. Han, J. & Pei, J. (2000). Mining frequent patterns by pattern-growth: methodology and implications. ACM SIGKDD explorations newsletter, 2(2):14–20. Date retrieved from: https://dl.acm.org/doi/10.1145/380995.381002.\n",
    "\n",
    "7. PubChem.(2024). Zirconium tetrachloride. Data retrieved from: https://pubchem.ncbi.nlm.nih.gov/compound/24817\n",
    "\n",
    "8. PubChem. (2024). ,2,3,4-Tetrachlorobutane\n",
    "(Compound). Data retrieved from: https://pubchem.ncbi.nlm.nih.gov/compound/18854\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tKQBNV_kPEwS"
   },
   "source": [
    "---\n",
    "# <font color=blue>7. Appendix</font>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtna8ncePMGw"
   },
   "source": [
    "* Figure 1\n",
    "![](https://drive.google.com/uc?id=1kUKdvfUpeKFjHxJH6PwJf6kS9auo7UMl)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "70eFO_fxPfiP"
   },
   "source": [
    "* Figure 2\n",
    "![](https://drive.google.com/uc?id=14x__vjANi_o7RoOMCecXFDgx_WrdhcTu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-QvaKVZSQL6r"
   },
   "source": [
    "* Figure 3\n",
    "\n",
    "![](https://drive.google.com/uc?id=1qLMTR15ms3aU4egNdE7twq_Bqm3aueee)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Y-5EAgMBw9Tq"
   },
   "source": [
    "* Figure 4\n",
    "\n",
    "![](https://drive.google.com/uc?id=1eICmM7z2HYmUn53rte7BS9m-BllTt4sq)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gc-7T2w-wWoQ"
   },
   "source": [
    "* Figure 5\n",
    "![](https://drive.google.com/uc?id=1jABkaZwkr1D1I9M4s2zLu8KrlXhMMSVB)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TElQYjFoPpF2"
   },
   "source": [
    "* Table 1\n",
    "\n",
    "![](https://drive.google.com/uc?id=1lCE0FeZm6lZUBT3gA0z4YBSO9MqY8eNd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UZrIqaeWQW3P"
   },
   "source": [
    "* Table 2\n",
    "\n",
    "![](https://drive.google.com/uc?id=1RpJ1Vzh3JFUwU1TC2dmzuY1c7pL0pYHi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nKtEhLhmQvty"
   },
   "source": [
    "* Table 4\n",
    "![](https://drive.google.com/uc?id=15V71cKTrhIctYbtChg-wJZTX331qlV2R)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4cs3ru2FQ3YT"
   },
   "source": [
    "* Table 5\n",
    "![](https://drive.google.com/uc?id=12XSjnAYPEKuNJ5eI9j4xhtt39VsKsh9Z)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "35697ac58f234506bf9c29b996a017bd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": ""
     }
    },
    "375e90df0c8a45ddb087a6c8249a9900": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3cd267c740384717bd920a7705177e4d": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "516f0a7f931c42ffa45a297bfa628e38": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "621cd25b67524460b998c428c33b2a68": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6ea3f8bb38a0462fa8f2e98ecdf937f6": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_621cd25b67524460b998c428c33b2a68",
      "placeholder": "​",
      "style": "IPY_MODEL_516f0a7f931c42ffa45a297bfa628e38",
      "value": " 3/? [00:14&lt;00:00,  4.35s/it]"
     }
    },
    "86dcd2ed50c14b349286786aa4c3a9cd": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_375e90df0c8a45ddb087a6c8249a9900",
      "max": 2,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_35697ac58f234506bf9c29b996a017bd",
      "value": 2
     }
    },
    "ad51fd1e718b41aa9c9732fae04d2c9e": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "c9ef4a6f36ab406885c00cdf76d0ddf2": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_fa523698e082448db270af282d427f91",
      "placeholder": "​",
      "style": "IPY_MODEL_3cd267c740384717bd920a7705177e4d",
      "value": "Installing Packages: "
     }
    },
    "f83a7aaaa6b14802a34044ccdd932b08": {
     "model_module": "@jupyter-widgets/controls",
     "model_module_version": "1.5.0",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_c9ef4a6f36ab406885c00cdf76d0ddf2",
       "IPY_MODEL_86dcd2ed50c14b349286786aa4c3a9cd",
       "IPY_MODEL_6ea3f8bb38a0462fa8f2e98ecdf937f6"
      ],
      "layout": "IPY_MODEL_ad51fd1e718b41aa9c9732fae04d2c9e"
     }
    },
    "fa523698e082448db270af282d427f91": {
     "model_module": "@jupyter-widgets/base",
     "model_module_version": "1.2.0",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
